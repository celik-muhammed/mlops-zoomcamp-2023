{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2525fb2f",
   "metadata": {},
   "source": [
    "<div style=\"align: center;\">\n",
    "    <br>\n",
    "    <img src=\"https://www.nyc.gov/assets/tlc/images/content/hero/MRP-Closing-Week.jpg\" style=\"display:block; margin:auto; width:65%; height:250px;\">\n",
    "</div><br><br> \n",
    "\n",
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "<!--   https://xkcd.com/color/rgb/   -->\n",
    "  <p style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>TLC Trip Record Data</strong></p>  \n",
    "  \n",
    "  <p style=\"text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:22px; font-weight:normal; text-transform: capitalize; padding: 5px;\"\n",
    "     >Machine Learning Module: PREFECT - Ride Duration Prediction using Regression Analysis<br>( MLFLOW & PREFECT )</p>    \n",
    "</div>\n",
    "\n",
    "- https://github.com/discdiver/prefect-mlops-zoomcamp\n",
    "- https://mlflow.org/docs/0.7.0/index.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32a8b00d",
   "metadata": {},
   "source": [
    "**Dataset Info**\n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "Yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
    "\n",
    "For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
    "\n",
    "\n",
    "**ATTENTION!**\n",
    "\n",
    "On 05/13/2022, we are making the following changes to trip record files:\n",
    "\n",
    "- All files will be stored in the PARQUET format. Please see the ‘Working With PARQUET Format’ under the Data Dictionaries and MetaData section.\n",
    "- Trip data will be published monthly (with two months delay) instead of bi-annually.\n",
    "- HVFHV files will now include 17 more columns (please see High Volume FHV Trips Dictionary for details). Additional columns will be added to the old files as well. The earliest date to include additional columns: February 2019.\n",
    "- Yellow trip data will now include 1 additional column (‘airport_fee’, please see Yellow Trips Dictionary for details). The additional column will be added to the old files as well. The earliest date to include the additional column: January 2011.\n",
    "\n",
    "\n",
    "**Download the data for January and February 2023**\n",
    "\n",
    "Dataset: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "\n",
    "**Data Dictionaries and MetaData**\n",
    "\n",
    "- We'll use the same `NYC taxi dataset`, but instead of \"Yellow Taxi Trip Records\", we'll use `\"Green Taxi Trip Records\"`.\n",
    "\n",
    "> `Green Trips Data Dictionary`: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33b74ab3",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "The goal of this homework is to familiarize users with workflow orchestration. \n",
    "\n",
    "Start with the orchestrate.py file in the 03-orchestration/3.4 folder\n",
    "of the course repo: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py<br>\n",
    "\n",
    "Questions: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2023/03-orchestration/homework.md\n",
    "\n",
    "- https://sagarthacker.com/posts/mlops/intro_workflow_orchestration.html\n",
    "- https://sagarthacker.com/posts/mlops/prefect-blocks.html\n",
    "- https://sagarthacker.com/posts/mlops/prefect-deployment.html\n",
    "\n",
    "\n",
    "**Table of Content**\n",
    "\n",
    "\n",
    "1. Import Libraries and Ingest Data\n",
    "    - Q1. Human-readable name<br>    \n",
    "2. Recognizing and Understanding Data\n",
    "    - Q2. Cron<br>\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78138c11",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>1. Import Libraries & Ingest Data</strong></h1>   \n",
    "</div>\n",
    "\n",
    "> ⚠️ Not Recommended conda `base` env, work on `venv`\n",
    "\n",
    "- https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "conda list -e > requirements.txt\n",
    "\n",
    "# new conda virtual environment\n",
    "conda create --name \"prefect-ops\" python=3.10 jupyter -y\n",
    "conda activate \"prefect-ops\"\n",
    "\n",
    "# install all package dependencies\n",
    "pip install -r requirements.txt\n",
    "conda install -c conda-forge --file=requirements.txt      # mostly not work\n",
    "conda install -c conda-forge pandas==2.0.2 -q -y\n",
    "\n",
    "# if The environment is inconsistent, try below\n",
    "conda update -n base -c defaults conda --force-reinstall\n",
    "conda install anaconda --force-reinstall\n",
    "\n",
    "```\n",
    "\n",
    "**You must use the `--no-deps` option in the pip install command in order to avoid bundling dependencies into your conda-package.**\n",
    "\n",
    "If you run pip install without the `--no-deps` option, pip will often install dependencies in your conda recipe and those dependencies will become part of your package. This wastes space in the package and `increases the risk of file overlap`, file clobbering, and broken packages.\n",
    "\n",
    "There might be cases where you want to install a package directly from a local directory or a specific location, without relying on the package indexes. In such situations, you can use the `--no-index` option to tell pip not to look for the package in any indexes.\n",
    "\n",
    "```\n",
    "- command1 & command2  # runs simultaneously\n",
    "- command1 ; command2  # runs sequentially\n",
    "- command1 && command2 # runs sequentially, runs command2 only if command1 succeeds\n",
    "- command1 || command2 # runs sequentially, runs command2 only if command1 fails\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ff0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture cap --no-stderr  # capture outputs  # cap.show()\n",
    "# !cat /etc/os-release\n",
    "# !grep -E -w 'VERSION|NAME|PRETTY_NAME' /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54cd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check enviroment\n",
    "# !conda env list\n",
    "# !conda info -e\n",
    "# !conda info | grep 'active env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74723240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt \n",
    "# To get started with MLflow you'll need to install the appropriate Python package.\n",
    "\n",
    "pandas==2.0.2\n",
    "orjson==3.9.1          # orjson is a fast, correct JSON library\n",
    "seaborn==0.12.2\n",
    "\n",
    "# ML Model packages\n",
    "scikit-learn==1.2.2\n",
    "xgboost==1.7.3\n",
    "\n",
    "# MLOPS packages\n",
    "mlflow==2.4.1\n",
    "wandb==0.15.4\n",
    "prefect==2.10.18\n",
    "prefect-email==0.2.2\n",
    "prefect-aws==0.3.4\n",
    "\n",
    "# Optionally\n",
    "black==23.3.0          # code style\n",
    "\n",
    "# ML Model packages\n",
    "hyperopt==0.2.7\n",
    "\n",
    "# for parquet file\n",
    "pyarrow==11.0.0\n",
    "fastparquet==2023.4.0\n",
    "\n",
    "# Optionally\n",
    "# jupyter\n",
    "# ipykernel\n",
    "# ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db08661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Python  : 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]\n",
      "Platform: Linux Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Actv Env: prefect-ops\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, IPython.display\n",
    "\n",
    "# !{sys.executable} -m pip install -Uq -r requirements.txt  #  --no-deps --no-cache-dir --force-reinstall --no-index\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# IPython.display.clear_output()\n",
    "print(\"Python  :\", sys.version)\n",
    "print(\"Platform:\", platform.system(), platform.platform())\n",
    "print(\"Actv Env:\", os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23feabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# Create a new directory for storing MLflow data\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "os.makedirs('./pycode', exist_ok=True)\n",
    "os.makedirs('./models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ffeeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import stats\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "import mlflow\n",
    "import pickle\n",
    "import pathlib\n",
    "import argparse\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "tqdm._instances.clear()\n",
    "\n",
    "from prefect import task, flow, Flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# memory management performs garbage collection \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb564b98",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>2. Recognizing and Understanding Data</strong></h1>   \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebb5b120",
   "metadata": {},
   "source": [
    "## Ingest Data [wget](https://linuxways.net/centos/linux-wget-command-with-examples/) or [curl](https://daniel.haxx.se/blog/2020/09/10/store-the-curl-output-over-there/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2a3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Green Taxi Trip Records\" Download the data for January, February and March 2022\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044ce591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/green_tripdata_2023-01.parquet',\n",
       " './data/green_tripdata_2023-02.parquet',\n",
       " './data/green_tripdata_2023-03.parquet',\n",
       " './data/green_tripdata_2023-04.parquet']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(f'./data/*.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f41e352",
   "metadata": {},
   "source": [
    "## Q1. Human-readable name\n",
    "\n",
    "You’d like to give the first task, `read_data` a nicely formatted name.\n",
    "How can you specify a task name?\n",
    "\n",
    "> Hint: look in the docs at https://docs.prefect.io or \n",
    "> check out the doc string in a code editor.\n",
    "\n",
    "- `@task(retries=3, retry_delay_seconds=2, name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task_name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task-name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task_name_function=lambda x: f\"Read taxi data\")`\n",
    "\n",
    "**How can you specify a task name?**\n",
    "- https://docs.prefect.io/2.10.13/concepts/tasks/\n",
    "- @task(retries=3, retry_delay_seconds=2, name=\"Read taxi data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "159982ef",
   "metadata": {},
   "source": [
    "## Q2. Cron\n",
    "\n",
    "Cron is a common scheduling specification for workflows. \n",
    "\n",
    "Using the flow in `orchestrate.py`, create a deployment.\n",
    "Schedule your deployment to run on the third day of every month at 9am UTC.\n",
    "\n",
    "**What’s the cron schedule for that?**\n",
    "\n",
    "components of the cron expression:\n",
    "\n",
    "- The first field, 0, represents the minute of the hour. In this case, it is set to 0, meaning the deployment will run at the start of the hour.\n",
    "- The second field, 9, represents the hour of the day. It is set to 9, indicating that the deployment will run at 9am.\n",
    "- The third field, 3, represents the day of the month. This field is set to 3, which means the deployment will run specifically on the third day of each month.\n",
    "- The fourth field, *, represents the month. It is set to *, indicating that the deployment will run every month.\n",
    "- The fifth field, *, represents the day of the week. It is also set to *, meaning that the deployment will run regardless of the day of the week.\n",
    "\n",
    "Therefore, the cron schedule for running the deployment on the third day of every month at 9am UTC is:\n",
    "- `0 9 3 * *`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c8f5e35",
   "metadata": {},
   "source": [
    "## Q3. RMSE \n",
    "\n",
    "Download the January 2023 Green Taxi data and use it for your training data.\n",
    "Download the February 2023 Green Taxi data and use it for your validation data. \n",
    "\n",
    "Make sure you upload the data to GitHub so it is available for your deployment.\n",
    "\n",
    "Create a custom flow run of your deployment from the UI. Choose Custom\n",
    "Run for the flow and enter the file path as a string on the JSON tab under Parameters.\n",
    "\n",
    "Make sure you have a worker running and polling the correct work pool.\n",
    "\n",
    "View the results in the UI.\n",
    "\n",
    "**What’s the final RMSE to five decimal places?**\n",
    "\n",
    "mlflow-remote code:\n",
    "- https://www.mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded\n",
    "- Run Terminal: \n",
    "\n",
    "```\n",
    "# if using port 5000 or use another port\n",
    "kill $(lsof -ti :5000)   # clear port 5000\n",
    "\n",
    "# after set_experiment (building mlflow.db), go to bash cd path to (mlruns ant mlflow.db) folder\n",
    "mlflow ui \\\n",
    "    --backend-store-uri sqlite:///mlflow.db \\\n",
    "    --default-artifact-root  file:mlruns \\\n",
    "    --host localhost --port 5000   \n",
    "```\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c31ba885",
   "metadata": {},
   "source": [
    "![prefec](https://docs.prefect.io/latest/img/concepts/flow-deployment-end-to-end.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5ae5287",
   "metadata": {},
   "source": [
    "Check Data\n",
    "```sh\n",
    "git remote -v\n",
    "```\n",
    "\n",
    "**Step 1: Create a Project in a New Directory**\n",
    "\n",
    "- .prefectignore File any Github-Gitlab\n",
    "```sh\n",
    "prefect init\n",
    "prefect project init # deprecated\n",
    "```\n",
    "\n",
    "**Step 2: Add an @flow decorator to your code's entrypoint function, give it a name and Save it to the New Directory**\n",
    "\n",
    "- like orchestrate.py, so lets rewrite and edit orchestrate.py then push it\n",
    "\n",
    "\n",
    "**Step 3: Orchestration Enviroment (Server Runs or Prefect Cloud)**\n",
    "\n",
    "- Start the Prefect server locally\n",
    "\n",
    "Create another window and activate your conda environment. Start the Prefect API server locally with\n",
    "Start and Open Source Prefect Server\n",
    "```sh\n",
    "prefect server start\n",
    "```\n",
    "\n",
    "- Or Login to Prefect Cloud\n",
    "```sh\n",
    "prefect cloud login\n",
    "```\n",
    "\n",
    "**Step 4: Execution Enviroment (Deployed! Flows Run)**\n",
    "\n",
    "- Start a worker that polls your work pool\n",
    "```sh\n",
    "# prefect worker start -p \"zoom_mlops_pool\" -t process --limit 2\n",
    "prefect worker start --pool 'zoom_mlops_pool'\n",
    "```\n",
    "\n",
    "**Step 5: Deploy your Flow**\n",
    "\n",
    "- Start a worker that polls your work pool\n",
    "```sh\n",
    "# prefect deploy ls\n",
    "# prefect deploy -n zoom_mlops_deployment\n",
    "prefect deploy \"./pycode/orchestrate.py:main_flow\" -n \"zoom_mlops_deployment\" -p \"zoom_mlops_pool\"\n",
    "# prefect deploy --all  # or \n",
    "```\n",
    "\n",
    "**Step 6: Start a run of the deployed flow from the CLI...**\n",
    "\n",
    "- Start a worker that polls your work pool\n",
    "```sh\n",
    "prefect deployment ls\n",
    "prefect deployment run 'Main Flow/zoom_mlops_deployment'\n",
    "```\n",
    "\n",
    "**Check...**\n",
    "\n",
    "- another pools\n",
    "```sh\n",
    "prefect block ls\n",
    "prefect block type ls\n",
    "```\n",
    "\n",
    "```sh\n",
    "prefect block register -m prefect_aws \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ce59af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/orchestrate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/orchestrate.py\n",
    "\n",
    "# Source: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py\n",
    "\n",
    "import os\n",
    "import click\n",
    "import pickle\n",
    "import pathlib\n",
    "import argparse\n",
    "import requests\n",
    "import urllib.request\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "import prefect\n",
    "from prefect import task, flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# from prefect_aws import S3Bucket\n",
    "# from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "\n",
    "\n",
    "@task(name=\"Fetch Data\", cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1),\n",
    "      retries=3, log_prints=True, )\n",
    "def fetch_data(raw_data_path: str, year: int, month: int, color: str) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "    os.makedirs(raw_data_path, exist_ok=True)  \n",
    "\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    url      = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0>2}.parquet'\n",
    "    filename = os.path.join(raw_data_path, f'{color}_tripdata_{year}-{month:0>2}.parquet')\n",
    "    # urllib.request.urlretrieve(url, filename)\n",
    "    # os.system(f\"wget -q -N -P {raw_data_path} {url}\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    return None\n",
    "    \n",
    "    \n",
    "@task(name=\"Read Taxi Data\", retries=3, retry_delay_seconds=2, log_prints=None)\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime  = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration    = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical     = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(name=\"Add Features Taxi Data\", log_prints=True)\n",
    "def add_features(\n",
    "    df_train: pd.DataFrame, df_val: pd.DataFrame, df_test: pd.DataFrame\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    df_train[\"PU_DO\"] = df_train[\"PULocationID\"] + \"_\" + df_train[\"DOLocationID\"]\n",
    "    df_val[\"PU_DO\"]   = df_val[\"PULocationID\"]   + \"_\" + df_val[\"DOLocationID\"]\n",
    "    df_test[\"PU_DO\"]  = df_test[\"PULocationID\"]  + \"_\" + df_test[\"DOLocationID\"]\n",
    "\n",
    "    categorical = [\"PU_DO\"]  #'PULocationID', 'DOLocationID']\n",
    "    numerical   = [\"trip_distance\"]\n",
    "\n",
    "    dv = DictVectorizer()\n",
    "\n",
    "    train_dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_train     = dv.fit_transform(train_dicts)\n",
    "    y_train     = df_train[\"duration\"].values\n",
    "\n",
    "    val_dicts   = df_val[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_val       = dv.transform(val_dicts)\n",
    "    y_val       = df_val[\"duration\"].values\n",
    "    \n",
    "    test_dicts  = df_test[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_test      = dv.transform(test_dicts)\n",
    "    y_test      = df_test[\"duration\"].values\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), dv\n",
    "\n",
    "\n",
    "@task(name=\"Train Best Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train  : scipy.sparse._csr.csr_matrix,\n",
    "    X_val    : scipy.sparse._csr.csr_matrix,\n",
    "    y_train  : np.ndarray,\n",
    "    y_val    : np.ndarray,\n",
    "    dv       : sklearn.feature_extraction.DictVectorizer,\n",
    "    dest_path: str,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"        \n",
    "    # Load train and test Data\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "        mlflow.set_tag(\"train-data-path\", f'./data/green_tripdata_2023-01.parquet')\n",
    "        mlflow.set_tag(\"valid-data-path\", f'./data/green_tripdata_2023-02.parquet')\n",
    "        mlflow.set_tag(\"test-data-path\",  f'./data/green_tripdata_2023-03.parquet')\n",
    "\n",
    "        # Set Model params information\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "            'objective': 'reg:squarederror',          # deprecated  \"reg:linear\"\n",
    "            # 'objective': \"reg:linear\",\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Build Model   \n",
    "        booster = xgb.train(\n",
    "            params               = best_params,\n",
    "            dtrain               = train,\n",
    "            num_boost_round      = 100,\n",
    "            evals                = [(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )   \n",
    "        \n",
    "        # Set Model Evaluation Metric\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse   = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)       \n",
    "\n",
    "        # Log Model two options\n",
    "        # Option1: Just log model\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")        \n",
    "        \n",
    "        # Option 2: save Model, Optional: Preprocessor or Pipeline         \n",
    "        # Create dest_path folder unless it already exists\n",
    "        # pathlib.Path(dest_path).mkdir(exist_ok=True) \n",
    "        os.makedirs(dest_path, exist_ok=True)       \n",
    "        local_file = os.path.join(dest_path, \"preprocessor.b\")\n",
    "        with open(local_file, \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        # whole proccess like pickle, saved Model, Optional: Preprocessor or Pipeline\n",
    "        mlflow.log_artifact(local_path = local_file, artifact_path=\"preprocessor\")        \n",
    "        \n",
    "        # print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "    return None\n",
    "\n",
    "\n",
    "@flow(name=\"Subflow Download Data\", log_prints=True)\n",
    "def download_data(raw_data_path: str, years: list, months: list, colors: list):\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for color in colors:\n",
    "                fetch_data(raw_data_path, year, month, color)\n",
    "                \n",
    "\n",
    "# click work on Local but it Gives ERRORS Deploy step\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--raw_data_path\",\n",
    "    default=\"./data\",\n",
    "    help=\"Location where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--dest_path\",\n",
    "    default=\"./models\",\n",
    "    help=\"Location where the resulting model files will be saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--years\",\n",
    "    default=\"2023\",\n",
    "    help=\"Years where the raw NYC taxi trip data was saved (space-separated)\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--months\",\n",
    "    default=\"1 2 3\",\n",
    "    help=\"Months where the raw NYC taxi trip data was saved (space-separated)\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--colors\",\n",
    "    default=\"green yellow\",\n",
    "    help=\"Colors where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(raw_data_path: str, dest_path: str, years: str, months: str, colors: str) -> None:\n",
    "    \"\"\"The main training pipeline\"\"\"\n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Build or Connect mlflow experiment\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "    \n",
    "    # Download data    \n",
    "    years  = [int(year) for year in years.split()]\n",
    "    months = [int(month) for month in months.split()]\n",
    "    colors = colors.split()[:1]\n",
    "    download_data(raw_data_path, years, months, colors)\n",
    "    print(sorted(glob(f'./data/*')))\n",
    "    \n",
    "    # list parquet files\n",
    "    # print(sorted(glob(f'{raw_data_path}/green*.parquet')))\n",
    "    train_path, val_path, test_path = sorted(glob(f'{raw_data_path}/*.parquet'))[:3:]\n",
    "\n",
    "    # Read parquet files\n",
    "    df_train = read_data(train_path)\n",
    "    df_val   = read_data(val_path)\n",
    "    df_test  = read_data(test_path)\n",
    "    # print(df_train.shape, df_val.shape, df_test.shape, )\n",
    "\n",
    "    # Transform\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test), dv = add_features(df_train, df_val, df_test)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv, dest_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # argparse work on Local but it Gives ERRORS Deploy step\n",
    "    # parser = argparse.ArgumentParser(description=\"Main Flow\")\n",
    "    # parser.add_argument(\"--raw_data_path\", default=\"./data\",       help=\"Location where the raw NYC taxi trip data was saved\")\n",
    "    # parser.add_argument(\"--dest_path\",     default=\"./models\",     help=\"Location where the resulting model files will be saved\")\n",
    "    # parser.add_argument(\"--years\",         default=\"2023\",         help=\"Years where the raw NYC taxi trip data was saved (space-separated)\")\n",
    "    # parser.add_argument(\"--months\",        default=\"1 2 3 4\",      help=\"Months where the raw NYC taxi trip data was saved (space-separated)\")\n",
    "    # parser.add_argument(\"--colors\",        default=\"green yellow\", help=\"Colors where the raw NYC taxi trip data was saved\")\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    # main_flow(args.raw_data_path, args.dest_path, args.years, args.months, args.colors)\n",
    "\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f85a3b6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:37:41.518 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'blond-hamster'\u001b[0m for flow\u001b[1;35m 'Main Flow'\u001b[0m\n",
      "18:37:41.521 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - View at \u001b[94mhttp://127.0.0.1:4200/flow-runs/flow-run/34ce9bc7-5b45-49f4-a896-068d2d1591a9\u001b[0m\n",
      "2023/07/03 18:37:42 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2023/07/03 18:37:42 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "18:37:42.797 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Created subflow run\u001b[35m 'abstract-taipan'\u001b[0m for flow\u001b[1;35m 'Subflow Download Data'\u001b[0m\n",
      "18:37:42.800 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'abstract-taipan'\u001b[0m - View at \u001b[94mhttp://127.0.0.1:4200/flow-runs/flow-run/c9352a64-de17-406f-abea-538e1f3fc185\u001b[0m\n",
      "18:37:43.058 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'abstract-taipan'\u001b[0m - Created task run 'Fetch Data-0' for task 'Fetch Data'\n",
      "18:37:43.059 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'abstract-taipan'\u001b[0m - Executing 'Fetch Data-0' immediately...\n",
      "18:37:45.346 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:37:45.412 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'abstract-taipan'\u001b[0m - Created task run 'Fetch Data-1' for task 'Fetch Data'\n",
      "18:37:45.414 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'abstract-taipan'\u001b[0m - Executing 'Fetch Data-1' immediately...\n",
      "18:37:47.423 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:37:47.504 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'abstract-taipan'\u001b[0m - Created task run 'Fetch Data-2' for task 'Fetch Data'\n",
      "18:37:47.506 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'abstract-taipan'\u001b[0m - Executing 'Fetch Data-2' immediately...\n",
      "18:37:49.725 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-2' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:37:49.863 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'abstract-taipan'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n",
      "['./data/green_tripdata_2023-01.parquet', './data/green_tripdata_2023-02.parquet', './data/green_tripdata_2023-03.parquet', './data/green_tripdata_2023-04.parquet']\n",
      "18:37:49.943 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Created task run 'Read Taxi Data-0' for task 'Read Taxi Data'\n",
      "18:37:49.945 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Executing 'Read Taxi Data-0' immediately...\n",
      "18:37:50.802 | \u001b[36mINFO\u001b[0m    | Task run 'Read Taxi Data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:37:50.868 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Created task run 'Read Taxi Data-1' for task 'Read Taxi Data'\n",
      "18:37:50.870 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Executing 'Read Taxi Data-1' immediately...\n",
      "18:37:51.599 | \u001b[36mINFO\u001b[0m    | Task run 'Read Taxi Data-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:37:51.668 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Created task run 'Read Taxi Data-2' for task 'Read Taxi Data'\n",
      "18:37:51.670 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Executing 'Read Taxi Data-2' immediately...\n",
      "18:37:52.489 | \u001b[36mINFO\u001b[0m    | Task run 'Read Taxi Data-2' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:37:52.960 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Created task run 'Add Features Taxi Data-0' for task 'Add Features Taxi Data'\n",
      "18:37:52.961 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Executing 'Add Features Taxi Data-0' immediately...\n",
      "18:37:54.572 | \u001b[36mINFO\u001b[0m    | Task run 'Add Features Taxi Data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:37:54.664 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Created task run 'Train Best Model-0' for task 'Train Best Model'\n",
      "18:37:54.667 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Executing 'Train Best Model-0' immediately...\n",
      "18:37:57.344 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [0]    validation-rmse:15.01627\n",
      "18:37:58.051 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [1]    validation-rmse:13.77591\n",
      "18:37:58.620 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [2]    validation-rmse:12.66953\n",
      "18:37:58.923 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [3]    validation-rmse:11.68972\n",
      "18:37:59.251 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [4]    validation-rmse:10.81927\n",
      "18:37:59.540 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [5]    validation-rmse:10.05352\n",
      "18:37:59.833 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [6]    validation-rmse:9.37638\n",
      "18:38:00.317 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [7]    validation-rmse:8.78514\n",
      "18:38:00.768 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [8]    validation-rmse:8.26684\n",
      "18:38:01.103 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [9]    validation-rmse:7.81889\n",
      "18:38:01.362 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [10]   validation-rmse:7.42994\n",
      "18:38:01.603 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [11]   validation-rmse:7.09348\n",
      "18:38:01.848 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [12]   validation-rmse:6.80836\n",
      "18:38:02.082 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [13]   validation-rmse:6.56211\n",
      "18:38:02.400 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [14]   validation-rmse:6.35332\n",
      "18:38:02.649 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [15]   validation-rmse:6.17838\n",
      "18:38:03.035 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [16]   validation-rmse:6.02763\n",
      "18:38:03.534 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [17]   validation-rmse:5.90232\n",
      "18:38:04.005 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [18]   validation-rmse:5.79691\n",
      "18:38:04.322 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [19]   validation-rmse:5.70817\n",
      "18:38:04.728 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [20]   validation-rmse:5.63182\n",
      "18:38:04.991 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [21]   validation-rmse:5.56852\n",
      "18:38:05.238 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [22]   validation-rmse:5.51520\n",
      "18:38:05.492 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [23]   validation-rmse:5.47115\n",
      "18:38:05.754 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [24]   validation-rmse:5.43421\n",
      "18:38:05.979 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [25]   validation-rmse:5.40206\n",
      "18:38:06.262 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [26]   validation-rmse:5.37653\n",
      "18:38:06.490 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [27]   validation-rmse:5.35308\n",
      "18:38:06.724 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [28]   validation-rmse:5.33437\n",
      "18:38:06.915 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [29]   validation-rmse:5.31842\n",
      "18:38:07.129 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [30]   validation-rmse:5.30434\n",
      "18:38:07.320 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [31]   validation-rmse:5.29293\n",
      "18:38:07.531 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [32]   validation-rmse:5.28270\n",
      "18:38:07.777 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [33]   validation-rmse:5.27307\n",
      "18:38:07.972 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [34]   validation-rmse:5.26638\n",
      "18:38:08.146 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [35]   validation-rmse:5.26043\n",
      "18:38:08.303 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [36]   validation-rmse:5.25512\n",
      "18:38:08.525 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [37]   validation-rmse:5.25088\n",
      "18:38:08.835 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [38]   validation-rmse:5.24664\n",
      "18:38:09.012 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [39]   validation-rmse:5.24209\n",
      "18:38:09.222 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [40]   validation-rmse:5.23909\n",
      "18:38:09.407 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [41]   validation-rmse:5.23607\n",
      "18:38:09.590 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [42]   validation-rmse:5.23344\n",
      "18:38:09.755 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [43]   validation-rmse:5.23200\n",
      "18:38:09.936 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [44]   validation-rmse:5.23097\n",
      "18:38:10.100 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [45]   validation-rmse:5.22865\n",
      "18:38:10.474 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [46]   validation-rmse:5.22775\n",
      "18:38:10.633 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [47]   validation-rmse:5.22687\n",
      "18:38:10.832 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [48]   validation-rmse:5.22582\n",
      "18:38:11.005 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [49]   validation-rmse:5.22487\n",
      "18:38:11.210 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [50]   validation-rmse:5.22442\n",
      "18:38:11.407 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [51]   validation-rmse:5.22332\n",
      "18:38:11.602 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [52]   validation-rmse:5.22236\n",
      "18:38:11.764 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [53]   validation-rmse:5.22142\n",
      "18:38:11.931 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [54]   validation-rmse:5.22022\n",
      "18:38:12.101 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [55]   validation-rmse:5.21987\n",
      "18:38:12.275 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [56]   validation-rmse:5.21908\n",
      "18:38:12.524 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [57]   validation-rmse:5.21873\n",
      "18:38:12.688 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [58]   validation-rmse:5.21820\n",
      "18:38:12.905 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [59]   validation-rmse:5.21787\n",
      "18:38:13.195 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [60]   validation-rmse:5.21702\n",
      "18:38:13.344 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [61]   validation-rmse:5.21632\n",
      "18:38:13.520 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [62]   validation-rmse:5.21571\n",
      "18:38:13.691 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [63]   validation-rmse:5.21404\n",
      "18:38:13.854 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [64]   validation-rmse:5.21348\n",
      "18:38:14.056 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [65]   validation-rmse:5.21290\n",
      "18:38:14.314 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [66]   validation-rmse:5.21262\n",
      "18:38:14.477 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [67]   validation-rmse:5.21181\n",
      "18:38:14.664 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [68]   validation-rmse:5.21179\n",
      "18:38:14.868 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [69]   validation-rmse:5.21113\n",
      "18:38:15.110 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [70]   validation-rmse:5.21051\n",
      "18:38:15.316 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [71]   validation-rmse:5.21000\n",
      "18:38:15.500 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [72]   validation-rmse:5.20944\n",
      "18:38:15.695 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [73]   validation-rmse:5.20928\n",
      "18:38:15.870 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [74]   validation-rmse:5.20890\n",
      "18:38:16.062 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [75]   validation-rmse:5.20840\n",
      "18:38:16.224 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [76]   validation-rmse:5.20723\n",
      "18:38:16.440 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [77]   validation-rmse:5.20677\n",
      "18:38:16.617 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [78]   validation-rmse:5.20653\n",
      "18:38:16.782 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [79]   validation-rmse:5.20608\n",
      "18:38:16.948 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [80]   validation-rmse:5.20599\n",
      "18:38:17.117 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [81]   validation-rmse:5.20558\n",
      "18:38:17.315 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [82]   validation-rmse:5.20516\n",
      "18:38:17.509 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [83]   validation-rmse:5.20428\n",
      "18:38:17.666 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [84]   validation-rmse:5.20377\n",
      "18:38:17.833 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [85]   validation-rmse:5.20323\n",
      "18:38:18.025 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [86]   validation-rmse:5.20281\n",
      "18:38:18.202 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [87]   validation-rmse:5.20294\n",
      "18:38:18.397 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [88]   validation-rmse:5.20271\n",
      "18:38:18.563 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [89]   validation-rmse:5.20245\n",
      "18:38:18.727 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [90]   validation-rmse:5.20201\n",
      "18:38:18.896 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [91]   validation-rmse:5.20186\n",
      "18:38:19.066 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [92]   validation-rmse:5.20146\n",
      "18:38:19.232 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [93]   validation-rmse:5.20144\n",
      "18:38:19.603 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [94]   validation-rmse:5.20096\n",
      "18:38:19.800 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [95]   validation-rmse:5.20087\n",
      "18:38:19.962 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [96]   validation-rmse:5.20016\n",
      "18:38:20.124 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [97]   validation-rmse:5.19983\n",
      "18:38:20.305 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [98]   validation-rmse:5.19931\n",
      "18:38:20.498 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [99]   validation-rmse:5.19931\n",
      "18:38:29.368 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:38:29.476 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'blond-hamster'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "# raw_data_path\n",
    "# DATA_PATH = f\"./data\"\n",
    "\n",
    "!python ./pycode/orchestrate.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fada2c5e",
   "metadata": {},
   "source": [
    "## Q4. RMSE (Markdown Artifact)\n",
    "\n",
    "Download the February 2023 Green Taxi data and use it for your training data.\n",
    "Download the March 2023 Green Taxi data and use it for your validation data. \n",
    "\n",
    "Create a Prefect Markdown artifact that displays the RMSE for the validation data.\n",
    "Create a deployment and run it.\n",
    "\n",
    "**What’s the RMSE in the artifact to two decimal places ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab8b3a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/create_s3_bucket_block.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/create_s3_bucket_block.py\n",
    "\n",
    "from time import sleep\n",
    "from prefect_aws import S3Bucket, AwsCredentials\n",
    "\n",
    "\n",
    "def create_aws_creds_block():\n",
    "    my_aws_creds_obj = AwsCredentials(\n",
    "        aws_access_key_id=\"123abc\", aws_secret_access_key=\"abc123\"\n",
    "    )\n",
    "    my_aws_creds_obj.save(name=\"my-aws-creds\", overwrite=True)\n",
    "\n",
    "\n",
    "def create_s3_bucket_block():\n",
    "    aws_creds = AwsCredentials.load(\"my-aws-creds\")\n",
    "    my_s3_bucket_obj = S3Bucket(\n",
    "        bucket_name=\"my-first-bucket-abc\", credentials=aws_creds\n",
    "    )\n",
    "    my_s3_bucket_obj.save(name=\"s3-bucket-example\", overwrite=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_aws_creds_block()\n",
    "    sleep(5)\n",
    "    create_s3_bucket_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b62c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/orchestrate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/orchestrate.py\n",
    "\n",
    "# Source: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py\n",
    "\n",
    "import os\n",
    "import click\n",
    "import pickle\n",
    "import pathlib\n",
    "import argparse\n",
    "import requests\n",
    "import urllib.request\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "import prefect\n",
    "from prefect import task, flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# from prefect_aws import S3Bucket\n",
    "# from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "\n",
    "\n",
    "@task(name=\"Fetch Data\", cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1),\n",
    "      retries=3, log_prints=True, )\n",
    "def fetch_data(raw_data_path: str, year: int, month: int, color: str) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "    os.makedirs(raw_data_path, exist_ok=True)  \n",
    "\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    url      = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0>2}.parquet'\n",
    "    filename = os.path.join(raw_data_path, f'{color}_tripdata_{year}-{month:0>2}.parquet')\n",
    "    # urllib.request.urlretrieve(url, filename)\n",
    "    # os.system(f\"wget -q -N -P {raw_data_path} {url}\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    return None\n",
    "    \n",
    "    \n",
    "@task(name=\"Read Taxi Data\", retries=3, retry_delay_seconds=2, log_prints=None)\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime  = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration    = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical     = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(name=\"Add Features Taxi Data\", log_prints=True)\n",
    "def add_features(\n",
    "    df_train: pd.DataFrame, df_val: pd.DataFrame, df_test: pd.DataFrame\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    df_train[\"PU_DO\"] = df_train[\"PULocationID\"] + \"_\" + df_train[\"DOLocationID\"]\n",
    "    df_val[\"PU_DO\"]   = df_val[\"PULocationID\"]   + \"_\" + df_val[\"DOLocationID\"]\n",
    "    df_test[\"PU_DO\"]  = df_test[\"PULocationID\"]  + \"_\" + df_test[\"DOLocationID\"]\n",
    "\n",
    "    categorical = [\"PU_DO\"]  #'PULocationID', 'DOLocationID']\n",
    "    numerical   = [\"trip_distance\"]\n",
    "\n",
    "    dv = DictVectorizer()\n",
    "\n",
    "    train_dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_train     = dv.fit_transform(train_dicts)\n",
    "    y_train     = df_train[\"duration\"].values\n",
    "\n",
    "    val_dicts   = df_val[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_val       = dv.transform(val_dicts)\n",
    "    y_val       = df_val[\"duration\"].values\n",
    "    \n",
    "    test_dicts  = df_test[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_test      = dv.transform(test_dicts)\n",
    "    y_test      = df_test[\"duration\"].values\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), dv\n",
    "\n",
    "\n",
    "@task(name=\"Train Best Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train  : scipy.sparse._csr.csr_matrix,\n",
    "    X_val    : scipy.sparse._csr.csr_matrix,\n",
    "    y_train  : np.ndarray,\n",
    "    y_val    : np.ndarray,\n",
    "    dv       : sklearn.feature_extraction.DictVectorizer,\n",
    "    dest_path: str,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"        \n",
    "    # Load train and test Data\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "        mlflow.set_tag(\"train-data-path\", f'./data/green_tripdata_2023-01.parquet')\n",
    "        mlflow.set_tag(\"valid-data-path\", f'./data/green_tripdata_2023-02.parquet')\n",
    "        mlflow.set_tag(\"test-data-path\",  f'./data/green_tripdata_2023-03.parquet')\n",
    "\n",
    "        # Set Model params information\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "#             'objective': 'reg:squarederror',          # deprecated  \"reg:linear\"\n",
    "            'objective': \"reg:linear\",\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Build Model   \n",
    "        booster = xgb.train(\n",
    "            params               = best_params,\n",
    "            dtrain               = train,\n",
    "            num_boost_round      = 100,\n",
    "            evals                = [(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )   \n",
    "        \n",
    "        # Set Model Evaluation Metric\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse   = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)       \n",
    "\n",
    "        # Log Model two options\n",
    "        # Option1: Just log model\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")        \n",
    "        \n",
    "        # Option 2: save Model, Optional: Preprocessor or Pipeline         \n",
    "        # Create dest_path folder unless it already exists\n",
    "        # pathlib.Path(dest_path).mkdir(exist_ok=True) \n",
    "        os.makedirs(dest_path, exist_ok=True)       \n",
    "        local_file = os.path.join(dest_path, \"preprocessor.b\")\n",
    "        with open(local_file, \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        # whole proccess like pickle, saved Model, Optional: Preprocessor or Pipeline\n",
    "        mlflow.log_artifact(local_path = local_file, artifact_path=\"preprocessor\")        \n",
    "        \n",
    "        # print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "\n",
    "        # Create markdown artifact with RMSE value\n",
    "        markdown__rmse_report = f\"\"\"\n",
    "# RMSE Report\n",
    "\n",
    "## Summary\n",
    "\n",
    "Duration Prediction \n",
    "\n",
    "## RMSE XGBoost Model\n",
    "\n",
    "| Region    | RMSE |\n",
    "|:----------|-------:|\n",
    "| {date.today()} | {rmse:.2f} |\n",
    "\"\"\"\n",
    "\n",
    "        create_markdown_artifact(\n",
    "            key=\"duration-model-report\", \n",
    "            markdown=markdown__rmse_report,\n",
    "            description=\"RMSE for Validation Data Report\",\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "@flow(name=\"Subflow Download Data\", log_prints=True)\n",
    "def download_data(raw_data_path: str, years: list, months: list, colors: list):\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for color in colors:\n",
    "                fetch_data(raw_data_path, year, month, color)\n",
    "\n",
    "\n",
    "# @flow(name=\"Email Server Crenditals\", log_prints=True)\n",
    "# def example_email_send_message_flow(email_addresses: list[str]):\n",
    "#     email_server_credentials = EmailServerCredentials.load(\"email-server-credentials\")\n",
    "    \n",
    "#     for email_address in email_addresses:\n",
    "#         subject = email_send_message.with_options(name=f\"email {email_address}\").submit(\n",
    "#             email_server_credentials=email_server_credentials,\n",
    "#             subject=\"Example Flow Notification using Gmail\",\n",
    "#             msg=\"This proves email_send_message works!\",\n",
    "#             email_to=email_address,\n",
    "#         )\n",
    "                \n",
    "\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(raw_data_path=\"./data\", dest_path=\"./models\", years=\"2023\", months=\"1 2 3 4\", colors=\"green yellow\") -> None:\n",
    "    \"\"\"The main training pipeline\"\"\"\n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Build or Connect mlflow experiment\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "    \n",
    "    # Download data    \n",
    "    years  = [int(year) for year in years.split()]\n",
    "    months = [int(month) for month in months.split()]\n",
    "    colors = colors.split()[:1]\n",
    "    download_data(raw_data_path, years, months, colors)\n",
    "    # print(glob(f'*'))\n",
    "    \n",
    "    # # Download the data from AWS S3 Bucket\n",
    "    # s3_bucket_block = S3Bucket.load(\"s3-bucket-block\")\n",
    "    # s3_bucket_block.download_folder_to_path(from_folder=\"data\", to_folder=\"data\")\n",
    "    \n",
    "    # list parquet files\n",
    "    # print(sorted(glob(f'{raw_data_path}/green*.parquet')))\n",
    "    train_path, val_path, test_path = sorted(glob(f'{raw_data_path}/*.parquet'))[-3::]\n",
    "\n",
    "    # Read parquet files\n",
    "    df_train = read_data(train_path)\n",
    "    df_val   = read_data(val_path)\n",
    "    df_test  = read_data(test_path)\n",
    "    # print(df_train.shape, df_val.shape, df_test.shape, )\n",
    "\n",
    "    # Transform\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test), dv = add_features(df_train, df_val, df_test)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv, dest_path)\n",
    "\n",
    "    # example_email_send_message_flow(['@gmail.com'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b5f353c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:39:53.665 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m for flow\u001b[1;35m 'Main Flow'\u001b[0m\n",
      "18:39:53.671 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - View at \u001b[94mhttp://127.0.0.1:4200/flow-runs/flow-run/b3deb933-fbfe-4a72-b7a6-8280c5046a00\u001b[0m\n",
      "2023/07/03 18:39:54 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2023/07/03 18:39:54 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "18:39:54.731 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Created subflow run\u001b[35m 'invisible-goshawk'\u001b[0m for flow\u001b[1;35m 'Subflow Download Data'\u001b[0m\n",
      "18:39:54.733 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'invisible-goshawk'\u001b[0m - View at \u001b[94mhttp://127.0.0.1:4200/flow-runs/flow-run/8b6ac94a-0ee9-4def-8a65-c6e0ea8ec258\u001b[0m\n",
      "18:39:54.996 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'invisible-goshawk'\u001b[0m - Created task run 'Fetch Data-0' for task 'Fetch Data'\n",
      "18:39:54.998 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'invisible-goshawk'\u001b[0m - Executing 'Fetch Data-0' immediately...\n",
      "18:39:55.136 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-0' - Finished in state Cached(type=COMPLETED)\n",
      "18:39:55.194 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'invisible-goshawk'\u001b[0m - Created task run 'Fetch Data-1' for task 'Fetch Data'\n",
      "18:39:55.195 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'invisible-goshawk'\u001b[0m - Executing 'Fetch Data-1' immediately...\n",
      "18:39:55.688 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-1' - Finished in state Cached(type=COMPLETED)\n",
      "18:39:55.753 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'invisible-goshawk'\u001b[0m - Created task run 'Fetch Data-2' for task 'Fetch Data'\n",
      "18:39:55.756 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'invisible-goshawk'\u001b[0m - Executing 'Fetch Data-2' immediately...\n",
      "18:39:55.899 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-2' - Finished in state Cached(type=COMPLETED)\n",
      "18:39:55.962 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'invisible-goshawk'\u001b[0m - Created task run 'Fetch Data-3' for task 'Fetch Data'\n",
      "18:39:55.964 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'invisible-goshawk'\u001b[0m - Executing 'Fetch Data-3' immediately...\n",
      "18:39:58.670 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-3' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:39:58.801 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'invisible-goshawk'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n",
      "18:39:58.879 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Created task run 'Read Taxi Data-0' for task 'Read Taxi Data'\n",
      "18:39:58.881 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Executing 'Read Taxi Data-0' immediately...\n",
      "18:39:59.628 | \u001b[36mINFO\u001b[0m    | Task run 'Read Taxi Data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:39:59.686 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Created task run 'Read Taxi Data-1' for task 'Read Taxi Data'\n",
      "18:39:59.688 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Executing 'Read Taxi Data-1' immediately...\n",
      "18:40:00.423 | \u001b[36mINFO\u001b[0m    | Task run 'Read Taxi Data-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:40:00.488 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Created task run 'Read Taxi Data-2' for task 'Read Taxi Data'\n",
      "18:40:00.489 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Executing 'Read Taxi Data-2' immediately...\n",
      "18:40:01.263 | \u001b[36mINFO\u001b[0m    | Task run 'Read Taxi Data-2' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:40:01.335 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Created task run 'Add Features Taxi Data-0' for task 'Add Features Taxi Data'\n",
      "18:40:01.337 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Executing 'Add Features Taxi Data-0' immediately...\n",
      "18:40:02.592 | \u001b[36mINFO\u001b[0m    | Task run 'Add Features Taxi Data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:40:02.656 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Created task run 'Train Best Model-0' for task 'Train Best Model'\n",
      "18:40:02.658 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Executing 'Train Best Model-0' immediately...\n",
      "[18:40:04] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "18:40:05.157 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [0]    validation-rmse:15.33078\n",
      "18:40:05.778 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [1]    validation-rmse:14.09739\n",
      "18:40:06.352 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [2]    validation-rmse:12.99920\n",
      "18:40:06.676 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [3]    validation-rmse:12.02243\n",
      "18:40:06.995 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [4]    validation-rmse:11.15585\n",
      "18:40:07.305 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [5]    validation-rmse:10.39226\n",
      "18:40:07.655 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [6]    validation-rmse:9.71696\n",
      "18:40:07.929 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [7]    validation-rmse:9.12782\n",
      "18:40:08.223 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [8]    validation-rmse:8.60975\n",
      "18:40:08.510 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [9]    validation-rmse:8.15872\n",
      "18:40:08.783 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [10]   validation-rmse:7.76686\n",
      "18:40:09.074 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [11]   validation-rmse:7.42798\n",
      "18:40:09.427 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [12]   validation-rmse:7.13629\n",
      "18:40:09.727 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [13]   validation-rmse:6.88486\n",
      "18:40:09.976 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [14]   validation-rmse:6.67077\n",
      "18:40:10.277 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [15]   validation-rmse:6.48829\n",
      "18:40:10.510 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [16]   validation-rmse:6.33274\n",
      "18:40:10.787 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [17]   validation-rmse:6.19826\n",
      "18:40:11.156 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [18]   validation-rmse:6.08427\n",
      "18:40:11.441 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [19]   validation-rmse:5.98670\n",
      "18:40:11.666 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [20]   validation-rmse:5.90483\n",
      "18:40:11.901 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [21]   validation-rmse:5.83439\n",
      "18:40:12.154 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [22]   validation-rmse:5.77680\n",
      "18:40:12.400 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [23]   validation-rmse:5.72543\n",
      "18:40:12.633 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [24]   validation-rmse:5.68091\n",
      "18:40:12.841 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [25]   validation-rmse:5.64424\n",
      "18:40:13.187 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [26]   validation-rmse:5.61182\n",
      "18:40:13.435 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [27]   validation-rmse:5.58395\n",
      "18:40:13.889 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [28]   validation-rmse:5.56122\n",
      "18:40:14.301 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [29]   validation-rmse:5.54000\n",
      "18:40:14.622 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [30]   validation-rmse:5.52124\n",
      "18:40:15.208 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [31]   validation-rmse:5.50483\n",
      "18:40:15.486 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [32]   validation-rmse:5.49293\n",
      "18:40:15.768 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [33]   validation-rmse:5.48085\n",
      "18:40:16.301 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [34]   validation-rmse:5.47086\n",
      "18:40:16.490 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [35]   validation-rmse:5.46186\n",
      "18:40:16.649 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [36]   validation-rmse:5.45498\n",
      "18:40:16.825 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [37]   validation-rmse:5.44771\n",
      "18:40:16.993 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [38]   validation-rmse:5.44088\n",
      "18:40:17.456 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [39]   validation-rmse:5.43434\n",
      "18:40:17.641 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [40]   validation-rmse:5.43017\n",
      "18:40:17.792 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [41]   validation-rmse:5.42578\n",
      "18:40:18.039 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [42]   validation-rmse:5.42230\n",
      "18:40:18.229 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [43]   validation-rmse:5.41903\n",
      "18:40:18.416 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [44]   validation-rmse:5.41589\n",
      "18:40:18.665 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [45]   validation-rmse:5.41305\n",
      "18:40:18.848 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [46]   validation-rmse:5.41009\n",
      "18:40:19.005 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [47]   validation-rmse:5.40823\n",
      "18:40:19.264 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [48]   validation-rmse:5.40656\n",
      "18:40:19.437 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [49]   validation-rmse:5.40523\n",
      "18:40:19.658 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [50]   validation-rmse:5.40380\n",
      "18:40:19.826 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [51]   validation-rmse:5.40214\n",
      "18:40:20.029 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [52]   validation-rmse:5.40109\n",
      "18:40:20.227 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [53]   validation-rmse:5.40025\n",
      "18:40:20.376 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [54]   validation-rmse:5.39867\n",
      "18:40:20.544 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [55]   validation-rmse:5.39758\n",
      "18:40:20.687 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [56]   validation-rmse:5.39653\n",
      "18:40:20.856 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [57]   validation-rmse:5.39541\n",
      "18:40:21.048 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [58]   validation-rmse:5.39468\n",
      "18:40:21.385 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [59]   validation-rmse:5.39393\n",
      "18:40:21.560 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [60]   validation-rmse:5.39310\n",
      "18:40:21.701 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [61]   validation-rmse:5.39250\n",
      "18:40:21.869 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [62]   validation-rmse:5.39156\n",
      "18:40:22.019 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [63]   validation-rmse:5.39117\n",
      "18:40:22.222 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [64]   validation-rmse:5.39068\n",
      "18:40:22.384 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [65]   validation-rmse:5.38985\n",
      "18:40:22.534 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [66]   validation-rmse:5.38964\n",
      "18:40:22.722 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [67]   validation-rmse:5.38894\n",
      "18:40:22.891 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [68]   validation-rmse:5.38829\n",
      "18:40:23.086 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [69]   validation-rmse:5.38770\n",
      "18:40:23.249 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [70]   validation-rmse:5.38663\n",
      "18:40:23.434 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [71]   validation-rmse:5.38621\n",
      "18:40:23.602 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [72]   validation-rmse:5.38556\n",
      "18:40:23.742 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [73]   validation-rmse:5.38495\n",
      "18:40:24.031 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [74]   validation-rmse:5.38442\n",
      "18:40:24.265 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [75]   validation-rmse:5.38382\n",
      "18:40:24.409 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [76]   validation-rmse:5.38359\n",
      "18:40:24.566 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [77]   validation-rmse:5.38310\n",
      "18:40:24.747 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [78]   validation-rmse:5.38273\n",
      "18:40:24.943 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [79]   validation-rmse:5.38246\n",
      "18:40:25.142 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [80]   validation-rmse:5.38223\n",
      "18:40:25.306 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [81]   validation-rmse:5.38196\n",
      "18:40:25.455 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [82]   validation-rmse:5.38102\n",
      "18:40:25.612 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [83]   validation-rmse:5.38055\n",
      "18:40:25.769 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [84]   validation-rmse:5.38044\n",
      "18:40:25.933 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [85]   validation-rmse:5.38019\n",
      "18:40:26.133 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [86]   validation-rmse:5.37990\n",
      "18:40:26.348 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [87]   validation-rmse:5.37936\n",
      "18:40:26.615 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [88]   validation-rmse:5.37883\n",
      "18:40:26.770 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [89]   validation-rmse:5.37832\n",
      "18:40:26.950 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [90]   validation-rmse:5.37805\n",
      "18:40:27.144 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [91]   validation-rmse:5.37788\n",
      "18:40:27.336 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [92]   validation-rmse:5.37722\n",
      "18:40:27.489 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [93]   validation-rmse:5.37667\n",
      "18:40:27.664 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [94]   validation-rmse:5.37631\n",
      "18:40:27.822 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [95]   validation-rmse:5.37589\n",
      "18:40:27.987 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [96]   validation-rmse:5.37561\n",
      "18:40:28.144 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [97]   validation-rmse:5.37512\n",
      "18:40:28.345 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [98]   validation-rmse:5.37468\n",
      "18:40:28.551 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [99]   validation-rmse:5.37450\n",
      "18:40:35.854 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "18:40:35.953 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'cryptic-chinchilla'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/orchestrate.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4852bb63",
   "metadata": {},
   "source": [
    "- Start a worker that polls your work pool\n",
    "```sh\n",
    "# prefect deploy --all  # or \n",
    "prefect deploy \"./pycode/orchestrate.py:main_flow\" -n \"zoom_mlops_deployment\" -p \"zoom_mlops_pool\"\n",
    "prefect worker start --pool 'zoom_mlops_pool'\n",
    "prefect deployment run 'Main Flow/zoom_mlops_deployment'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3476a2d",
   "metadata": {},
   "source": [
    "```\n",
    "31f95cf7\n",
    "Flow run\n",
    "wonderful-bat\n",
    "Task run\n",
    "Train Best Model-0\n",
    "RMSE for Validation Data Report\n",
    "RMSE for Validation Data\n",
    "RMSE: 5.374495195206525\n",
    "\n",
    "\n",
    "dc032057\n",
    "Flow run\n",
    "interesting-cougar\n",
    "Task run\n",
    "Train Best Model-0\n",
    "Created gtm-report\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb448527",
   "metadata": {},
   "source": [
    "## Q5. Emails\n",
    "\n",
    "\n",
    "It’s often helpful to be notified when something with your dataflow doesn’t work\n",
    "as planned. Create an email notification for to use with your own Prefect server instance.\n",
    "In your virtual environment, install the prefect-email integration with \n",
    "\n",
    "```bash\n",
    "pip install prefect-email\n",
    "```\n",
    "\n",
    "Make sure you are connected to a running Prefect server instance through your\n",
    "Prefect profile.\n",
    "See the docs if needed: https://docs.prefect.io/latest/concepts/settings/#configuration-profiles\n",
    "\n",
    "Register the new block with your server with \n",
    "\n",
    "```bash\n",
    "prefect block register -m prefect_email\n",
    "```\n",
    "\n",
    "Remember that a block is a Prefect class with a nice UI form interface.\n",
    "Block objects live on the server and can be created and accessed in your Python code. \n",
    "\n",
    "See the docs for how to authenticate by saving your email credentials to\n",
    "a block and note that you will need an App Password to send emails with\n",
    "Gmail and other services. Follow the instructions in the docs.\n",
    "\n",
    "Create and save an `EmailServerCredentials` notification block.\n",
    "Use the credentials block to send an email.\n",
    "\n",
    "Test the notification functionality by running a deployment.\n",
    "\n",
    "**What is the name of the pre-built prefect-email task function?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e209b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSuccessfully registered 1 block\u001b[0m\r\n",
      "\r\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┓\r\n",
      "┃\u001b[1m \u001b[0m\u001b[1mRegistered Blocks       \u001b[0m\u001b[1m \u001b[0m┃\r\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━┩\r\n",
      "│ Email Server Credentials │\r\n",
      "└──────────────────────────┘\r\n",
      "\r\n",
      " To configure the newly registered blocks, go to the Blocks page in the Prefect \r\n",
      "UI.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!prefect block register -m prefect_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84767650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Block.save at 0x7f5e362fb450>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prefect_email import EmailServerCredentials\n",
    "\n",
    "credentials = EmailServerCredentials(\n",
    "    username=\"EMAIL-ADDRESS-PLACEHOLDER\",\n",
    "    password=\"PASSWORD-PLACEHOLDER\",  # must be an app password\n",
    ")\n",
    "credentials.save(\"BLOCK-NAME-PLACEHOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91168fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Block.load at 0x7f5e362fb140>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prefect_email import EmailServerCredentials\n",
    "\n",
    "EmailServerCredentials.load(\"BLOCK_NAME_PLACEHOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f441a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Send an email using Gmail\n",
    "from prefect import flow\n",
    "from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "@flow(name=\"Email Server Credentials\")\n",
    "def example_email_send_message_flow():\n",
    "    email_server_credentials = EmailServerCredentials(\n",
    "        username=\"your_email_address@gmail.com\",\n",
    "        password=\"MUST_be_an_app_password_here!\",\n",
    "    )\n",
    "    # email_server_credentials.save(\"BLOCK-NAME-PLACEHOLDER\")\n",
    "    # email_server_credentials = EmailServerCredentials.load(\"BLOCK_NAME_PLACEHOLDER\")\n",
    "    \n",
    "    subject = email_send_message(\n",
    "        email_server_credentials=email_server_credentials,\n",
    "        subject=\"Example Flow Notification using Gmail\",\n",
    "        msg=\"This proves email_send_message works!\",\n",
    "        email_to=\"someone_awesome@gmail.com\",\n",
    "    )\n",
    "    return subject\n",
    "\n",
    "# example_email_send_message_flow()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c53b9c2",
   "metadata": {},
   "source": [
    "## Q6. Prefect Cloud\n",
    "\n",
    "The hosted Prefect Cloud lets you avoid running your own Prefect server and\n",
    "has automations that allow you to get notifications when certain events occur\n",
    "or don’t occur. \n",
    "\n",
    "Create a free forever Prefect Cloud account at [app.prefect.cloud](https://app.prefect.cloud/) and connect\n",
    "your workspace to it following the steps in the UI when you sign up. \n",
    "\n",
    "Set up an Automation from the UI that will send yourself an email when\n",
    "a flow run completes. Run one of your existing deployments and check\n",
    "your email to see the notification.\n",
    "\n",
    "Make sure your active profile is pointing toward Prefect Cloud and\n",
    "make sure you have a worker active.\n",
    "\n",
    "**What is the name of the second step in the Automation creation process?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5552947",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "- The name of the second step in the Automation creation process in Prefect Cloud is \"Configure Triggers.\" This step allows you to define the conditions or events that will trigger the Automation to execute. In this case, you would configure the trigger to activate when a flow run completes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf34682f",
   "metadata": {},
   "source": [
    "# End of The Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
