{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2525fb2f",
   "metadata": {},
   "source": [
    "<div style=\"align: center;\">\n",
    "    <br>\n",
    "    <img src=\"https://www.nyc.gov/assets/tlc/images/content/hero/MRP-Closing-Week.jpg\" style=\"display:block; margin:auto; width:65%; height:250px;\">\n",
    "</div><br><br> \n",
    "\n",
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "<!--   https://xkcd.com/color/rgb/   -->\n",
    "  <p style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>TLC Trip Record Data</strong></p>  \n",
    "  \n",
    "  <p style=\"text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:22px; font-weight:normal; text-transform: capitalize; padding: 5px;\"\n",
    "     >Machine Learning Module: MLFLOW - Ride Duration Prediction using Regression Analysis<br>( MLFLOW )</p>    \n",
    "</div>\n",
    "\n",
    "- https://mlflow.org/docs/0.7.0/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8b00d",
   "metadata": {},
   "source": [
    "**Dataset Info**\n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "Yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
    "\n",
    "For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
    "\n",
    "\n",
    "**ATTENTION!**\n",
    "\n",
    "On 05/13/2022, we are making the following changes to trip record files:\n",
    "\n",
    "- All files will be stored in the PARQUET format. Please see the ‘Working With PARQUET Format’ under the Data Dictionaries and MetaData section.\n",
    "- Trip data will be published monthly (with two months delay) instead of bi-annually.\n",
    "- HVFHV files will now include 17 more columns (please see High Volume FHV Trips Dictionary for details). Additional columns will be added to the old files as well. The earliest date to include additional columns: February 2019.\n",
    "- Yellow trip data will now include 1 additional column (‘airport_fee’, please see Yellow Trips Dictionary for details). The additional column will be added to the old files as well. The earliest date to include the additional column: January 2011.\n",
    "\n",
    "\n",
    "**Download the data for January and February 2023**\n",
    "\n",
    "Dataset: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "\n",
    "**Data Dictionaries and MetaData**\n",
    "\n",
    "- We'll use the same `NYC taxi dataset`, but instead of \"Yellow Taxi Trip Records\", we'll use `\"Green Taxi Trip Records\"`.\n",
    "\n",
    "> `Green Trips Data Dictionary`: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b74ab3",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "The goal of this homework is to familiarize users with workflow orchestration. \n",
    "\n",
    "Start with the orchestrate.py file in the 03-orchestration/3.4 folder\n",
    "of the course repo: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py<br>\n",
    "\n",
    "Questions: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2023/03-orchestration/homework.md\n",
    "\n",
    "- https://sagarthacker.com/posts/mlops/intro_workflow_orchestration.html\n",
    "- https://sagarthacker.com/posts/mlops/prefect-blocks.html\n",
    "- https://sagarthacker.com/posts/mlops/prefect-deployment.html\n",
    "\n",
    "\n",
    "**Table of Content**\n",
    "\n",
    "\n",
    "1. Import Libraries and Ingest Data\n",
    "    - Q1. Human-readable name<br>    \n",
    "2. Recognizing and Understanding Data\n",
    "    - Q2. Cron<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78138c11",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>1. Import Libraries & Ingest Data</strong></h1>   \n",
    "</div>\n",
    "\n",
    "**pip freeze**\n",
    "\n",
    "- https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "```\n",
    "- command1 & command2  # runs simultaneously\n",
    "- command1 ; command2  # runs sequentially\n",
    "- command1 && command2 # runs sequentially, runs command2 only if command1 succeeds\n",
    "- command1 || command2 # runs sequentially, runs command2 only if command1 fails\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30ff0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat /etc/os-release\n",
    "# !grep -E -w 'VERSION|NAME|PRETTY_NAME' /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f916a6f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # %%capture cap --no-stderr  # capture outputs  # cap.show()\n",
    "\n",
    "# !conda create --name \"exp-tracking-env-py39\" python=3.9  jupyter -y > outputs.txt && tail -12 outputs.txt\n",
    "\n",
    "# # run in termanal\n",
    "# # conda activate exp-tracking-env-py39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a54cd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check enviroment\n",
    "# !conda env list\n",
    "# !conda info -e\n",
    "# !conda info | grep 'active env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23feabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# Create a new directory for storing MLflow data\n",
    "os.makedirs('./data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74723240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/requirements.txt \n",
    "# To get started with MLflow you'll need to install the appropriate Python package.\n",
    "# for new env\n",
    "# jupyter==1.0.0\n",
    "\n",
    "black==23.3.0          # code style\n",
    "pandas==2.0.2\n",
    "fastparquet==2023.4.0\n",
    "# pyarrow==11.0.0\n",
    "orjson==3.8.1          # orjson is a fast, correct JSON library\n",
    "seaborn==0.12.2\n",
    "scikit-learn==1.2.2\n",
    "xgboost==1.7.5\n",
    "hyperopt==0.2.7\n",
    "\n",
    "# MLOPS packages\n",
    "mlflow==2.3.2\n",
    "wandb==0.15.4\n",
    "prefect==2.10.13\n",
    "prefect-aws==0.3.4\n",
    "psycopg2-binary==2.9.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69a8c258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python  : 3.9.16 (main, Mar  8 2023, 14:00:05) \n",
      "[GCC 11.2.0]\n",
      "Platform: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Actv Env: base\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform\n",
    "print(\"Python  :\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Actv Env:\", os.environ['CONDA_DEFAULT_ENV'])\n",
    "\n",
    "!{sys.executable} -m pip install -r ./data/requirements.txt -Uq #  --no-cache-dir --force-reinstall --no-deps --no-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c9080",
   "metadata": {},
   "source": [
    "for errors:\n",
    "- https://www.datasciencelearner.com/importerror-cannot-import-name-escape-from-jinja2-solved/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4477d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c dsdale24 pyqt5 -y\n",
    "# !conda install -c conda-forge pyqtwebengine -y\n",
    "\n",
    "# !conda install -c anaconda sqlalchemy -y\n",
    "\n",
    "# !conda install -c anaconda boto3 -y --force-reinstall\n",
    "\n",
    "# !conda install -c anaconda flask -y\n",
    "# !conda install -c anaconda jinja2==3.0.3 -y\n",
    "\n",
    "# conda install -c conda-forge google-auth -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81ffeeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import stats\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "import mlflow\n",
    "import pickle\n",
    "import pathlib\n",
    "import argparse\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "tqdm._instances.clear()\n",
    "from prefect import flow, task\n",
    "\n",
    "# memory management performs garbage collection \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb564b98",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>2. Recognizing and Understanding Data</strong></h1>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5b120",
   "metadata": {},
   "source": [
    "## Ingest Data [wget](https://linuxways.net/centos/linux-wget-command-with-examples/) or [curl](https://daniel.haxx.se/blog/2020/09/10/store-the-curl-output-over-there/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e2a3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Green Taxi Trip Records\" Download the data for January, February and March 2022\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "044ce591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(f'./data/*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41e352",
   "metadata": {},
   "source": [
    "## Q1. Human-readable name\n",
    "\n",
    "You’d like to give the first task, `read_data` a nicely formatted name.\n",
    "How can you specify a task name?\n",
    "\n",
    "> Hint: look in the docs at https://docs.prefect.io or \n",
    "> check out the doc string in a code editor.\n",
    "\n",
    "- `@task(retries=3, retry_delay_seconds=2, name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task_name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task-name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task_name_function=lambda x: f\"Read taxi data\")`\n",
    "\n",
    "**How can you specify a task name?**\n",
    "- https://docs.prefect.io/2.10.13/concepts/tasks/\n",
    "- @task(retries=3, retry_delay_seconds=2, name=\"Read taxi data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fdfe95",
   "metadata": {},
   "source": [
    "## Q2. Cron\n",
    "\n",
    "Cron is a common scheduling specification for workflows. \n",
    "\n",
    "Using the flow in `orchestrate.py`, create a deployment.\n",
    "Schedule your deployment to run on the third day of every month at 9am UTC.\n",
    "\n",
    "**What’s the cron schedule for that?**\n",
    "\n",
    "components of the cron expression:\n",
    "\n",
    "- The first field, 0, represents the minute of the hour. In this case, it is set to 0, meaning the deployment will run at the start of the hour.\n",
    "- The second field, 9, represents the hour of the day. It is set to 9, indicating that the deployment will run at 9am.\n",
    "- The third field, 3, represents the day of the month. This field is set to 3, which means the deployment will run specifically on the third day of each month.\n",
    "- The fourth field, *, represents the month. It is set to *, indicating that the deployment will run every month.\n",
    "- The fifth field, *, represents the day of the week. It is also set to *, meaning that the deployment will run regardless of the day of the week.\n",
    "\n",
    "Therefore, the cron schedule for running the deployment on the third day of every month at 9am UTC is:\n",
    "- `0 9 3 * *`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc362bb",
   "metadata": {},
   "source": [
    "## Q3. RMSE \n",
    "\n",
    "Download the January 2023 Green Taxi data and use it for your training data.\n",
    "Download the February 2023 Green Taxi data and use it for your validation data. \n",
    "\n",
    "Make sure you upload the data to GitHub so it is available for your deployment.\n",
    "\n",
    "Create a custom flow run of your deployment from the UI. Choose Custom\n",
    "Run for the flow and enter the file path as a string on the JSON tab under Parameters.\n",
    "\n",
    "Make sure you have a worker running and polling the correct work pool.\n",
    "\n",
    "View the results in the UI.\n",
    "\n",
    "**What’s the final RMSE to five decimal places?**\n",
    "\n",
    "mlflow-remote code:\n",
    "- https://www.mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded\n",
    "- Run Terminal: \n",
    "\n",
    "```\n",
    "# if using port 5000 or use another port\n",
    "kill $(lsof -ti :5000)   # clear port 5000\n",
    "\n",
    "# after set_experiment (building mlflow.db), go to bash cd path to (mlruns ant mlflow.db) folder\n",
    "mlflow ui \\\n",
    "    --backend-store-uri sqlite:///mlflow.db \\\n",
    "    --default-artifact-root  file:mlruns \\\n",
    "    --host localhost --port 5000   \n",
    "```\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9def2209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/orchestrate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/orchestrate.py\n",
    "\n",
    "# Source: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py\n",
    "\n",
    "import os\n",
    "import click\n",
    "import pickle\n",
    "import pathlib\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "from glob import glob\n",
    "from datetime import timedelta\n",
    "from prefect import flow, task\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "\n",
    "\n",
    "@task(retries=3, log_prints=True, name=\"Fetch Data\", \n",
    "      cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\n",
    "def fetch_data(raw_data_path: str, year: int, month: int, color: str) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0>2}.parquet'\n",
    "    os.makedirs(raw_data_path, exist_ok=True)  \n",
    "    os.system(f\"wget -q -N -P {raw_data_path} {url}\")\n",
    "    return None\n",
    "    \n",
    "    \n",
    "@task(retries=3, retry_delay_seconds=2, name=\"Read taxi data\")\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime  = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration    = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical     = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@task\n",
    "def add_features(\n",
    "    df_train: pd.DataFrame, df_val: pd.DataFrame, df_test: pd.DataFrame\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    df_train[\"PU_DO\"] = df_train[\"PULocationID\"] + \"_\" + df_train[\"DOLocationID\"]\n",
    "    df_val[\"PU_DO\"]   = df_val[\"PULocationID\"]   + \"_\" + df_val[\"DOLocationID\"]\n",
    "    df_test[\"PU_DO\"]  = df_test[\"PULocationID\"]  + \"_\" + df_test[\"DOLocationID\"]\n",
    "\n",
    "    categorical = [\"PU_DO\"]  #'PULocationID', 'DOLocationID']\n",
    "    numerical   = [\"trip_distance\"]\n",
    "\n",
    "    dv = DictVectorizer()\n",
    "\n",
    "    train_dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_train     = dv.fit_transform(train_dicts)\n",
    "    y_train     = df_train[\"duration\"].values\n",
    "\n",
    "    val_dicts   = df_val[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_val       = dv.transform(val_dicts)\n",
    "    y_val       = df_val[\"duration\"].values\n",
    "    \n",
    "    test_dicts  = df_test[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_test      = dv.transform(test_dicts)\n",
    "    y_test      = df_test[\"duration\"].values\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), dv\n",
    "\n",
    "\n",
    "@task(name=\"Train Best Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train  : scipy.sparse._csr.csr_matrix,\n",
    "    X_val    : scipy.sparse._csr.csr_matrix,\n",
    "    y_train  : np.ndarray,\n",
    "    y_val    : np.ndarray,\n",
    "    dv       : sklearn.feature_extraction.DictVectorizer,\n",
    "    dest_path: str,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"        \n",
    "    # Load train and test Data\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        # mlflow.log_param(\"train-data-path\", f'./data/green_tripdata_2022-01.parquet')\n",
    "        # mlflow.log_param(\"valid-data-path\", f'./data/green_tripdata_2022-02.parquet')\n",
    "\n",
    "        # Set Model params information\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "#             'objective': 'reg:squarederror',          # deprecated  \"reg:linear\"\n",
    "            'objective': \"reg:linear\",\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Build Model   \n",
    "        booster = xgb.train(\n",
    "            params               = best_params,\n",
    "            dtrain               = train,\n",
    "            num_boost_round      = 100,\n",
    "            evals                = [(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )   \n",
    "        \n",
    "        # Set Model Evaluation Metric\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse   = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)       \n",
    "\n",
    "        # Log Model two options\n",
    "        # Option1: Just log model\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")        \n",
    "        \n",
    "        # Option 2: save Model, Optional: Preprocessor or Pipeline         \n",
    "        # Create dest_path folder unless it already exists\n",
    "        # pathlib.Path(dest_path).mkdir(exist_ok=True) \n",
    "        os.makedirs(dest_path, exist_ok=True)       \n",
    "        local_file = os.path.join(dest_path, \"preprocessor.b\")\n",
    "        with open(local_file, \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        # whole proccess like pickle, saved Model, Optional: Preprocessor or Pipeline\n",
    "        mlflow.log_artifact(local_path = local_file, artifact_path=\"preprocessor\")        \n",
    "        \n",
    "        # print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "    return None\n",
    "\n",
    "\n",
    "@flow(name=\"Subflow - Download Data\", log_prints=True)\n",
    "def download_data(raw_data_path: str, years: list, months: list, colors: list):\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for color in colors:\n",
    "                fetch_data(raw_data_path, year, month, color)\n",
    "                \n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--raw_data_path\",\n",
    "    default=\"./data\",\n",
    "    help=\"Location where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--dest_path\",\n",
    "    default=\"./models\",\n",
    "    help=\"Location where the resulting model files will be saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--years\",\n",
    "    default=\"2023\",\n",
    "    help=\"Years where the raw NYC taxi trip data was saved (space-separated)\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--months\",\n",
    "    default=\"1 2 3\",\n",
    "    help=\"Months where the raw NYC taxi trip data was saved (space-separated)\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--colors\",\n",
    "    default=\"green yellow\",\n",
    "    help=\"Colors where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(raw_data_path: str, dest_path: str, years: str, months: str, colors: str) -> None:\n",
    "    \"\"\"The main training pipeline\"\"\"\n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Build or Connect mlflow experiment\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "    \n",
    "    # Download data    \n",
    "    years  = [int(year) for year in years.split()]\n",
    "    months = [int(month) for month in months.split()]\n",
    "    colors = colors.split()[:1]\n",
    "    download_data(raw_data_path, years, months, colors)\n",
    "    \n",
    "    # list parquet files\n",
    "    # print(sorted(glob(f'{raw_data_path}/green*.parquet')))\n",
    "    train_path, val_path, test_path = sorted(glob(f'{raw_data_path}/*.parquet'))\n",
    "\n",
    "    # Read parquet files\n",
    "    df_train = read_data(train_path)\n",
    "    df_val   = read_data(val_path)\n",
    "    df_test  = read_data(test_path)\n",
    "    # print(df_train.shape, df_val.shape, df_test.shape, )\n",
    "\n",
    "    # Transform\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test), dv = add_features(df_train, df_val, df_test)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv, dest_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9d96482",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:42:58.854 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'wooden-mushroom'\u001b[0m for flow\u001b[1;35m 'Main Flow'\u001b[0m\n",
      "04:42:59.830 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Created subflow run\u001b[35m 'lavender-wasp'\u001b[0m for flow\u001b[1;35m 'Subflow - Download Data'\u001b[0m\n",
      "04:43:00.036 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'lavender-wasp'\u001b[0m - Created task run 'Fetch Data-0' for task 'Fetch Data'\n",
      "04:43:00.038 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'lavender-wasp'\u001b[0m - Executing 'Fetch Data-0' immediately...\n",
      "04:43:00.191 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-0' - Finished in state Cached(type=COMPLETED)\n",
      "04:43:00.267 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'lavender-wasp'\u001b[0m - Created task run 'Fetch Data-1' for task 'Fetch Data'\n",
      "04:43:00.268 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'lavender-wasp'\u001b[0m - Executing 'Fetch Data-1' immediately...\n",
      "04:43:00.492 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-1' - Finished in state Cached(type=COMPLETED)\n",
      "04:43:00.546 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'lavender-wasp'\u001b[0m - Created task run 'Fetch Data-2' for task 'Fetch Data'\n",
      "04:43:00.548 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'lavender-wasp'\u001b[0m - Executing 'Fetch Data-2' immediately...\n",
      "04:43:00.687 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-2' - Finished in state Cached(type=COMPLETED)\n",
      "04:43:00.821 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'lavender-wasp'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n",
      "04:43:00.896 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Created task run 'Read taxi data-0' for task 'Read taxi data'\n",
      "04:43:00.897 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Executing 'Read taxi data-0' immediately...\n",
      "04:43:01.630 | \u001b[36mINFO\u001b[0m    | Task run 'Read taxi data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "04:43:01.692 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Created task run 'Read taxi data-1' for task 'Read taxi data'\n",
      "04:43:01.693 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Executing 'Read taxi data-1' immediately...\n",
      "04:43:02.382 | \u001b[36mINFO\u001b[0m    | Task run 'Read taxi data-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "04:43:02.440 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Created task run 'Read taxi data-2' for task 'Read taxi data'\n",
      "04:43:02.442 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Executing 'Read taxi data-2' immediately...\n",
      "04:43:03.019 | \u001b[36mINFO\u001b[0m    | Task run 'Read taxi data-2' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "04:43:03.095 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Created task run 'add_features-0' for task 'add_features'\n",
      "04:43:03.097 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Executing 'add_features-0' immediately...\n",
      "04:43:04.317 | \u001b[36mINFO\u001b[0m    | Task run 'add_features-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "04:43:04.384 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Created task run 'Train Best Model-0' for task 'Train Best Model'\n",
      "04:43:04.385 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Executing 'Train Best Model-0' immediately...\n",
      "[04:43:06] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "04:43:06.910 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [0]    validation-rmse:15.01627\n",
      "04:43:07.207 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [1]    validation-rmse:13.77591\n",
      "04:43:07.605 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [2]    validation-rmse:12.66953\n",
      "04:43:07.898 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [3]    validation-rmse:11.68972\n",
      "04:43:08.225 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [4]    validation-rmse:10.81927\n",
      "04:43:08.695 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [5]    validation-rmse:10.05352\n",
      "04:43:09.061 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [6]    validation-rmse:9.37638\n",
      "04:43:09.366 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [7]    validation-rmse:8.78514\n",
      "04:43:09.690 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [8]    validation-rmse:8.26684\n",
      "04:43:09.943 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [9]    validation-rmse:7.81889\n",
      "04:43:10.364 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [10]   validation-rmse:7.42994\n",
      "04:43:11.291 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [11]   validation-rmse:7.09348\n",
      "04:43:11.764 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [12]   validation-rmse:6.80836\n",
      "04:43:12.474 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [13]   validation-rmse:6.56211\n",
      "04:43:12.929 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [14]   validation-rmse:6.35332\n",
      "04:43:13.129 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [15]   validation-rmse:6.17838\n",
      "04:43:13.359 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [16]   validation-rmse:6.02763\n",
      "04:43:13.569 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [17]   validation-rmse:5.90232\n",
      "04:43:13.954 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [18]   validation-rmse:5.79691\n",
      "04:43:14.321 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [19]   validation-rmse:5.70817\n",
      "04:43:14.531 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [20]   validation-rmse:5.63182\n",
      "04:43:14.853 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [21]   validation-rmse:5.56852\n",
      "04:43:15.177 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [22]   validation-rmse:5.51520\n",
      "04:43:15.433 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [23]   validation-rmse:5.47115\n",
      "04:43:15.840 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [24]   validation-rmse:5.43421\n",
      "04:43:16.215 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [25]   validation-rmse:5.40206\n",
      "04:43:16.463 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [26]   validation-rmse:5.37653\n",
      "04:43:16.850 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [27]   validation-rmse:5.35308\n",
      "04:43:17.058 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [28]   validation-rmse:5.33437\n",
      "04:43:17.259 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [29]   validation-rmse:5.31842\n",
      "04:43:17.467 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [30]   validation-rmse:5.30434\n",
      "04:43:17.650 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [31]   validation-rmse:5.29293\n",
      "04:43:17.850 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [32]   validation-rmse:5.28270\n",
      "04:43:18.057 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [33]   validation-rmse:5.27307\n",
      "04:43:18.298 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [34]   validation-rmse:5.26638\n",
      "04:43:18.521 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [35]   validation-rmse:5.26043\n",
      "04:43:18.692 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [36]   validation-rmse:5.25512\n",
      "04:43:19.084 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [37]   validation-rmse:5.25088\n",
      "04:43:19.283 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [38]   validation-rmse:5.24664\n",
      "04:43:19.657 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [39]   validation-rmse:5.24209\n",
      "04:43:20.183 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [40]   validation-rmse:5.23909\n",
      "04:43:20.586 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [41]   validation-rmse:5.23607\n",
      "04:43:20.983 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [42]   validation-rmse:5.23344\n",
      "04:43:21.174 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [43]   validation-rmse:5.23200\n",
      "04:43:21.350 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [44]   validation-rmse:5.23097\n",
      "04:43:21.548 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [45]   validation-rmse:5.22865\n",
      "04:43:21.728 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [46]   validation-rmse:5.22775\n",
      "04:43:21.860 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [47]   validation-rmse:5.22687\n",
      "04:43:22.034 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [48]   validation-rmse:5.22582\n",
      "04:43:22.340 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [49]   validation-rmse:5.22487\n",
      "04:43:22.560 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [50]   validation-rmse:5.22442\n",
      "04:43:22.749 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [51]   validation-rmse:5.22332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:43:22.922 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [52]   validation-rmse:5.22236\n",
      "04:43:23.063 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [53]   validation-rmse:5.22142\n",
      "04:43:23.205 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [54]   validation-rmse:5.22022\n",
      "04:43:23.443 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [55]   validation-rmse:5.21987\n",
      "04:43:23.590 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [56]   validation-rmse:5.21908\n",
      "04:43:23.743 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [57]   validation-rmse:5.21873\n",
      "04:43:23.868 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [58]   validation-rmse:5.21820\n",
      "04:43:24.025 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [59]   validation-rmse:5.21787\n",
      "04:43:24.168 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [60]   validation-rmse:5.21702\n",
      "04:43:24.320 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [61]   validation-rmse:5.21632\n",
      "04:43:24.499 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [62]   validation-rmse:5.21571\n",
      "04:43:24.626 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [63]   validation-rmse:5.21404\n",
      "04:43:24.776 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [64]   validation-rmse:5.21348\n",
      "04:43:24.916 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [65]   validation-rmse:5.21290\n",
      "04:43:25.083 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [66]   validation-rmse:5.21262\n",
      "04:43:25.252 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [67]   validation-rmse:5.21181\n",
      "04:43:25.387 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [68]   validation-rmse:5.21179\n",
      "04:43:25.550 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [69]   validation-rmse:5.21113\n",
      "04:43:25.696 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [70]   validation-rmse:5.21051\n",
      "04:43:25.859 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [71]   validation-rmse:5.21000\n",
      "04:43:26.018 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [72]   validation-rmse:5.20944\n",
      "04:43:26.139 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [73]   validation-rmse:5.20928\n",
      "04:43:26.294 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [74]   validation-rmse:5.20890\n",
      "04:43:26.439 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [75]   validation-rmse:5.20840\n",
      "04:43:26.592 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [76]   validation-rmse:5.20723\n",
      "04:43:26.712 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [77]   validation-rmse:5.20677\n",
      "04:43:26.919 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [78]   validation-rmse:5.20653\n",
      "04:43:27.187 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [79]   validation-rmse:5.20608\n",
      "04:43:27.341 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [80]   validation-rmse:5.20599\n",
      "04:43:27.472 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [81]   validation-rmse:5.20558\n",
      "04:43:27.630 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [82]   validation-rmse:5.20516\n",
      "04:43:27.906 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [83]   validation-rmse:5.20428\n",
      "04:43:28.097 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [84]   validation-rmse:5.20377\n",
      "04:43:28.252 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [85]   validation-rmse:5.20323\n",
      "04:43:28.511 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [86]   validation-rmse:5.20281\n",
      "04:43:28.688 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [87]   validation-rmse:5.20294\n",
      "04:43:28.843 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [88]   validation-rmse:5.20271\n",
      "04:43:28.993 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [89]   validation-rmse:5.20245\n",
      "04:43:29.134 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [90]   validation-rmse:5.20201\n",
      "04:43:29.295 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [91]   validation-rmse:5.20186\n",
      "04:43:29.449 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [92]   validation-rmse:5.20146\n",
      "04:43:29.577 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [93]   validation-rmse:5.20144\n",
      "04:43:29.831 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [94]   validation-rmse:5.20096\n",
      "04:43:29.959 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [95]   validation-rmse:5.20087\n",
      "04:43:30.111 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [96]   validation-rmse:5.20016\n",
      "04:43:30.272 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [97]   validation-rmse:5.19983\n",
      "04:43:30.430 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [98]   validation-rmse:5.19931\n",
      "04:43:30.585 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [99]   validation-rmse:5.19931\n",
      "04:43:36.167 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "04:43:36.253 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'wooden-mushroom'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "# raw_data_path\n",
    "# DATA_PATH = f\"./data\"\n",
    "\n",
    "!python ./data/orchestrate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada2c5e",
   "metadata": {},
   "source": [
    "## Q4. RMSE (Markdown Artifact)\n",
    "\n",
    "Download the February 2023 Green Taxi data and use it for your training data.\n",
    "Download the March 2023 Green Taxi data and use it for your validation data. \n",
    "\n",
    "Create a Prefect Markdown artifact that displays the RMSE for the validation data.\n",
    "Create a deployment and run it.\n",
    "\n",
    "**What’s the RMSE in the artifact to two decimal places ?**\n",
    "```sh\n",
    "prefect server start\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e318968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/orchestrate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/orchestrate.py\n",
    "\n",
    "# Source: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py\n",
    "\n",
    "import os\n",
    "import click\n",
    "import pickle\n",
    "import pathlib\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "from glob import glob\n",
    "from datetime import timedelta\n",
    "from prefect import task, flow, Flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "\n",
    "\n",
    "@task(retries=3, log_prints=True, name=\"Fetch Data\", \n",
    "      cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\n",
    "def fetch_data(raw_data_path: str, year: int, month: int, color: str) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0>2}.parquet'\n",
    "    os.makedirs(raw_data_path, exist_ok=True)  \n",
    "    os.system(f\"wget -q -N -P {raw_data_path} {url}\")\n",
    "    return None\n",
    "    \n",
    "    \n",
    "@task(retries=3, retry_delay_seconds=2, name=\"Read taxi data\")\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime  = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration    = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical     = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@task\n",
    "def add_features(\n",
    "    df_train: pd.DataFrame, df_val: pd.DataFrame, df_test: pd.DataFrame\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    df_train[\"PU_DO\"] = df_train[\"PULocationID\"] + \"_\" + df_train[\"DOLocationID\"]\n",
    "    df_val[\"PU_DO\"]   = df_val[\"PULocationID\"]   + \"_\" + df_val[\"DOLocationID\"]\n",
    "    df_test[\"PU_DO\"]  = df_test[\"PULocationID\"]  + \"_\" + df_test[\"DOLocationID\"]\n",
    "\n",
    "    categorical = [\"PU_DO\"]  #'PULocationID', 'DOLocationID']\n",
    "    numerical   = [\"trip_distance\"]\n",
    "\n",
    "    dv = DictVectorizer()\n",
    "\n",
    "    train_dicts = df_train[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_train     = dv.fit_transform(train_dicts)\n",
    "    y_train     = df_train[\"duration\"].values\n",
    "\n",
    "    val_dicts   = df_val[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_val       = dv.transform(val_dicts)\n",
    "    y_val       = df_val[\"duration\"].values\n",
    "    \n",
    "    test_dicts  = df_test[categorical + numerical].to_dict(orient=\"records\")\n",
    "    X_test      = dv.transform(test_dicts)\n",
    "    y_test      = df_test[\"duration\"].values\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), dv\n",
    "\n",
    "\n",
    "@task(name=\"Train Best Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train  : scipy.sparse._csr.csr_matrix,\n",
    "    X_val    : scipy.sparse._csr.csr_matrix,\n",
    "    y_train  : np.ndarray,\n",
    "    y_val    : np.ndarray,\n",
    "    dv       : sklearn.feature_extraction.DictVectorizer,\n",
    "    dest_path: str,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"        \n",
    "    # Load train and test Data\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "        mlflow.set_tag(\"train-data-path\", f'./data/green_tripdata_2022-01.parquet')\n",
    "        mlflow.set_tag(\"valid-data-path\", f'./data/green_tripdata_2022-02.parquet')\n",
    "\n",
    "        # Set Model params information\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "#             'objective': 'reg:squarederror',          # deprecated  \"reg:linear\"\n",
    "            'objective': \"reg:linear\",\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Build Model   \n",
    "        booster = xgb.train(\n",
    "            params               = best_params,\n",
    "            dtrain               = train,\n",
    "            num_boost_round      = 100,\n",
    "            evals                = [(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )   \n",
    "        \n",
    "        # Set Model Evaluation Metric\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse   = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)   \n",
    "\n",
    "        # Create markdown artifact with RMSE value\n",
    "        markdown_report = f\"\"\"# RMSE for Validation Data\\n\\nRMSE: {rmse}\"\"\"        \n",
    "        create_markdown_artifact(\n",
    "            key=\"gtm-report\",\n",
    "            markdown=markdown_report,\n",
    "            description=\"RMSE for Validation Data Report\",\n",
    "        )\n",
    "        \n",
    "        # Log Model two options\n",
    "        # Option1: Just log model\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")        \n",
    "        \n",
    "        # Option 2: save Model, Optional: Preprocessor or Pipeline         \n",
    "        # Create dest_path folder unless it already exists\n",
    "        # pathlib.Path(dest_path).mkdir(exist_ok=True) \n",
    "        os.makedirs(dest_path, exist_ok=True)       \n",
    "        local_file = os.path.join(dest_path, \"preprocessor.b\")\n",
    "        with open(local_file, \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        # whole proccess like pickle, saved Model, Optional: Preprocessor or Pipeline\n",
    "        mlflow.log_artifact(local_path = local_file, artifact_path=\"preprocessor\")        \n",
    "        \n",
    "        # print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "    return None\n",
    "\n",
    "\n",
    "@flow(name=\"Subflow - Download Data\", log_prints=True)\n",
    "def download_data(raw_data_path: str, years: list, months: list, colors: list):\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for color in colors:\n",
    "                fetch_data(raw_data_path, year, month, color)\n",
    "                \n",
    "                \n",
    "from prefect import flow\n",
    "from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "\n",
    "@flow(name=\"Email Server Crenditals\", log_prints=True)\n",
    "def example_email_send_message_flow(email_addresses: list[str]):\n",
    "    email_server_credentials = EmailServerCredentials.load(\"email-server-credentials\")\n",
    "    \n",
    "    for email_address in email_addresses:\n",
    "        subject = email_send_message.with_options(name=f\"email {email_address}\").submit(\n",
    "            email_server_credentials=email_server_credentials,\n",
    "            subject=\"Example Flow Notification using Gmail\",\n",
    "            msg=\"This proves email_send_message works!\",\n",
    "            email_to=email_address,\n",
    "        )\n",
    "        \n",
    "        \n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--raw_data_path\",\n",
    "    default=\"./data\",\n",
    "    help=\"Location where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--dest_path\",\n",
    "    default=\"./models\",\n",
    "    help=\"Location where the resulting model files will be saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--years\",\n",
    "    default=\"2023\",\n",
    "    help=\"Years where the raw NYC taxi trip data was saved (space-separated)\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--months\",\n",
    "    default=\"1 2 3\",\n",
    "    help=\"Months where the raw NYC taxi trip data was saved (space-separated)\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--colors\",\n",
    "    default=\"green yellow\",\n",
    "    help=\"Colors where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(raw_data_path: str, dest_path: str, years: str, months: str, colors: str) -> None:\n",
    "    \"\"\"The main training pipeline\"\"\"\n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Build or Connect mlflow experiment\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "    \n",
    "    # Download data    \n",
    "    years  = [int(year) for year in years.split()]\n",
    "    months = [int(month) for month in months.split()]\n",
    "    colors = colors.split()[:1]\n",
    "    download_data(raw_data_path, years, months, colors)\n",
    "    \n",
    "    # list parquet files\n",
    "    # print(sorted(glob(f'{raw_data_path}/green*.parquet')))\n",
    "    test_path, train_path, val_path = sorted(glob(f'{raw_data_path}/*.parquet'))\n",
    "\n",
    "    # Read parquet files\n",
    "    df_train = read_data(train_path)\n",
    "    df_val   = read_data(val_path)\n",
    "    df_test  = read_data(test_path)\n",
    "    # print(df_train.shape, df_val.shape, df_test.shape, )\n",
    "\n",
    "    # Transform\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test), dv = add_features(df_train, df_val, df_test)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv, dest_path)\n",
    "    \n",
    "    # Train\n",
    "    # example_email_send_message_flow(['@gmail.com'])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_flow()\n",
    "    \n",
    "\n",
    "# from prefect.deployments import Deployment\n",
    "# from prefect.orion.schemas.schedules import CronSchedule\n",
    "# from prefect.flow_runners import SubprocessFlowRunner\n",
    "\n",
    "# Deployment(\n",
    "#     flow        = main_flow,\n",
    "#     name        = \"model_training\",\n",
    "#     schedule    = CronSchedule(cron=\"0 9 3 * *\"),\n",
    "#     flow_runner = SubprocessFlowRunner(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "877650ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:55:36.488 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'bright-degu'\u001b[0m for flow\u001b[1;35m 'Main Flow'\u001b[0m\n",
      "05:55:37.198 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Created subflow run\u001b[35m 'apricot-slug'\u001b[0m for flow\u001b[1;35m 'Subflow - Download Data'\u001b[0m\n",
      "05:55:37.474 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'apricot-slug'\u001b[0m - Created task run 'Fetch Data-0' for task 'Fetch Data'\n",
      "05:55:37.476 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'apricot-slug'\u001b[0m - Executing 'Fetch Data-0' immediately...\n",
      "05:55:37.622 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-0' - Finished in state Cached(type=COMPLETED)\n",
      "05:55:37.685 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'apricot-slug'\u001b[0m - Created task run 'Fetch Data-1' for task 'Fetch Data'\n",
      "05:55:37.686 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'apricot-slug'\u001b[0m - Executing 'Fetch Data-1' immediately...\n",
      "05:55:37.811 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-1' - Finished in state Cached(type=COMPLETED)\n",
      "05:55:37.869 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'apricot-slug'\u001b[0m - Created task run 'Fetch Data-2' for task 'Fetch Data'\n",
      "05:55:37.871 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'apricot-slug'\u001b[0m - Executing 'Fetch Data-2' immediately...\n",
      "05:55:38.124 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-2' - Finished in state Cached(type=COMPLETED)\n",
      "05:55:38.257 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'apricot-slug'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n",
      "05:55:38.334 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Created task run 'Read taxi data-0' for task 'Read taxi data'\n",
      "05:55:38.335 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Executing 'Read taxi data-0' immediately...\n",
      "05:55:39.045 | \u001b[36mINFO\u001b[0m    | Task run 'Read taxi data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "05:55:39.103 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Created task run 'Read taxi data-1' for task 'Read taxi data'\n",
      "05:55:39.104 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Executing 'Read taxi data-1' immediately...\n",
      "05:55:39.942 | \u001b[36mINFO\u001b[0m    | Task run 'Read taxi data-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "05:55:40.023 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Created task run 'Read taxi data-2' for task 'Read taxi data'\n",
      "05:55:40.025 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Executing 'Read taxi data-2' immediately...\n",
      "05:55:40.491 | \u001b[36mINFO\u001b[0m    | Task run 'Read taxi data-2' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "05:55:40.549 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Created task run 'add_features-0' for task 'add_features'\n",
      "05:55:40.551 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Executing 'add_features-0' immediately...\n",
      "05:55:41.915 | \u001b[36mINFO\u001b[0m    | Task run 'add_features-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "05:55:41.990 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Created task run 'Train Best Model-0' for task 'Train Best Model'\n",
      "05:55:41.992 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Executing 'Train Best Model-0' immediately...\n",
      "[05:55:44] WARNING: ../src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "05:55:46.030 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [0]    validation-rmse:15.33078\n",
      "05:55:46.530 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [1]    validation-rmse:14.09739\n",
      "05:55:46.744 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [2]    validation-rmse:12.99920\n",
      "05:55:46.950 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [3]    validation-rmse:12.02243\n",
      "05:55:47.141 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [4]    validation-rmse:11.15585\n",
      "05:55:47.340 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [5]    validation-rmse:10.39226\n",
      "05:55:47.538 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [6]    validation-rmse:9.71696\n",
      "05:55:47.734 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [7]    validation-rmse:9.12782\n",
      "05:55:47.989 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [8]    validation-rmse:8.60975\n",
      "05:55:48.231 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [9]    validation-rmse:8.15872\n",
      "05:55:48.545 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [10]   validation-rmse:7.76686\n",
      "05:55:48.802 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [11]   validation-rmse:7.42798\n",
      "05:55:49.086 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [12]   validation-rmse:7.13629\n",
      "05:55:49.592 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [13]   validation-rmse:6.88486\n",
      "05:55:49.785 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [14]   validation-rmse:6.67077\n",
      "05:55:50.146 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [15]   validation-rmse:6.48829\n",
      "05:55:50.994 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [16]   validation-rmse:6.33274\n",
      "05:55:51.254 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [17]   validation-rmse:6.19826\n",
      "05:55:51.454 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [18]   validation-rmse:6.08427\n",
      "05:55:51.829 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [19]   validation-rmse:5.98670\n",
      "05:55:51.990 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [20]   validation-rmse:5.90483\n",
      "05:55:52.257 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [21]   validation-rmse:5.83439\n",
      "05:55:52.406 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [22]   validation-rmse:5.77680\n",
      "05:55:52.525 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [23]   validation-rmse:5.72543\n",
      "05:55:52.687 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [24]   validation-rmse:5.68091\n",
      "05:55:52.838 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [25]   validation-rmse:5.64424\n",
      "05:55:52.981 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [26]   validation-rmse:5.61182\n",
      "05:55:53.129 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [27]   validation-rmse:5.58395\n",
      "05:55:53.266 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [28]   validation-rmse:5.56122\n",
      "05:55:53.388 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [29]   validation-rmse:5.54000\n",
      "05:55:53.548 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [30]   validation-rmse:5.52124\n",
      "05:55:53.780 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [31]   validation-rmse:5.50483\n",
      "05:55:53.904 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [32]   validation-rmse:5.49293\n",
      "05:55:54.055 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [33]   validation-rmse:5.48085\n",
      "05:55:54.405 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [34]   validation-rmse:5.47086\n",
      "05:55:54.778 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [35]   validation-rmse:5.46186\n",
      "05:55:54.935 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [36]   validation-rmse:5.45498\n",
      "05:55:55.371 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [37]   validation-rmse:5.44771\n",
      "05:55:55.663 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [38]   validation-rmse:5.44088\n",
      "05:55:55.896 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [39]   validation-rmse:5.43434\n",
      "05:55:56.031 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [40]   validation-rmse:5.43017\n",
      "05:55:56.185 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [41]   validation-rmse:5.42578\n",
      "05:55:56.531 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [42]   validation-rmse:5.42230\n",
      "05:55:56.809 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [43]   validation-rmse:5.41903\n",
      "05:55:57.024 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [44]   validation-rmse:5.41589\n",
      "05:55:57.231 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [45]   validation-rmse:5.41305\n",
      "05:55:57.373 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [46]   validation-rmse:5.41009\n",
      "05:55:57.476 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [47]   validation-rmse:5.40823\n",
      "05:55:57.621 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [48]   validation-rmse:5.40656\n",
      "05:55:57.761 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [49]   validation-rmse:5.40523\n",
      "05:55:57.911 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [50]   validation-rmse:5.40380\n",
      "05:55:58.019 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [51]   validation-rmse:5.40214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:55:58.155 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [52]   validation-rmse:5.40109\n",
      "05:55:58.253 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [53]   validation-rmse:5.40025\n",
      "05:55:58.369 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [54]   validation-rmse:5.39867\n",
      "05:55:58.471 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [55]   validation-rmse:5.39758\n",
      "05:55:58.596 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [56]   validation-rmse:5.39653\n",
      "05:55:58.796 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [57]   validation-rmse:5.39541\n",
      "05:55:58.900 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [58]   validation-rmse:5.39468\n",
      "05:55:59.011 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [59]   validation-rmse:5.39393\n",
      "05:55:59.114 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [60]   validation-rmse:5.39310\n",
      "05:55:59.217 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [61]   validation-rmse:5.39250\n",
      "05:55:59.320 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [62]   validation-rmse:5.39156\n",
      "05:55:59.439 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [63]   validation-rmse:5.39117\n",
      "05:55:59.539 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [64]   validation-rmse:5.39068\n",
      "05:55:59.660 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [65]   validation-rmse:5.38985\n",
      "05:55:59.784 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [66]   validation-rmse:5.38964\n",
      "05:55:59.910 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [67]   validation-rmse:5.38894\n",
      "05:56:00.008 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [68]   validation-rmse:5.38829\n",
      "05:56:00.231 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [69]   validation-rmse:5.38770\n",
      "05:56:00.346 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [70]   validation-rmse:5.38663\n",
      "05:56:00.466 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [71]   validation-rmse:5.38621\n",
      "05:56:00.576 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [72]   validation-rmse:5.38556\n",
      "05:56:00.685 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [73]   validation-rmse:5.38495\n",
      "05:56:01.015 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [74]   validation-rmse:5.38442\n",
      "05:56:01.191 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [75]   validation-rmse:5.38382\n",
      "05:56:01.305 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [76]   validation-rmse:5.38359\n",
      "05:56:01.407 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [77]   validation-rmse:5.38310\n",
      "05:56:01.520 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [78]   validation-rmse:5.38273\n",
      "05:56:01.622 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [79]   validation-rmse:5.38246\n",
      "05:56:01.733 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [80]   validation-rmse:5.38223\n",
      "05:56:01.854 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [81]   validation-rmse:5.38196\n",
      "05:56:01.982 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [82]   validation-rmse:5.38102\n",
      "05:56:02.101 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [83]   validation-rmse:5.38055\n",
      "05:56:02.219 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [84]   validation-rmse:5.38044\n",
      "05:56:02.321 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [85]   validation-rmse:5.38019\n",
      "05:56:02.436 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [86]   validation-rmse:5.37990\n",
      "05:56:02.547 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [87]   validation-rmse:5.37936\n",
      "05:56:02.649 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [88]   validation-rmse:5.37883\n",
      "05:56:02.764 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [89]   validation-rmse:5.37832\n",
      "05:56:02.881 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [90]   validation-rmse:5.37805\n",
      "05:56:03.085 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [91]   validation-rmse:5.37788\n",
      "05:56:03.329 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [92]   validation-rmse:5.37722\n",
      "05:56:03.491 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [93]   validation-rmse:5.37667\n",
      "05:56:03.633 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [94]   validation-rmse:5.37631\n",
      "05:56:03.740 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [95]   validation-rmse:5.37589\n",
      "05:56:03.860 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [96]   validation-rmse:5.37561\n",
      "05:56:03.988 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [97]   validation-rmse:5.37512\n",
      "05:56:04.093 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [98]   validation-rmse:5.37468\n",
      "05:56:04.206 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [99]   validation-rmse:5.37450\n",
      "05:56:10.426 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "05:56:10.661 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'bright-degu'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "!python ./data/orchestrate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af4195",
   "metadata": {},
   "source": [
    "```\n",
    "31f95cf7\n",
    "Flow run\n",
    "wonderful-bat\n",
    "Task run\n",
    "Train Best Model-0\n",
    "RMSE for Validation Data Report\n",
    "RMSE for Validation Data\n",
    "RMSE: 5.374495195206525\n",
    "\n",
    "\n",
    "dc032057\n",
    "Flow run\n",
    "interesting-cougar\n",
    "Task run\n",
    "Train Best Model-0\n",
    "Created gtm-report\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b663b",
   "metadata": {},
   "source": [
    "## Q5. Emails\n",
    "\n",
    "\n",
    "It’s often helpful to be notified when something with your dataflow doesn’t work\n",
    "as planned. Create an email notification for to use with your own Prefect server instance.\n",
    "In your virtual environment, install the prefect-email integration with \n",
    "\n",
    "```bash\n",
    "pip install prefect-email\n",
    "```\n",
    "\n",
    "Make sure you are connected to a running Prefect server instance through your\n",
    "Prefect profile.\n",
    "See the docs if needed: https://docs.prefect.io/latest/concepts/settings/#configuration-profiles\n",
    "\n",
    "Register the new block with your server with \n",
    "\n",
    "```bash\n",
    "prefect block register -m prefect_email\n",
    "```\n",
    "\n",
    "Remember that a block is a Prefect class with a nice UI form interface.\n",
    "Block objects live on the server and can be created and accessed in your Python code. \n",
    "\n",
    "See the docs for how to authenticate by saving your email credentials to\n",
    "a block and note that you will need an App Password to send emails with\n",
    "Gmail and other services. Follow the instructions in the docs.\n",
    "\n",
    "Create and save an `EmailServerCredentials` notification block.\n",
    "Use the credentials block to send an email.\n",
    "\n",
    "Test the notification functionality by running a deployment.\n",
    "\n",
    "**What is the name of the pre-built prefect-email task function?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9f5e654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSuccessfully registered 1 block\u001b[0m\r\n",
      "\r\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┓\r\n",
      "┃\u001b[1m \u001b[0m\u001b[1mRegistered Blocks       \u001b[0m\u001b[1m \u001b[0m┃\r\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━┩\r\n",
      "│ Email Server Credentials │\r\n",
      "└──────────────────────────┘\r\n",
      "\r\n",
      " To configure the newly registered blocks, go to the Blocks page in the Prefect \r\n",
      "UI.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!prefect block register -m prefect_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3e739f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Block.save at 0x7f513ae1f1c0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prefect_email import EmailServerCredentials\n",
    "\n",
    "credentials = EmailServerCredentials(\n",
    "    username=\"EMAIL-ADDRESS-PLACEHOLDER\",\n",
    "    password=\"PASSWORD-PLACEHOLDER\",  # must be an app password\n",
    ")\n",
    "credentials.save(\"BLOCK-NAME-PLACEHOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f32c95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Block.load at 0x7f513ae1f2c0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prefect_email import EmailServerCredentials\n",
    "\n",
    "EmailServerCredentials.load(\"BLOCK_NAME_PLACEHOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "41166db0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Send an email using Gmail\n",
    "from prefect import flow\n",
    "from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "@flow(name=\"Email Server Credentials\")\n",
    "def example_email_send_message_flow():\n",
    "    email_server_credentials = EmailServerCredentials(\n",
    "        username=\"your_email_address@gmail.com\",\n",
    "        password=\"MUST_be_an_app_password_here!\",\n",
    "    )\n",
    "    # email_server_credentials.save(\"BLOCK-NAME-PLACEHOLDER\")\n",
    "    # email_server_credentials = EmailServerCredentials.load(\"BLOCK_NAME_PLACEHOLDER\")\n",
    "    \n",
    "    subject = email_send_message(\n",
    "        email_server_credentials=email_server_credentials,\n",
    "        subject=\"Example Flow Notification using Gmail\",\n",
    "        msg=\"This proves email_send_message works!\",\n",
    "        email_to=\"someone_awesome@gmail.com\",\n",
    "    )\n",
    "    return subject\n",
    "\n",
    "# example_email_send_message_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64e3d3",
   "metadata": {},
   "source": [
    "## Q6. Prefect Cloud\n",
    "\n",
    "The hosted Prefect Cloud lets you avoid running your own Prefect server and\n",
    "has automations that allow you to get notifications when certain events occur\n",
    "or don’t occur. \n",
    "\n",
    "Create a free forever Prefect Cloud account at [app.prefect.cloud](https://app.prefect.cloud/) and connect\n",
    "your workspace to it following the steps in the UI when you sign up. \n",
    "\n",
    "Set up an Automation from the UI that will send yourself an email when\n",
    "a flow run completes. Run one of your existing deployments and check\n",
    "your email to see the notification.\n",
    "\n",
    "Make sure your active profile is pointing toward Prefect Cloud and\n",
    "make sure you have a worker active.\n",
    "\n",
    "**What is the name of the second step in the Automation creation process?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705914d",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "- The name of the second step in the Automation creation process in Prefect Cloud is \"Configure Triggers.\" This step allows you to define the conditions or events that will trigger the Automation to execute. In this case, you would configure the trigger to activate when a flow run completes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34682f",
   "metadata": {},
   "source": [
    "# End of The Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
