{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2525fb2f",
   "metadata": {},
   "source": [
    "<div style=\"align: center; margin: 0; padding: 0; height: 250px;\">\n",
    "    <br>\n",
    "    <img src=\"https://www.nyc.gov/assets/tlc/images/content/hero/MRP-Closing-Week.jpg\" style=\"display:block; margin:auto; width:65%; height:100%;\">\n",
    "</div><br><br> \n",
    "\n",
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "<!--   https://xkcd.com/color/rgb/   -->\n",
    "  <p style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>TLC Trip Record Data</strong></p>  \n",
    "  \n",
    "  <p style=\"text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:22px; font-weight:normal; text-transform: capitalize; padding: 5px;\"\n",
    "     >Machine Learning Module: PREFECT - Ride Duration Prediction<br>using Regression Analysis<br></p><br>\n",
    "    \n",
    "  <div style=\"align: center;\">\n",
    "  <table style=\"text-align: center; background-color: romance; color: Jaguar; border-radius: 10px; font-family: monospace;\n",
    "                  line-height:1.4; font-size: 21px; font-weight: normal; text-transform: capitalize; padding: 5px; \n",
    "                  margin: 0 auto;\">\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > PREFECT <span style=\"font-size: 16px;\">(framework for building, Deploying,<br>Scheduling, and Monitoring Data Pipelines and Workflows)</span></td></tr>\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > MLFLOW <span style=\"font-size: 16px;\">(Machine Learning Flow)</span></td></tr>\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > MLOps <span style=\"font-size: 16px;\">(CI/CD, Model Versioning, Monitoring, Automated Retraining,<br>Security, Scalability, Collaboration)</span></td></tr>\n",
    "  </table>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "- https://github.com/discdiver/prefect-mlops-zoomcamp\n",
    "- https://mlflow.org/docs/0.7.0/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8b00d",
   "metadata": {},
   "source": [
    "**Dataset Info**\n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "Yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
    "\n",
    "For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
    "\n",
    "\n",
    "**ATTENTION!**\n",
    "\n",
    "On 05/13/2022, we are making the following changes to trip record files:\n",
    "\n",
    "- All files will be stored in the PARQUET format. Please see the ‘Working With PARQUET Format’ under the Data Dictionaries and MetaData section.\n",
    "- Trip data will be published monthly (with two months delay) instead of bi-annually.\n",
    "- HVFHV files will now include 17 more columns (please see High Volume FHV Trips Dictionary for details). Additional columns will be added to the old files as well. The earliest date to include additional columns: February 2019.\n",
    "- Yellow trip data will now include 1 additional column (‘airport_fee’, please see Yellow Trips Dictionary for details). The additional column will be added to the old files as well. The earliest date to include the additional column: January 2011.\n",
    "\n",
    "\n",
    "**Download the data for January and February 2023**\n",
    "\n",
    "Dataset: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "\n",
    "**Data Dictionaries and MetaData**\n",
    "\n",
    "- We'll use the same `NYC taxi dataset`, but instead of \"Yellow Taxi Trip Records\", we'll use `\"Green Taxi Trip Records\"`.\n",
    "\n",
    "> `Green Trips Data Dictionary`: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b74ab3",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "The goal of this homework is to familiarize users with workflow orchestration. \n",
    "\n",
    "Start with the orchestrate.py file in the 03-orchestration/3.4 folder\n",
    "of the course repo: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py<br>\n",
    "\n",
    "Questions: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2023/03-orchestration/homework.md\n",
    "\n",
    "- https://sagarthacker.com/posts/mlops/intro_workflow_orchestration.html\n",
    "- https://sagarthacker.com/posts/mlops/prefect-blocks.html\n",
    "- https://sagarthacker.com/posts/mlops/prefect-deployment.html\n",
    "\n",
    "\n",
    "**Table of Content**\n",
    "\n",
    "\n",
    "1. Import Libraries and Ingest Data\n",
    "    - Q1. Human-readable name<br>    \n",
    "2. Recognizing and Understanding Data\n",
    "    - Q2. Cron<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78138c11",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>1. Import Libraries & Ingest Data</strong></h1>   \n",
    "</div>\n",
    "\n",
    "> ⚠️ Not Recommended conda `base` env, work on `venv`\n",
    "\n",
    "- https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "conda list -e > requirements.txt\n",
    "\n",
    "# new conda virtual environment\n",
    "conda create --name \"prefect-ops\" python=3.10 jupyter -y\n",
    "conda activate \"prefect-ops\"\n",
    "\n",
    "# install all package dependencies\n",
    "pip install -r requirements.txt\n",
    "conda install -c conda-forge --file=requirements.txt      # mostly not work\n",
    "conda install -c conda-forge pandas==2.0.2 -q -y\n",
    "\n",
    "# if The environment is inconsistent, try below\n",
    "conda update -n base -c defaults conda --force-reinstall\n",
    "conda install anaconda --force-reinstall\n",
    "\n",
    "```\n",
    "\n",
    "**You must use the `--no-deps` option in the pip install command in order to avoid bundling dependencies into your conda-package.**\n",
    "\n",
    "If you run pip install without the `--no-deps` option, pip will often install dependencies in your conda recipe and those dependencies will become part of your package. This wastes space in the package and `increases the risk of file overlap`, file clobbering, and broken packages.\n",
    "\n",
    "There might be cases where you want to install a package directly from a local directory or a specific location, without relying on the package indexes. In such situations, you can use the `--no-index` option to tell pip not to look for the package in any indexes.\n",
    "\n",
    "```\n",
    "- command1 & command2  # runs simultaneously\n",
    "- command1 ; command2  # runs sequentially\n",
    "- command1 && command2 # runs sequentially, runs command2 only if command1 succeeds\n",
    "- command1 || command2 # runs sequentially, runs command2 only if command1 fails\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ff0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture cap --no-stderr  # capture outputs  # cap.show()\n",
    "# !cat /etc/os-release\n",
    "# !grep -E -w 'VERSION|NAME|PRETTY_NAME' /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54cd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check enviroment\n",
    "# !conda env list\n",
    "# !conda info -e\n",
    "# !conda info | grep 'active env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74723240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt \n",
    "# To get started with MLflow you'll need to install the appropriate Python package.\n",
    "\n",
    "pandas==2.0.2\n",
    "orjson==3.9.1          # orjson is a fast, correct JSON library\n",
    "seaborn==0.12.2\n",
    "\n",
    "# ML Model packages\n",
    "scikit-learn==1.2.2\n",
    "xgboost==1.7.3\n",
    "\n",
    "# MLOPS packages\n",
    "mlflow==2.4.1\n",
    "wandb==0.15.4\n",
    "prefect==2.10.18\n",
    "prefect-email==0.2.2\n",
    "\n",
    "# MLOPS Cloud packages\n",
    "boto3==1.24.28\n",
    "prefect-aws==0.3.4\n",
    "\n",
    "# ML Model packages\n",
    "hyperopt==0.2.7\n",
    "\n",
    "# for parquet file\n",
    "pyarrow==11.0.0\n",
    "fastparquet==2023.4.0\n",
    "\n",
    "# Optionally\n",
    "# click==8.0.4\n",
    "black==23.3.0          # code style\n",
    "\n",
    "# Optionally\n",
    "jupyter\n",
    "ipykernel\n",
    "ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db08661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Python  : 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]\n",
      "Platform: Linux Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Actv Env: base\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, IPython.display\n",
    "\n",
    "# !{sys.executable} -m pip install -Uq -r requirements.txt  #  --no-deps --no-cache-dir --force-reinstall --no-index\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# IPython.display.clear_output()\n",
    "print(\"Python  :\", sys.version)\n",
    "print(\"Platform:\", platform.system(), platform.platform())\n",
    "print(\"Actv Env:\", os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ffeeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import stats\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import click\n",
    "import pickle\n",
    "# import pathlib\n",
    "# import argparse\n",
    "# import requests\n",
    "# import urllib.request\n",
    "from glob import glob\n",
    "\n",
    "# from tqdm import tqdm           # console-based\n",
    "# from tqdm.notebook import tqdm  # jupyter-based\n",
    "from tqdm.auto import tqdm        # automatically selects\n",
    "# tqdm._instances.clear()\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# import wandb\n",
    "import mlflow\n",
    "import prefect\n",
    "from prefect import task, flow, Flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# memory management performs garbage collection \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb564b98",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>2. Recognizing and Understanding Data</strong></h1>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5b120",
   "metadata": {},
   "source": [
    "## Ingest Data [wget](https://linuxways.net/centos/linux-wget-command-with-examples/) or [curl](https://daniel.haxx.se/blog/2020/09/10/store-the-curl-output-over-there/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2a3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Green Taxi Trip Records\" Download the data for January, February and March 2022\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044ce591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(f'./data/*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41e352",
   "metadata": {},
   "source": [
    "## Q1. Human-readable name\n",
    "\n",
    "You’d like to give the first task, `read_data` a nicely formatted name.\n",
    "How can you specify a task name?\n",
    "\n",
    "> Hint: look in the docs at https://docs.prefect.io or \n",
    "> check out the doc string in a code editor.\n",
    "\n",
    "- `@task(retries=3, retry_delay_seconds=2, name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task_name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task-name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task_name_function=lambda x: f\"Read taxi data\")`\n",
    "\n",
    "**How can you specify a task name?**\n",
    "- https://docs.prefect.io/2.10.13/concepts/tasks/\n",
    "- @task(retries=3, retry_delay_seconds=2, name=\"Read taxi data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159982ef",
   "metadata": {},
   "source": [
    "## Q2. Cron\n",
    "\n",
    "Cron is a common scheduling specification for workflows. \n",
    "\n",
    "Using the flow in `orchestrate.py`, create a deployment.\n",
    "Schedule your deployment to run on the third day of every month at 9am UTC.\n",
    "\n",
    "**What’s the cron schedule for that?**\n",
    "\n",
    "components of the cron expression:\n",
    "\n",
    "- The first field, 0, represents the minute of the hour. In this case, it is set to 0, meaning the deployment will run at the start of the hour.\n",
    "- The second field, 9, represents the hour of the day. It is set to 9, indicating that the deployment will run at 9am.\n",
    "- The third field, 3, represents the day of the month. This field is set to 3, which means the deployment will run specifically on the third day of each month.\n",
    "- The fourth field, *, represents the month. It is set to *, indicating that the deployment will run every month.\n",
    "- The fifth field, *, represents the day of the week. It is also set to *, meaning that the deployment will run regardless of the day of the week.\n",
    "\n",
    "Therefore, the cron schedule for running the deployment on the third day of every month at 9am UTC is:\n",
    "- `0 9 3 * *`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f5e35",
   "metadata": {},
   "source": [
    "## Q3. RMSE \n",
    "\n",
    "Download the January 2023 Green Taxi data and use it for your training data.\n",
    "Download the February 2023 Green Taxi data and use it for your validation data. \n",
    "\n",
    "Make sure you upload the data to GitHub so it is available for your deployment.\n",
    "\n",
    "Create a custom flow run of your deployment from the UI. Choose Custom\n",
    "Run for the flow and enter the file path as a string on the JSON tab under Parameters.\n",
    "\n",
    "Make sure you have a worker running and polling the correct work pool.\n",
    "\n",
    "View the results in the UI.\n",
    "\n",
    "**What’s the final RMSE to five decimal places?**\n",
    "\n",
    "mlflow-remote code:\n",
    "- https://www.mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded\n",
    "- Run Terminal: \n",
    "\n",
    "```\n",
    "# if using port 5000 or use another port\n",
    "kill $(lsof -ti :5000)   # clear port 5000\n",
    "\n",
    "# after set_experiment (building mlflow.db), go to bash cd path to (mlruns ant mlflow.db) folder\n",
    "mlflow ui \\\n",
    "    --backend-store-uri sqlite:///mlflow.db \\\n",
    "    --default-artifact-root  file:mlruns \\\n",
    "    --host localhost --port 5000   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae5287",
   "metadata": {},
   "source": [
    "<div style=\"height: 600px; margin: 0; padding: 0;\">\n",
    "  <img src=\"https://docs.prefect.io/latest/img/concepts/flow-deployment-end-to-end.png\" alt=\"flow-deployment\" style=\"width: 100%; height: 100%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Clone your Github Repo, then Check Data\n",
    "```sh\n",
    "git remote -v\n",
    "```\n",
    "\n",
    "**Step 1: In this task, the code will be modified and improved by adding an @flow decorator to the entrypoint function.**\n",
    "\n",
    "- First, modify the existing code by adding an @flow decorator to the code's entrypoint function. The @flow decorator helps in type checking and ensures more robust code.\n",
    "- Specify a name for the @flow decorator. For example, you can name it \"main\" if the entrypoint function is named \"main.\"\n",
    "- After adding the @flow decorator, make sure to save the modified code to a new directory. This ensures that you keep a copy of the enhanced code separately.\n",
    "- Now, continue to run the Prefect step 5, ensure you have Prefect installed on your system. If not, you can install it using pip:\n",
    "```py\n",
    "# @flow\n",
    "\n",
    "# Add your original code here\n",
    "# ...\n",
    "```\n",
    "- Now, to run the Prefect step 5, ensure you have Prefect installed on your system. If not, you can install it using pip:\n",
    "```sh\n",
    "pip install -U \"prefect==2.10.18\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23feabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# Create a new directory for storing MLflow data\n",
    "os.makedirs('./pycode', exist_ok=True)\n",
    "# os.makedirs('./data', exist_ok=True)\n",
    "# os.makedirs('./output', exist_ok=True)\n",
    "# os.makedirs('./models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ce59af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pycode/orchestrate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/orchestrate.py\n",
    "\n",
    "# Source: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py\n",
    "\n",
    "import os\n",
    "import click\n",
    "import pickle\n",
    "\n",
    "import pathlib\n",
    "import argparse\n",
    "import requests\n",
    "import urllib.request\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "import prefect\n",
    "from prefect import task, flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# from prefect_aws import S3Bucket\n",
    "# from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "\n",
    "\n",
    "@task(name=\"Fetch Data\", cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1),\n",
    "      retries=3, log_prints=True, )\n",
    "def fetch_data(raw_data_path: str, year: int, month: int, color: str) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "    os.makedirs(raw_data_path, exist_ok=True)  \n",
    "\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    url      = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0>2}.parquet'\n",
    "    filename = os.path.join(raw_data_path, f'{color}_tripdata_{year}-{month:0>2}.parquet')\n",
    "    # urllib.request.urlretrieve(url, filename)\n",
    "    # os.system(f\"wget -q -N -P {raw_data_path} {url}\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    return None\n",
    "\n",
    "\n",
    "@flow(name=\"Subflow Download Data\", log_prints=True)\n",
    "def download_data(raw_data_path: str, years: list, months: list, colors: list) -> None:\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for color in colors:\n",
    "                fetch_data(raw_data_path, year, month, color)\n",
    "    return None\n",
    "    \n",
    "    \n",
    "@task(name=\"Read Taxi Data\", retries=3, retry_delay_seconds=2, log_prints=None)\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime  = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration    = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical     = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(name=\"Preprocess: Add Features Taxi Data\", log_prints=True)\n",
    "def preprocess(\n",
    "    df: pd.DataFrame,dv: DictVectorizer = None, fit_dv: bool = False\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "    categorical = [\"PU_DO\"]\n",
    "    numerical   = ['trip_distance']\n",
    "    dicts       = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    if fit_dv:\n",
    "        # return sparse matrix\n",
    "        dv = DictVectorizer()\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "        \n",
    "    # Convert X the sparse matrix  to pandas DataFrame, but too slow\n",
    "    # X = pd.DataFrame(X.toarray(), columns=dv.get_feature_names_out())\n",
    "    # X = pd.DataFrame.sparse.from_spmatrix(X, columns=dv.get_feature_names_out())\n",
    "\n",
    "    try:\n",
    "        # Extract the target\n",
    "        target = 'duration'\n",
    "        y = df[target].values\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (X, y), dv\n",
    "\n",
    "\n",
    "@task(name=\"Train Best Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train  : scipy.sparse._csr.csr_matrix,\n",
    "    X_val    : scipy.sparse._csr.csr_matrix,\n",
    "    y_train  : np.ndarray,\n",
    "    y_val    : np.ndarray,\n",
    "    dv       : sklearn.feature_extraction.DictVectorizer,\n",
    "    raw_data_path: str,\n",
    "    dest_path: str,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"        \n",
    "    # Load train and test Data\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "        mlflow.set_tag(\"train-data-path\", f'{raw_data_path}/green_tripdata_2023-01.parquet')\n",
    "        mlflow.set_tag(\"valid-data-path\", f'{raw_data_path}/green_tripdata_2023-02.parquet')\n",
    "        mlflow.set_tag(\"test-data-path\",  f'{raw_data_path}/green_tripdata_2023-03.parquet') \n",
    "\n",
    "        # Set Model params information\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "            'objective': 'reg:squarederror',          # deprecated  \"reg:linear\"\n",
    "            # 'objective': \"reg:linear\",\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Build Model   \n",
    "        booster = xgb.train(\n",
    "            params               = best_params,\n",
    "            dtrain               = train,\n",
    "            num_boost_round      = 100,\n",
    "            evals                = [(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )   \n",
    "        \n",
    "        # Set Model Evaluation Metric\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse   = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)       \n",
    "\n",
    "        # Log Model two options\n",
    "        # Option1: Just log model\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")        \n",
    "        \n",
    "        # Option 2: save Model, Optional: Preprocessor or Pipeline         \n",
    "        # Create dest_path folder unless it already exists\n",
    "        # pathlib.Path(dest_path).mkdir(exist_ok=True) \n",
    "        os.makedirs(dest_path, exist_ok=True)       \n",
    "        local_file = os.path.join(dest_path, \"preprocessor.b\")\n",
    "        with open(local_file, \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        # whole proccess like pickle, saved Model, Optional: Preprocessor or Pipeline\n",
    "        mlflow.log_artifact(local_path = local_file, artifact_path=\"preprocessor\")        \n",
    "        \n",
    "        # print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "    return None\n",
    "                \n",
    "\n",
    "# click work on Local but it Gives ERRORS Deploy step\n",
    "@click.command()\n",
    "@click.option(\n",
    "    \"--raw_data_path\",\n",
    "    default=\"./data\",\n",
    "    help=\"Location where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--dest_path\",\n",
    "    default=\"./models\",\n",
    "    help=\"Location where the resulting model files will be saved\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--years\",\n",
    "    default=\"2023\",\n",
    "    help=\"Years where the raw NYC taxi trip data was saved (space-separated)\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--months\",\n",
    "    default=\"1 2 3 4\",\n",
    "    help=\"Months where the raw NYC taxi trip data was saved (space-separated)\"\n",
    ")\n",
    "@click.option(\n",
    "    \"--colors\",\n",
    "    default=\"green yellow\",\n",
    "    help=\"Colors where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(raw_data_path: str, dest_path: str, years: str, months: str, colors: str) -> None:\n",
    "    \"\"\"The main training pipeline\"\"\"\n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Build or Connect mlflow experiment\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "    \n",
    "    # Download data    \n",
    "    years  = [int(year) for year in years.split()]\n",
    "    months = [int(month) for month in months.split()]\n",
    "    colors = colors.split()[:1]\n",
    "    download_data(raw_data_path, years, months, colors)\n",
    "    # print(sorted(glob(f'{raw_data_path}/*')))\n",
    "    \n",
    "    # list parquet files\n",
    "    # print(sorted(glob(f'{raw_data_path}/green*.parquet')))\n",
    "    train_path, val_path, test_path = sorted(glob(f'{raw_data_path}/*.parquet'))[:3:]\n",
    "\n",
    "    # Read parquet files\n",
    "    df_train = read_data(train_path)\n",
    "    df_val   = read_data(val_path)\n",
    "    df_test  = read_data(test_path)\n",
    "    # print(df_train.shape, df_val.shape, df_test.shape, )    \n",
    "\n",
    "    # Fit the DictVectorizer and preprocess data\n",
    "    (X_train, y_train), dv = preprocess(df_train, fit_dv=True)\n",
    "    (X_val, y_val)    , _  = preprocess(df_val, dv, fit_dv=False)\n",
    "    (X_test, y_test)  , _  = preprocess(df_test, dv, fit_dv=False)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv, raw_data_path, dest_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # argparse work on Local but it Gives ERRORS Deploy step\n",
    "    # parser = argparse.ArgumentParser(description=\"Main Flow\")\n",
    "    # parser.add_argument(\"--raw_data_path\", default=\"./data\",       help=\"Location where the raw NYC taxi trip data was saved\")\n",
    "    # parser.add_argument(\"--dest_path\",     default=\"./models\",     help=\"Location where the resulting model files will be saved\")\n",
    "    # parser.add_argument(\"--years\",         default=\"2023\",         help=\"Years where the raw NYC taxi trip data was saved (space-separated)\")\n",
    "    # parser.add_argument(\"--months\",        default=\"1 2 3 4\",      help=\"Months where the raw NYC taxi trip data was saved (space-separated)\")\n",
    "    # parser.add_argument(\"--colors\",        default=\"green yellow\", help=\"Colors where the raw NYC taxi trip data was saved\")\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    # main_flow(args.raw_data_path, args.dest_path, args.years, args.months, args.colors)\n",
    "\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f85a3b6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/acer/anaconda3/lib/python3.10/contextlib.py:142: SAWarning: Skipped unsupported reflection of expression-based index ix_flow_run__coalesce_start_time_expected_start_time_desc\n",
      "  next(self.gen)\n",
      "/home/acer/anaconda3/lib/python3.10/contextlib.py:142: SAWarning: Skipped unsupported reflection of expression-based index ix_flow_run__coalesce_start_time_expected_start_time_asc\n",
      "  next(self.gen)\n",
      "19:49:12.853 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'grumpy-wasp'\u001b[0m for flow\u001b[1;35m 'Main Flow'\u001b[0m\n",
      "2023/07/04 19:49:13 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2023/07/04 19:49:13 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2023/07/04 19:49:17 INFO mlflow.tracking.fluent: Experiment with name 'nyc-taxi-experiment' does not exist. Creating a new experiment.\n",
      "19:49:17.386 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Created subflow run\u001b[35m 'psychedelic-macaw'\u001b[0m for flow\u001b[1;35m 'Subflow Download Data'\u001b[0m\n",
      "19:49:17.560 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'psychedelic-macaw'\u001b[0m - Created task run 'Fetch Data-0' for task 'Fetch Data'\n",
      "19:49:17.563 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'psychedelic-macaw'\u001b[0m - Executing 'Fetch Data-0' immediately...\n",
      "19:49:18.447 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:18.503 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'psychedelic-macaw'\u001b[0m - Created task run 'Fetch Data-1' for task 'Fetch Data'\n",
      "19:49:18.504 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'psychedelic-macaw'\u001b[0m - Executing 'Fetch Data-1' immediately...\n",
      "19:49:19.627 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:19.698 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'psychedelic-macaw'\u001b[0m - Created task run 'Fetch Data-2' for task 'Fetch Data'\n",
      "19:49:19.700 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'psychedelic-macaw'\u001b[0m - Executing 'Fetch Data-2' immediately...\n",
      "19:49:20.622 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-2' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:20.781 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'psychedelic-macaw'\u001b[0m - Created task run 'Fetch Data-3' for task 'Fetch Data'\n",
      "19:49:20.783 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'psychedelic-macaw'\u001b[0m - Executing 'Fetch Data-3' immediately...\n",
      "19:49:21.676 | \u001b[36mINFO\u001b[0m    | Task run 'Fetch Data-3' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:21.796 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'psychedelic-macaw'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n",
      "19:49:21.862 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Created task run 'Read Taxi Data-0' for task 'Read Taxi Data'\n",
      "19:49:21.863 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Executing 'Read Taxi Data-0' immediately...\n",
      "19:49:22.713 | \u001b[36mINFO\u001b[0m    | Task run 'Read Taxi Data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:22.768 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Created task run 'Read Taxi Data-1' for task 'Read Taxi Data'\n",
      "19:49:22.770 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Executing 'Read Taxi Data-1' immediately...\n",
      "19:49:23.282 | \u001b[36mINFO\u001b[0m    | Task run 'Read Taxi Data-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:23.437 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Created task run 'Read Taxi Data-2' for task 'Read Taxi Data'\n",
      "19:49:23.438 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Executing 'Read Taxi Data-2' immediately...\n",
      "19:49:24.356 | \u001b[36mINFO\u001b[0m    | Task run 'Read Taxi Data-2' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:24.534 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Created task run 'Preprocess: Add Features Taxi Data-0' for task 'Preprocess: Add Features Taxi Data'\n",
      "19:49:24.536 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Executing 'Preprocess: Add Features Taxi Data-0' immediately...\n",
      "19:49:25.118 | \u001b[36mINFO\u001b[0m    | Task run 'Preprocess: Add Features Taxi Data-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:25.311 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Created task run 'Preprocess: Add Features Taxi Data-1' for task 'Preprocess: Add Features Taxi Data'\n",
      "19:49:25.312 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Executing 'Preprocess: Add Features Taxi Data-1' immediately...\n",
      "19:49:25.862 | \u001b[36mINFO\u001b[0m    | Task run 'Preprocess: Add Features Taxi Data-1' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:26.059 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Created task run 'Preprocess: Add Features Taxi Data-2' for task 'Preprocess: Add Features Taxi Data'\n",
      "19:49:26.063 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Executing 'Preprocess: Add Features Taxi Data-2' immediately...\n",
      "19:49:26.608 | \u001b[36mINFO\u001b[0m    | Task run 'Preprocess: Add Features Taxi Data-2' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:26.661 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Created task run 'Train Best Model-0' for task 'Train Best Model'\n",
      "19:49:26.663 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Executing 'Train Best Model-0' immediately...\n",
      "19:49:28.139 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [0]    validation-rmse:15.01627\n",
      "19:49:28.439 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [1]    validation-rmse:13.77591\n",
      "19:49:28.783 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [2]    validation-rmse:12.66953\n",
      "19:49:28.932 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [3]    validation-rmse:11.68972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:49:29.079 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [4]    validation-rmse:10.81927\n",
      "19:49:29.402 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [5]    validation-rmse:10.05352\n",
      "19:49:29.555 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [6]    validation-rmse:9.37638\n",
      "19:49:29.905 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [7]    validation-rmse:8.78514\n",
      "19:49:30.253 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [8]    validation-rmse:8.26684\n",
      "19:49:30.524 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [9]    validation-rmse:7.81889\n",
      "19:49:30.789 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [10]   validation-rmse:7.42994\n",
      "19:49:31.021 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [11]   validation-rmse:7.09348\n",
      "19:49:31.190 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [12]   validation-rmse:6.80836\n",
      "19:49:31.702 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [13]   validation-rmse:6.56211\n",
      "19:49:31.830 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [14]   validation-rmse:6.35332\n",
      "19:49:31.952 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [15]   validation-rmse:6.17838\n",
      "19:49:32.092 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [16]   validation-rmse:6.02763\n",
      "19:49:32.240 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [17]   validation-rmse:5.90232\n",
      "19:49:32.417 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [18]   validation-rmse:5.79691\n",
      "19:49:32.636 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [19]   validation-rmse:5.70817\n",
      "19:49:32.900 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [20]   validation-rmse:5.63182\n",
      "19:49:33.133 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [21]   validation-rmse:5.56852\n",
      "19:49:33.328 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [22]   validation-rmse:5.51520\n",
      "19:49:33.472 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [23]   validation-rmse:5.47115\n",
      "19:49:33.623 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [24]   validation-rmse:5.43421\n",
      "19:49:33.942 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [25]   validation-rmse:5.40206\n",
      "19:49:34.070 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [26]   validation-rmse:5.37653\n",
      "19:49:34.323 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [27]   validation-rmse:5.35308\n",
      "19:49:34.504 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [28]   validation-rmse:5.33437\n",
      "19:49:34.634 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [29]   validation-rmse:5.31842\n",
      "19:49:34.996 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [30]   validation-rmse:5.30434\n",
      "19:49:35.185 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [31]   validation-rmse:5.29293\n",
      "19:49:35.284 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [32]   validation-rmse:5.28270\n",
      "19:49:35.417 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [33]   validation-rmse:5.27307\n",
      "19:49:35.569 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [34]   validation-rmse:5.26638\n",
      "19:49:35.699 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [35]   validation-rmse:5.26043\n",
      "19:49:35.808 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [36]   validation-rmse:5.25512\n",
      "19:49:35.976 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [37]   validation-rmse:5.25088\n",
      "19:49:36.058 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [38]   validation-rmse:5.24664\n",
      "19:49:36.152 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [39]   validation-rmse:5.24209\n",
      "19:49:36.249 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [40]   validation-rmse:5.23909\n",
      "19:49:36.356 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [41]   validation-rmse:5.23607\n",
      "19:49:36.495 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [42]   validation-rmse:5.23344\n",
      "19:49:36.608 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [43]   validation-rmse:5.23200\n",
      "19:49:36.690 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [44]   validation-rmse:5.23097\n",
      "19:49:36.777 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [45]   validation-rmse:5.22865\n",
      "19:49:37.050 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [46]   validation-rmse:5.22775\n",
      "19:49:37.187 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [47]   validation-rmse:5.22687\n",
      "19:49:37.297 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [48]   validation-rmse:5.22582\n",
      "19:49:37.521 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [49]   validation-rmse:5.22487\n",
      "19:49:37.636 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [50]   validation-rmse:5.22442\n",
      "19:49:37.722 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [51]   validation-rmse:5.22332\n",
      "19:49:37.802 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [52]   validation-rmse:5.22236\n",
      "19:49:38.004 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [53]   validation-rmse:5.22142\n",
      "19:49:38.192 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [54]   validation-rmse:5.22022\n",
      "19:49:38.279 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [55]   validation-rmse:5.21987\n",
      "19:49:38.376 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [56]   validation-rmse:5.21908\n",
      "19:49:38.533 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [57]   validation-rmse:5.21873\n",
      "19:49:38.782 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [58]   validation-rmse:5.21820\n",
      "19:49:38.884 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [59]   validation-rmse:5.21787\n",
      "19:49:38.973 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [60]   validation-rmse:5.21702\n",
      "19:49:39.054 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [61]   validation-rmse:5.21632\n",
      "19:49:39.219 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [62]   validation-rmse:5.21571\n",
      "19:49:39.314 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [63]   validation-rmse:5.21404\n",
      "19:49:39.409 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [64]   validation-rmse:5.21348\n",
      "19:49:39.623 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [65]   validation-rmse:5.21290\n",
      "19:49:39.766 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [66]   validation-rmse:5.21262\n",
      "19:49:39.846 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [67]   validation-rmse:5.21181\n",
      "19:49:39.936 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [68]   validation-rmse:5.21179\n",
      "19:49:40.027 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [69]   validation-rmse:5.21113\n",
      "19:49:40.129 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [70]   validation-rmse:5.21051\n",
      "19:49:40.314 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [71]   validation-rmse:5.21000\n",
      "19:49:40.488 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [72]   validation-rmse:5.20944\n",
      "19:49:40.722 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [73]   validation-rmse:5.20928\n",
      "19:49:40.910 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [74]   validation-rmse:5.20890\n",
      "19:49:41.049 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [75]   validation-rmse:5.20840\n",
      "19:49:41.467 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [76]   validation-rmse:5.20723\n",
      "19:49:41.858 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [77]   validation-rmse:5.20677\n",
      "19:49:42.144 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [78]   validation-rmse:5.20653\n",
      "19:49:42.237 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [79]   validation-rmse:5.20608\n",
      "19:49:42.345 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [80]   validation-rmse:5.20599\n",
      "19:49:42.446 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [81]   validation-rmse:5.20558\n",
      "19:49:42.543 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [82]   validation-rmse:5.20516\n",
      "19:49:42.660 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [83]   validation-rmse:5.20428\n",
      "19:49:42.778 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [84]   validation-rmse:5.20377\n",
      "19:49:42.869 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [85]   validation-rmse:5.20323\n",
      "19:49:42.962 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [86]   validation-rmse:5.20281\n",
      "19:49:43.091 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [87]   validation-rmse:5.20294\n",
      "19:49:43.449 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [88]   validation-rmse:5.20271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:49:43.560 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [89]   validation-rmse:5.20245\n",
      "19:49:43.695 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [90]   validation-rmse:5.20201\n",
      "19:49:43.864 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [91]   validation-rmse:5.20186\n",
      "19:49:43.977 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [92]   validation-rmse:5.20146\n",
      "19:49:44.054 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [93]   validation-rmse:5.20144\n",
      "19:49:44.175 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [94]   validation-rmse:5.20096\n",
      "19:49:44.272 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [95]   validation-rmse:5.20087\n",
      "19:49:44.517 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [96]   validation-rmse:5.20016\n",
      "19:49:44.759 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [97]   validation-rmse:5.19983\n",
      "19:49:44.941 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [98]   validation-rmse:5.19931\n",
      "19:49:45.106 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - [99]   validation-rmse:5.19931\n",
      "19:49:51.461 | \u001b[36mINFO\u001b[0m    | Task run 'Train Best Model-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:49:51.555 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'grumpy-wasp'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/orchestrate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada2c5e",
   "metadata": {},
   "source": [
    "## Q4. RMSE (Markdown Artifact), Prefect Deployment\n",
    "\n",
    "Download the February 2023 Green Taxi data and use it for your training data.\n",
    "Download the March 2023 Green Taxi data and use it for your validation data. \n",
    "\n",
    "Create a Prefect Markdown artifact that displays the RMSE for the validation data.\n",
    "Create a deployment and run it.\n",
    "\n",
    "**What’s the RMSE in the artifact to two decimal places ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa05e61f",
   "metadata": {},
   "source": [
    "Clone your Github Repo, then Check Data\n",
    "```sh\n",
    "git clone \n",
    "git remote -v\n",
    "```\n",
    "\n",
    "**Step 1: In this task, the code will be modified and improved by adding an @flow decorator to the entrypoint function.**\n",
    "\n",
    "- First, modify the existing code by adding an @flow decorator to the code's entrypoint function. The @flow decorator helps in type checking and ensures more robust code.\n",
    "- Specify a name for the @flow decorator. For example, you can name it \"main\" if the entrypoint function is named \"main.\"\n",
    "- After adding the @flow decorator, make sure to save the modified code to a new directory. This ensures that you keep a copy of the enhanced code separately. if use `Github`, make sure **to push** the modified **code** and available **data**.\n",
    "- Now, continue to run the Prefect step 5, ensure you have Prefect installed on your system. If not, you can install it using pip:\n",
    "```py\n",
    "# @flow\n",
    "\n",
    "# Add your original code here\n",
    "# ...\n",
    "```\n",
    "- Now, to run the Prefect step 5, ensure you have Prefect installed on your system. If not, you can install it using pip:\n",
    "```sh\n",
    "pip install -U \"prefect==2.10.18\"\n",
    "```\n",
    "\n",
    "**Step 2: Create a Project in a New Directory**\n",
    "\n",
    "- using the command-line interface (CLI) Select any git-local-s3\n",
    "```sh\n",
    "# follow the Prefect CLI instructions\n",
    "prefect init\n",
    "prefect project init # deprecated\n",
    "```\n",
    "\n",
    "**Step 3: Orchestration Enviroment (Server Runs or Prefect Cloud)**\n",
    "\n",
    "- Initialize the Prefect server locally\n",
    "\n",
    "Create another window and activate your conda environment. Start the Prefect API server locally with\n",
    "Start and Open Source Prefect Server\n",
    "```sh\n",
    "prefect server start\n",
    "```\n",
    "\n",
    "- Or Login to Prefect Cloud\n",
    "```sh\n",
    "prefect cloud login\n",
    "```\n",
    "\n",
    "**Step 4: Execution Enviroment (Deployed Code Flows Run!)**\n",
    "\n",
    "- **Initialize a worker** If there is no deployed code, initiate a worker that creates your work pool and polls it. Alternatively, deploy the code first and follow the Prefect CLI instructions to create the work pool.\n",
    "```sh\n",
    "# prefect worker start -p 'zoom_mlops_pool' -t process --limit 2\n",
    "prefect worker start --pool 'zoom_mlops_pool'\n",
    "```\n",
    "\n",
    "**Step 5: Deploy your Flow**\n",
    "\n",
    "- **Start a worker** that polls your work pool\n",
    "```sh\n",
    "# prefect deploy ls\n",
    "\n",
    "# recommended: follow the Prefect CLI instructions to create the work pool, then save it.\n",
    "prefect deploy -n 'zoom_mlops_deployment'\n",
    "\n",
    "# define deployment and work pool\n",
    "# prefect deploy './pycode/orchestrate.py:main_flow' -n 'zoom_mlops_deployment' -p 'zoom_mlops_pool'\n",
    "\n",
    "# prefect deploy --all  # or \n",
    "\n",
    "\n",
    "# ading prefect.yaml like below\n",
    "# the deployments section allows you to provide configuration for deploying flows\n",
    "deployments:\n",
    "- name: zoom_mlops_deployment\n",
    "  version:\n",
    "  tags: []\n",
    "  description: The main training pipeline\n",
    "  entrypoint: pycode/orchestrate.py:main_flow\n",
    "  parameters: {}\n",
    "  work_pool:\n",
    "    name: zoom_mlops_pool\n",
    "    work_queue_name:\n",
    "    job_variables: {}\n",
    "  schedule:\n",
    "    interval: 120.0\n",
    "    anchor_date: '2023-07-04T16:28:15.553849+00:00'\n",
    "    timezone: UTC\n",
    "```\n",
    "\n",
    "**Step 6: Start a run of the deployed flow from the CLI...**\n",
    "\n",
    "- Start a worker that polls your work pool\n",
    "```sh\n",
    "prefect deployment ls\n",
    "prefect deployment run 'Main Flow/zoom_mlops_deployment'\n",
    "```\n",
    "\n",
    "**Check...**\n",
    "\n",
    "- another pools\n",
    "```sh\n",
    "prefect block ls\n",
    "prefect block type ls\n",
    "```\n",
    "\n",
    "```sh\n",
    "prefect block register -m prefect_aws \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab8b3a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pycode/create_s3_bucket_block.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/create_s3_bucket_block.py\n",
    "\n",
    "from time import sleep\n",
    "from prefect_aws import S3Bucket, AwsCredentials\n",
    "\n",
    "\n",
    "def create_aws_creds_block():\n",
    "    my_aws_creds_obj = AwsCredentials(\n",
    "        aws_access_key_id=\"123abc\", aws_secret_access_key=\"abc123\"\n",
    "    )\n",
    "    my_aws_creds_obj.save(name=\"my-aws-creds\", overwrite=True)\n",
    "\n",
    "\n",
    "def create_s3_bucket_block():\n",
    "    aws_creds = AwsCredentials.load(\"my-aws-creds\")\n",
    "    my_s3_bucket_obj = S3Bucket(\n",
    "        bucket_name=\"my-first-bucket-abc\", credentials=aws_creds\n",
    "    )\n",
    "    my_s3_bucket_obj.save(name=\"s3-bucket-example\", overwrite=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_aws_creds_block()\n",
    "    sleep(5)\n",
    "    create_s3_bucket_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9a4e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/orchestrate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/orchestrate.py\n",
    "\n",
    "# Source: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py\n",
    "\n",
    "import os\n",
    "import click\n",
    "import pickle\n",
    "\n",
    "import pathlib\n",
    "import argparse\n",
    "import requests\n",
    "import urllib.request\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "import prefect\n",
    "from prefect import task, flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# from prefect_aws import S3Bucket\n",
    "# from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "\n",
    "\n",
    "@task(name=\"Fetch Data\", cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1),\n",
    "      retries=3, log_prints=True, )\n",
    "def fetch_data(raw_data_path: str, year: int, month: int, color: str) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "    os.makedirs(raw_data_path, exist_ok=True)  \n",
    "\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    url      = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0>2}.parquet'\n",
    "    filename = os.path.join(raw_data_path, f'{color}_tripdata_{year}-{month:0>2}.parquet')\n",
    "    # urllib.request.urlretrieve(url, filename)\n",
    "    # os.system(f\"wget -q -N -P {raw_data_path} {url}\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    return None\n",
    "\n",
    "\n",
    "@flow(name=\"Subflow Download Data\", log_prints=True)\n",
    "def download_data(raw_data_path: str, years: list, months: list, colors: list) -> None:\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for color in colors:\n",
    "                fetch_data(raw_data_path, year, month, color)\n",
    "    return None\n",
    "    \n",
    "    \n",
    "@task(name=\"Read Taxi Data\", retries=3, retry_delay_seconds=2, log_prints=None)\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime  = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration    = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical     = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(name=\"Preprocess: Add Features Taxi Data\", log_prints=True)\n",
    "def preprocess(\n",
    "    df: pd.DataFrame,dv: DictVectorizer = None, fit_dv: bool = False\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "    categorical = [\"PU_DO\"]\n",
    "    numerical   = ['trip_distance']\n",
    "    dicts       = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    if fit_dv:\n",
    "        # return sparse matrix\n",
    "        dv = DictVectorizer()\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "        \n",
    "    # Convert X the sparse matrix  to pandas DataFrame, but too slow\n",
    "    # X = pd.DataFrame(X.toarray(), columns=dv.get_feature_names_out())\n",
    "    # X = pd.DataFrame.sparse.from_spmatrix(X, columns=dv.get_feature_names_out())\n",
    "\n",
    "    try:\n",
    "        # Extract the target\n",
    "        target = 'duration'\n",
    "        y = df[target].values\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (X, y), dv\n",
    "\n",
    "\n",
    "@task(name=\"Train Best Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train  : scipy.sparse._csr.csr_matrix,\n",
    "    X_val    : scipy.sparse._csr.csr_matrix,\n",
    "    y_train  : np.ndarray,\n",
    "    y_val    : np.ndarray,\n",
    "    dv       : sklearn.feature_extraction.DictVectorizer,\n",
    "    raw_data_path: str,\n",
    "    dest_path: str,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"        \n",
    "    # Load train and test Data\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "        mlflow.set_tag(\"train-data-path\", f'{raw_data_path}/green_tripdata_2023-01.parquet')\n",
    "        mlflow.set_tag(\"valid-data-path\", f'{raw_data_path}/green_tripdata_2023-02.parquet')\n",
    "        mlflow.set_tag(\"test-data-path\",  f'{raw_data_path}/green_tripdata_2023-03.parquet') \n",
    "\n",
    "        # Set Model params information\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "            'objective': 'reg:squarederror',          # deprecated  \"reg:linear\"\n",
    "            # 'objective': \"reg:linear\",\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Build Model   \n",
    "        booster = xgb.train(\n",
    "            params               = best_params,\n",
    "            dtrain               = train,\n",
    "            num_boost_round      = 100,\n",
    "            evals                = [(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )   \n",
    "        \n",
    "        # Set Model Evaluation Metric\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse   = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)       \n",
    "\n",
    "        # Log Model two options\n",
    "        # Option1: Just log model\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")        \n",
    "        \n",
    "        # Option 2: save Model, Optional: Preprocessor or Pipeline         \n",
    "        # Create dest_path folder unless it already exists\n",
    "        # pathlib.Path(dest_path).mkdir(exist_ok=True) \n",
    "        os.makedirs(dest_path, exist_ok=True)       \n",
    "        local_file = os.path.join(dest_path, \"preprocessor.b\")\n",
    "        with open(local_file, \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        # whole proccess like pickle, saved Model, Optional: Preprocessor or Pipeline\n",
    "        mlflow.log_artifact(local_path = local_file, artifact_path=\"preprocessor\")      \n",
    "        \n",
    "        # print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "\n",
    "        # Create markdown artifact with RMSE value\n",
    "        markdown__rmse_report = f\"\"\"\n",
    "# RMSE Report\n",
    "\n",
    "## Summary\n",
    "\n",
    "Duration Prediction \n",
    "\n",
    "## RMSE XGBoost Model\n",
    "\n",
    "| Region    | RMSE |\n",
    "|:----------|-------:|\n",
    "| {date.today()} | {rmse:.2f} |\n",
    "\"\"\"\n",
    "        create_markdown_artifact(\n",
    "            key=\"duration-model-report\", \n",
    "            markdown=markdown__rmse_report,\n",
    "            description=\"RMSE for Validation Data Report\",\n",
    "        )\n",
    "    return None           \n",
    "\n",
    "\n",
    "# @flow(name=\"Email Server Crenditals\", log_prints=True)\n",
    "# def example_email_send_message_flow(email_addresses: list[str]):\n",
    "#     email_server_credentials = EmailServerCredentials.load(\"email-server-credentials\")\n",
    "    \n",
    "#     for email_address in email_addresses:\n",
    "#         subject = email_send_message.with_options(name=f\"email {email_address}\").submit(\n",
    "#             email_server_credentials=email_server_credentials,\n",
    "#             subject=\"Example Flow Notification using Gmail\",\n",
    "#             msg=\"This proves email_send_message works!\",\n",
    "#             email_to=email_address,\n",
    "#         )\n",
    "\n",
    "\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(raw_data_path=\"./data\", dest_path=\"./models\", years=\"2023\", months=\"1 2 3 4\", colors=\"green yellow\") -> None:\n",
    "    \"\"\"The main training pipeline\"\"\"\n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Build or Connect mlflow experiment\n",
    "    mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "    \n",
    "    # Download data    \n",
    "    years  = [int(year) for year in years.split()]\n",
    "    months = [int(month) for month in months.split()]\n",
    "    colors = colors.split()[:1]\n",
    "    download_data(raw_data_path, years, months, colors)\n",
    "    # print(sorted(glob(f'{raw_data_path}/*')))\n",
    "    \n",
    "    # # Download the data from AWS S3 Bucket\n",
    "    # s3_bucket_block = S3Bucket.load(\"s3-bucket-block\")\n",
    "    # s3_bucket_block.download_folder_to_path(from_folder=\"data\", to_folder=\"data\")\n",
    "    \n",
    "    # list parquet files\n",
    "    # print(sorted(glob(f'{raw_data_path}/green*.parquet')))\n",
    "    train_path, val_path, test_path = sorted(glob(f'{raw_data_path}/*.parquet'))[:3:]\n",
    "\n",
    "    # Read parquet files\n",
    "    df_train = read_data(train_path)\n",
    "    df_val   = read_data(val_path)\n",
    "    df_test  = read_data(test_path)\n",
    "    # print(df_train.shape, df_val.shape, df_test.shape, )    \n",
    "\n",
    "    # Fit the DictVectorizer and preprocess data\n",
    "    (X_train, y_train), dv = preprocess(df_train, fit_dv=True)\n",
    "    (X_val, y_val)    , _  = preprocess(df_val, dv, fit_dv=False)\n",
    "    (X_test, y_test)  , _  = preprocess(df_test, dv, fit_dv=False)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv, raw_data_path, dest_path)\n",
    "\n",
    "    # example_email_send_message_flow(['@gmail.com'])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b5f353c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !python ./pycode/orchestrate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852bb63",
   "metadata": {},
   "source": [
    "### Prefect Deployment\n",
    "\n",
    "```sh\n",
    "prefect server start\n",
    "\n",
    "# follow the Prefect CLI instructions\n",
    "prefect init\n",
    "\n",
    "# make sure to save the modified code and available data.\n",
    "# push the code GitHub and available data.\n",
    "\n",
    "# recommended: follow the Prefect CLI instructions to create the work pool, then save it.\n",
    "prefect deploy -n 'zoom_mlops_deployment'\n",
    "\n",
    "# Start a worker\n",
    "prefect worker start --pool 'zoom_mlops_pool'\n",
    "\n",
    "prefect deployment run 'Main Flow/zoom_mlops_deployment'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3476a2d",
   "metadata": {},
   "source": [
    "```\n",
    "31f95cf7\n",
    "Flow run\n",
    "wonderful-bat\n",
    "Task run\n",
    "Train Best Model-0\n",
    "RMSE for Validation Data Report\n",
    "RMSE for Validation Data\n",
    "RMSE: 5.374495195206525\n",
    "\n",
    "\n",
    "dc032057\n",
    "Flow run\n",
    "interesting-cougar\n",
    "Task run\n",
    "Train Best Model-0\n",
    "Created gtm-report\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb448527",
   "metadata": {},
   "source": [
    "## Q5. Emails\n",
    "\n",
    "\n",
    "It’s often helpful to be notified when something with your dataflow doesn’t work\n",
    "as planned. Create an email notification for to use with your own Prefect server instance.\n",
    "In your virtual environment, install the prefect-email integration with \n",
    "\n",
    "```bash\n",
    "pip install prefect-email\n",
    "```\n",
    "\n",
    "Make sure you are connected to a running Prefect server instance through your\n",
    "Prefect profile.\n",
    "See the docs if needed: https://docs.prefect.io/latest/concepts/settings/#configuration-profiles\n",
    "\n",
    "Register the new block with your server with \n",
    "\n",
    "```bash\n",
    "prefect block register -m prefect_email\n",
    "```\n",
    "\n",
    "Remember that a block is a Prefect class with a nice UI form interface.\n",
    "Block objects live on the server and can be created and accessed in your Python code. \n",
    "\n",
    "See the docs for how to authenticate by saving your email credentials to\n",
    "a block and note that you will need an App Password to send emails with\n",
    "Gmail and other services. Follow the instructions in the docs.\n",
    "\n",
    "Create and save an `EmailServerCredentials` notification block.\n",
    "Use the credentials block to send an email.\n",
    "\n",
    "Test the notification functionality by running a deployment.\n",
    "\n",
    "**What is the name of the pre-built prefect-email task function?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e209b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSuccessfully registered 1 block\u001b[0m\r\n",
      "\r\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┓\r\n",
      "┃\u001b[1m \u001b[0m\u001b[1mRegistered Blocks       \u001b[0m\u001b[1m \u001b[0m┃\r\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━┩\r\n",
      "│ Email Server Credentials │\r\n",
      "└──────────────────────────┘\r\n",
      "\r\n",
      " To configure the newly registered blocks, go to the Blocks page in the Prefect \r\n",
      "UI.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!prefect block register -m prefect_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84767650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Block.save at 0x7f5e362fb450>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prefect_email import EmailServerCredentials\n",
    "\n",
    "credentials = EmailServerCredentials(\n",
    "    username=\"EMAIL-ADDRESS-PLACEHOLDER\",\n",
    "    password=\"PASSWORD-PLACEHOLDER\",  # must be an app password\n",
    ")\n",
    "credentials.save(\"BLOCK-NAME-PLACEHOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91168fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Block.load at 0x7f5e362fb140>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prefect_email import EmailServerCredentials\n",
    "\n",
    "EmailServerCredentials.load(\"BLOCK_NAME_PLACEHOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f441a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Send an email using Gmail\n",
    "from prefect import flow\n",
    "from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "@flow(name=\"Email Server Credentials\")\n",
    "def example_email_send_message_flow():\n",
    "    email_server_credentials = EmailServerCredentials(\n",
    "        username=\"your_email_address@gmail.com\",\n",
    "        password=\"MUST_be_an_app_password_here!\",\n",
    "    )\n",
    "    # email_server_credentials.save(\"BLOCK-NAME-PLACEHOLDER\")\n",
    "    # email_server_credentials = EmailServerCredentials.load(\"BLOCK_NAME_PLACEHOLDER\")\n",
    "    \n",
    "    subject = email_send_message(\n",
    "        email_server_credentials=email_server_credentials,\n",
    "        subject=\"Example Flow Notification using Gmail\",\n",
    "        msg=\"This proves email_send_message works!\",\n",
    "        email_to=\"someone_awesome@gmail.com\",\n",
    "    )\n",
    "    return subject\n",
    "\n",
    "# example_email_send_message_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53b9c2",
   "metadata": {},
   "source": [
    "## Q6. Prefect Cloud\n",
    "\n",
    "The hosted Prefect Cloud lets you avoid running your own Prefect server and\n",
    "has automations that allow you to get notifications when certain events occur\n",
    "or don’t occur. \n",
    "\n",
    "Create a free forever Prefect Cloud account at [app.prefect.cloud](https://app.prefect.cloud/) and connect\n",
    "your workspace to it following the steps in the UI when you sign up. \n",
    "\n",
    "Set up an Automation from the UI that will send yourself an email when\n",
    "a flow run completes. Run one of your existing deployments and check\n",
    "your email to see the notification.\n",
    "\n",
    "Make sure your active profile is pointing toward Prefect Cloud and\n",
    "make sure you have a worker active.\n",
    "\n",
    "**What is the name of the second step in the Automation creation process?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5552947",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "- The name of the second step in the Automation creation process in Prefect Cloud is \"Configure Triggers.\" This step allows you to define the conditions or events that will trigger the Automation to execute. In this case, you would configure the trigger to activate when a flow run completes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34682f",
   "metadata": {},
   "source": [
    "# End of The Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
