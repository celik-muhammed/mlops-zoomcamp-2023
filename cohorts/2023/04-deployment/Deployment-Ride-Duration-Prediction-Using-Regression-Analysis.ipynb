{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2525fb2f",
   "metadata": {},
   "source": [
    "<div style=\"align: center; margin: 0; padding: 0; height: 250px;\">\n",
    "    <br>\n",
    "    <img src=\"https://www.nyc.gov/assets/tlc/images/content/hero/MRP-Closing-Week.jpg\" style=\"display:block; margin:auto; width:65%; height:100%;\">\n",
    "</div><br><br> \n",
    "\n",
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "<!--   https://xkcd.com/color/rgb/   -->\n",
    "  <p style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>TLC Trip Record Data</strong></p>  \n",
    "  \n",
    "  <p style=\"text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:22px; font-weight:normal; text-transform: capitalize; padding: 5px;\"\n",
    "     >Machine Learning Module: DEPLOYMENT - Ride Duration Prediction<br>using Regression Analysis<br></p><br>\n",
    "    \n",
    "  <div style=\"align: center;\">\n",
    "  <table style=\"text-align: center; background-color: romance; color: Jaguar; border-radius: 10px; font-family: monospace;\n",
    "                  line-height:1.4; font-size: 21px; font-weight: normal; text-transform: capitalize; padding: 5px; \n",
    "                  margin: 0 auto;\">\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > DEPLOYMENT <span style=\"font-size: 16px;\">Continuous Integration and Continuous Deployment (CI/CD)</span></td></tr>\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > MLOps <span style=\"font-size: 16px;\">(CI/CD, Model Versioning, Monitoring, Automated Retraining,<br>Security, Scalability, Collaboration)</span></td></tr>\n",
    "  </table>\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8b00d",
   "metadata": {},
   "source": [
    "**Dataset Info**\n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "Yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
    "\n",
    "For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
    "\n",
    "\n",
    "**ATTENTION!**\n",
    "\n",
    "On 05/13/2022, we are making the following changes to trip record files:\n",
    "\n",
    "- All files will be stored in the PARQUET format. Please see the ‘Working With PARQUET Format’ under the Data Dictionaries and MetaData section.\n",
    "- Trip data will be published monthly (with two months delay) instead of bi-annually.\n",
    "- HVFHV files will now include 17 more columns (please see High Volume FHV Trips Dictionary for details). Additional columns will be added to the old files as well. The earliest date to include additional columns: February 2019.\n",
    "- Yellow trip data will now include 1 additional column (‘airport_fee’, please see Yellow Trips Dictionary for details). The additional column will be added to the old files as well. The earliest date to include the additional column: January 2011.\n",
    "\n",
    "\n",
    "**Download the data for January, February, March and April 2022**\n",
    "\n",
    "Dataset: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "\n",
    "**Data Dictionaries and MetaData**\n",
    "\n",
    "- We'll use the same `NYC taxi dataset`, but instead of \"Green Taxi Trip Records\", we'll use `\"Yellow Taxi Trip Records\"`.\n",
    "\n",
    "> `Yellow Trips Data Dictionary`: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b74ab3",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "In this homework, we'll deploy the ride duration model in batch mode. Like in homework 1, we'll use the Yellow Taxi Trip Records dataset. \n",
    "\n",
    "You'll find the starter code in the [homework](https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2023/04-deployment/homework.md) directory.\n",
    "\n",
    "\n",
    "**Note**: Continuous Integration and Continuous Deployment (CI/CD): MLOps incorporates CI/CD pipelines to automate the testing, deployment, and monitoring of machine learning models. This ensures that new model versions or updates are automatically deployed to production after passing various validation and testing stages.\n",
    "\n",
    "\n",
    "**Table of Content**\n",
    "\n",
    "\n",
    "1. Import Libraries and Ingest Data\n",
    "    - Q1. Notebook<br>    \n",
    "2. Recognizing and Understanding Data\n",
    "    - Q2. Preparing the output<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78138c11",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>1. Import Libraries & Ingest Data</strong></h1>   \n",
    "</div>\n",
    "\n",
    "> ⚠️ Not Recommended conda `base` env, work on `venv`\n",
    "\n",
    "- https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "conda list -e > requirements.txt\n",
    "\n",
    "# new conda virtual environment\n",
    "conda create --name \"deployment-ops\" python=3.10 jupyter -y\n",
    "conda activate \"deployment-ops\"\n",
    "\n",
    "# install all package dependencies\n",
    "pip install -r requirements.txt\n",
    "conda install -c conda-forge --file=requirements.txt      # mostly not work\n",
    "conda install -c conda-forge pandas==2.0.2 -q -y\n",
    "\n",
    "# if The environment is inconsistent, try below\n",
    "conda update -n base -c defaults conda --force-reinstall\n",
    "conda install anaconda --force-reinstall\n",
    "\n",
    "```\n",
    "\n",
    "**You must use the `--no-deps` option in the pip install command in order to avoid bundling dependencies into your conda-package.**\n",
    "\n",
    "If you run pip install without the `--no-deps` option, pip will often install dependencies in your conda recipe and those dependencies will become part of your package. This wastes space in the package and `increases the risk of file overlap`, file clobbering, and broken packages.\n",
    "\n",
    "There might be cases where you want to install a package directly from a local directory or a specific location, without relying on the package indexes. In such situations, you can use the `--no-index` option to tell pip not to look for the package in any indexes.\n",
    "\n",
    "```\n",
    "- command1 & command2  # runs simultaneously\n",
    "- command1 ; command2  # runs sequentially\n",
    "- command1 && command2 # runs sequentially, runs command2 only if command1 succeeds\n",
    "- command1 || command2 # runs sequentially, runs command2 only if command1 fails\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ff0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture cap --no-stderr  # capture outputs  # cap.show()\n",
    "# !cat /etc/os-release\n",
    "# !grep -E -w 'VERSION|NAME|PRETTY_NAME' /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54cd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check enviroment\n",
    "# !conda env list\n",
    "# !conda info -e\n",
    "# !conda info | grep 'active env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74723240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt \n",
    "# To get started with MLflow you'll need to install the appropriate Python package.\n",
    "\n",
    "pandas==2.0.2\n",
    "orjson==3.9.1          # orjson is a fast, correct JSON library\n",
    "seaborn==0.12.2\n",
    "\n",
    "# ML Model packages\n",
    "scikit-learn==1.2.2\n",
    "xgboost==1.7.3\n",
    "\n",
    "# MLOPS packages\n",
    "mlflow==2.4.1\n",
    "wandb==0.15.4\n",
    "prefect==2.10.18\n",
    "prefect-email==0.2.2\n",
    "\n",
    "# MLOPS Cloud packages\n",
    "boto3==1.24.28\n",
    "prefect-aws==0.3.4\n",
    "\n",
    "# ML Model packages\n",
    "hyperopt==0.2.7\n",
    "\n",
    "# for parquet file\n",
    "pyarrow==11.0.0\n",
    "fastparquet==2023.4.0\n",
    "\n",
    "# Optionally\n",
    "# click==8.0.4\n",
    "black==23.3.0          # code style\n",
    "\n",
    "# Optionally\n",
    "jupyter\n",
    "ipykernel\n",
    "ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db08661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Python  : 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]\n",
      "Platform: Linux Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Actv Env: base\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, IPython.display\n",
    "\n",
    "# !{sys.executable} -m pip install -Uq -r requirements.txt  #  --no-deps --no-cache-dir --force-reinstall --no-index\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# IPython.display.clear_output()\n",
    "print(\"Python  :\", sys.version)\n",
    "print(\"Platform:\", platform.system(), platform.platform())\n",
    "print(\"Actv Env:\", os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ffeeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import stats\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import click\n",
    "import pickle\n",
    "# import pathlib\n",
    "# import argparse\n",
    "# import requests\n",
    "# import urllib.request\n",
    "from glob import glob\n",
    "\n",
    "# from tqdm import tqdm           # console-based\n",
    "# from tqdm.notebook import tqdm  # jupyter-based\n",
    "from tqdm.auto import tqdm        # automatically selects\n",
    "# tqdm._instances.clear()\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import mlflow\n",
    "# import wandb\n",
    "# import prefect\n",
    "# from prefect import task, flow, Flow\n",
    "# from prefect.tasks import task_input_hash\n",
    "# from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# memory management performs garbage collection \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb564b98",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>2. Recognizing and Understanding Data</strong></h1>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5b120",
   "metadata": {},
   "source": [
    "## Ingest Data [wget](https://linuxways.net/centos/linux-wget-command-with-examples/) or [curl](https://daniel.haxx.se/blog/2020/09/10/store-the-curl-output-over-there/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2a3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Green Taxi Trip Records\" Download the data for January, February and March 2022\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044ce591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(f'./data/*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41e352",
   "metadata": {},
   "source": [
    "## Q1. Notebook\n",
    "\n",
    "We'll start with the same notebook we ended up with in homework 1.\n",
    "We cleaned it a little bit and kept only the scoring part. You can find the initial notebook [here](homework/starter.ipynb).\n",
    "\n",
    "Run this notebook for the February 2022 data.\n",
    "\n",
    "**What's the standard deviation of the predicted duration for this dataset?**\n",
    "\n",
    "* 5.28\n",
    "* 10.28\n",
    "* 15.28\n",
    "* 20.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c54455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -Nq \"https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/cohorts/2023/04-deployment/homework/starter.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036ce14",
   "metadata": {},
   "source": [
    "**y_pred.std()**\n",
    "\n",
    "5.28140357655334"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159982ef",
   "metadata": {},
   "source": [
    "## Q2. Preparing the output\n",
    "\n",
    "Like in the course videos, we want to prepare the dataframe with the output. \n",
    "\n",
    "First, let's create an artificial `ride_id` column:\n",
    "\n",
    "```python\n",
    "df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "```\n",
    "\n",
    "Next, write the ride id and the predictions to a dataframe with results. \n",
    "\n",
    "Save it as parquet:\n",
    "\n",
    "```python\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")\n",
    "```\n",
    "\n",
    "**What's the size of the output file?**\n",
    "\n",
    "* 28M\n",
    "* 38M\n",
    "* 48M\n",
    "* 58M\n",
    "\n",
    "__Note:__ Make sure you use the snippet above for saving the file. It should contain only these two columns. For this question, don't change the\n",
    "dtypes of the columns and use pyarrow, not fastparquet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70bbf4",
   "metadata": {},
   "source": [
    "**!du -h ./output/yellow_tripdata_2022-02.parquet | awk '{print $1}'**\n",
    "\n",
    "58M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f5e35",
   "metadata": {},
   "source": [
    "## Q3. Creating the scoring script\n",
    "\n",
    "Now let's turn the `NOTEBOOK` into a `SCRIPT`. \n",
    "\n",
    "**Which command you need to execute for that?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23feabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# Create a new directory for storing MLflow data\n",
    "os.makedirs('./pycode', exist_ok=True)\n",
    "# os.makedirs('./data', exist_ok=True)\n",
    "# os.makedirs('./output', exist_ok=True)\n",
    "# os.makedirs('./models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0ce59af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pycode/batch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/batch.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "year        = int(sys.argv[1]) # 2022\n",
    "month       = int(sys.argv[2]) # 2\n",
    "\n",
    "input_file  = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "output_file = f'output/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "\n",
    "with open('model.bin', 'rb') as f_in:\n",
    "    dv, lr = pickle.load(f_in)\n",
    "    \n",
    "    \n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df.tpep_dropoff_datetime )\n",
    "    df[\"tpep_pickup_datetime\"]  = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]\n",
    "    df['duration'] = df['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    # df[categorical] = df[categorical].astype(str)\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df            = read_data(input_file)\n",
    "df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "\n",
    "\n",
    "dicts  = df[categorical].to_dict(orient='records')\n",
    "X_val  = dv.transform(dicts)\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "\n",
    "print('predicted mean duration:', y_pred.mean())\n",
    "\n",
    "\n",
    "df_result = pd.DataFrame()\n",
    "df_result['ride_id'] = df['ride_id']\n",
    "df_result['predicted_duration'] = y_pred\n",
    "\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bacee26",
   "metadata": {},
   "source": [
    "## Q4. Virtual environment\n",
    "\n",
    "Now let's put everything into a virtual environment. We'll use pipenv for that.\n",
    "\n",
    "- https://github.com/pypa/pipenv#usage-examples\n",
    "\n",
    "```sh\n",
    "pip install --user pipenv\n",
    "conda install -c conda-forge pipenv -y\n",
    "```\n",
    "\n",
    "Install all the required libraries. Pay attention to the Scikit-Learn version:\n",
    "it should be `scikit-learn==1.2.2`. \n",
    "```sh\n",
    "pipenv install scikit-learn==1.2.2 pandas pyarrow s3fs tqdm boto3==1.24.28\n",
    "```\n",
    "After installing the libraries, pipenv creates two files: \n",
    "- `Pipfile` and \n",
    "- `Pipfile.lock`. \n",
    "The `Pipfile.lock` file keeps the hashes of the dependencies we use for the virtual env.\n",
    "\n",
    "**What's the first hash for the Scikit-Learn dependency?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9b64a",
   "metadata": {},
   "source": [
    "```py\n",
    "\"scikit-learn\": {\n",
    "    \"hashes\": [\n",
    "        \"sha256:065e9673e24e0dc5113e2dd2b4ca30c9d8aa2fa90f4c0597241c93b63130d233\",\n",
    "        \"sha256:2dd3ffd3950e3d6c0c0ef9033a9b9b32d910c61bd06cb8206303fb4514b88a49\",\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587e61c",
   "metadata": {},
   "source": [
    "## Q5. Parametrize the script\n",
    "\n",
    "Let's now make the script configurable via CLI. We'll create two \n",
    "parameters: year and month.\n",
    "\n",
    "Run the script for March 2022. \n",
    "\n",
    "**What's the mean predicted duration?**\n",
    "\n",
    "* 7.76\n",
    "* 12.76\n",
    "* 17.76\n",
    "* 22.76\n",
    "\n",
    "**Hint**: just add a print statement to your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa987e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/batch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/batch.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "tqdm._instances.clear()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]\n",
    "    df['duration'] = df['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    # df[categorical] = df[categorical].astype(str)\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_duration(df: pd.DataFrame, dv, lr) -> np.ndarray:\n",
    "    \"\"\"Predict the duration using the trained model\"\"\"\n",
    "    dicts  = df[categorical].to_dict(orient='records')\n",
    "    X_val  = dv.transform(dicts)\n",
    "    y_pred = lr.predict(X_val)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def save_results(df: pd.DataFrame, y_pred: np.ndarray, output_file: str) -> None:\n",
    "    \"\"\"Save the predicted results to a parquet file\"\"\"\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['ride_id'] = df['ride_id']\n",
    "    df_result['predicted_duration'] = y_pred\n",
    "    df_result.to_parquet(        \n",
    "        output_file,\n",
    "        engine='pyarrow',\n",
    "        compression=None,\n",
    "        index=False\n",
    "    )\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():    \n",
    "    steps = [\"Loading model\", \"Reading data\", \"Predict data\"]\n",
    "    with tqdm(total=len(steps), desc=\"Running steps\", leave=True) as pbar:\n",
    "        # Step 1: Loading model\n",
    "        pbar.set_description(steps[0])\n",
    "        with open('model.bin', 'rb') as f_in:\n",
    "            dv, lr = pickle.load(f_in)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 2: Reading data\n",
    "        pbar.set_description(steps[1])\n",
    "        df = read_data(input_file)\n",
    "        df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Step 3: Predict data\n",
    "        pbar.set_description(steps[2])\n",
    "        y_pred = predict_duration(df, dv, lr)\n",
    "        pbar.update(1)\n",
    "        pbar.close()    \n",
    "\n",
    "\n",
    "    # Print Prediction\n",
    "    print('predicted mean duration:', y_pred.mean().round(2))\n",
    "\n",
    "    # save_results\n",
    "    save_results(df, y_pred, output_file)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # Global Parameters\n",
    "    year        = int(sys.argv[1]) # 2022\n",
    "    month       = int(sys.argv[2]) # 2\n",
    "    input_file  = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    output_file = f'output/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    categorical = ['PULocationID', 'DOLocationID']    \n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c98c50f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict data: 100%|███████████████████████████████| 3/3 [00:39<00:00, 13.12s/it]\n",
      "predicted mean duration: 12.76\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/batch.py 2022 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb92aaf",
   "metadata": {},
   "source": [
    "## Q6. Docker container \n",
    "\n",
    "Finally, we'll package the script in the docker container. \n",
    "For that, you'll need to use a base image that we prepared. \n",
    "\n",
    "This is how it looks like:\n",
    "\n",
    "```sh\n",
    "FROM python:3.10.0-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY [ \"model2.bin\", \"model.bin\" ]\n",
    "```\n",
    "\n",
    "(see [`homework/Dockerfile`](https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2023/04-deployment/homework_solution/Dockerfile))\n",
    "\n",
    "We pushed it to [`svizor/zoomcamp-model:mlops-3.10.0-slim`](https://hub.docker.com/layers/svizor/zoomcamp-model/mlops-3.10.0-slim/images/sha256-595bf690875f5b9075550b61c609be10f05e6915609ef4ea4ce9797116c99eff?context=repo),\n",
    "which you should use as your base image.\n",
    "\n",
    "That is, this is how your Dockerfile should start:\n",
    "\n",
    "```docker\n",
    "FROM svizor/zoomcamp-model:mlops-3.10.0-slim\n",
    "\n",
    "# do stuff here\n",
    "```\n",
    "Save the Dockerfile in the same directory as your `script`.\n",
    "\n",
    "This image already has a pickle file with a dictionary vectorizer\n",
    "and a model. You will need to use them.\n",
    "\n",
    "**Important**: _don't copy the model to the docker image_. You will need\n",
    "to use the pickle file already in the image.\n",
    "\n",
    "\n",
    "Now, open a terminal or command prompt, navigate to the directory containing the Dockerfile, and build the Docker image using the following command:\n",
    "```sh\n",
    "docker build -t ./pycode/batch.py .\n",
    "```\n",
    "\n",
    "This command will **create a Docker image** based on the Dockerfile configuration with using the provided base image and your script file. Docker image named **script file name**.\n",
    "\n",
    "Once the **docker image is built**, you can run the script inside the **Docker container**:\n",
    "```sh\n",
    "docker run ./pycode/batch.py\n",
    "```\n",
    "The script will execute inside the container, and you should see the output, including the mean predicted duration for April 2022.\n",
    "\n",
    "Now run the script with docker. What's the mean predicted duration\n",
    "for April 2022? \n",
    "\n",
    "\n",
    "* 7.92\n",
    "* 12.83\n",
    "* 17.92\n",
    "* 22.83"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9a9f1",
   "metadata": {},
   "source": [
    "Create a new Dockerfile in your project directory with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48634523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM svizor/zoomcamp-model:mlops-3.10.0-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install pipenv\n",
    "RUN pip install -U pip & pip install pipenv\n",
    "\n",
    "# Copy the Pipfile and Pipfile.lock to the Docker container\n",
    "COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n",
    "\n",
    "# Install the dependencies using pipenv\n",
    "RUN pipenv install --system --deploy\n",
    "\n",
    "# Copy your script file to the Docker container\n",
    "COPY ./pycode/batch.py /app/batch.py\n",
    "\n",
    "# Set the command to run your script, can override the command by passing arguments\n",
    "CMD [\"python\", \"/app/batch.py\", \"2022\", \"4\"]\n",
    "\n",
    "# Set the command to run your script, want to enforce a specific command\n",
    "# ENTRYPOINT [\"python\", \"/app/batch.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5039582",
   "metadata": {},
   "source": [
    "Now, open a terminal or command prompt, navigate to the directory containing the Dockerfile, and build the Docker image using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb74c13d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !docker build -t \"pycode/batch.py\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4beb8e9",
   "metadata": {},
   "source": [
    "This command will create a Docker image based on the Dockerfile configuration with using the provided base image and your script file. it named script file.\n",
    "\n",
    "- Once the image is built, you can run the script inside the Docker container:\n",
    "- The script will execute inside the container, and you should see the output, including the mean predicted duration for April 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4766d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict data: 100%|██████████| 3/3 [01:59<00:00, 39.67s/it] \n",
      "predicted mean duration: 12.83\n"
     ]
    }
   ],
   "source": [
    "!docker run pycode/batch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ca1f9",
   "metadata": {},
   "source": [
    "## Bonus: upload the result to the cloud (Not graded)\n",
    "\n",
    "Just printing the mean duration inside the docker image \n",
    "doesn't seem very practical. Typically, after creating the output \n",
    "file, we upload it to the cloud storage.\n",
    "\n",
    "To run the script inside the Docker container with the cloud storage upload, use the same docker run command as before:\n",
    "```sh\n",
    "docker run my_script\n",
    "```\n",
    "\n",
    "Modify your code to upload the parquet file to S3/GCS/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "647406f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pycode/batch_s3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/batch_s3.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "tqdm._instances.clear()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import boto3  # Import the boto3 library for S3\n",
    "\n",
    "\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]\n",
    "    df['duration'] = df['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    # df[categorical] = df[categorical].astype(str)\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_duration(df: pd.DataFrame, dv, lr) -> np.ndarray:\n",
    "    \"\"\"Predict the duration using the trained model\"\"\"\n",
    "    dicts  = df[categorical].to_dict(orient='records')\n",
    "    X_val  = dv.transform(dicts)\n",
    "    y_pred = lr.predict(X_val)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def save_results(df: pd.DataFrame, y_pred: np.ndarray, output_file: str) -> None:\n",
    "    \"\"\"Save the predicted results to a parquet file\"\"\"\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['ride_id'] = df['ride_id']\n",
    "    df_result['predicted_duration'] = y_pred\n",
    "    df_result.to_parquet(        \n",
    "        output_file,\n",
    "        engine='pyarrow',\n",
    "        compression=None,\n",
    "        index=False\n",
    "    )\n",
    "    return None\n",
    "\n",
    "\n",
    "def upload_to_s3(file_path: str, s3_bucket: str, s3_key: str):\n",
    "    \"\"\"Upload a file to S3 bucket\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.upload_file(file_path, s3_bucket, s3_key)\n",
    "    \n",
    "    print(f\"Uploaded file to S3: s3://{s3_bucket}/{s3_key}\")\n",
    "    \n",
    "\n",
    "def main():    \n",
    "    steps = [\"Reading data\", \"Loading model\"]\n",
    "    with tqdm(total=len(steps), desc=\"Running steps\", leave=True) as pbar:\n",
    "        # Step 1: Reading data\n",
    "        pbar.set_description(steps[0])\n",
    "        df = read_data(input_file)\n",
    "        df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 2: Loading model\n",
    "        pbar.set_description(steps[1])\n",
    "        with open('model.bin', 'rb') as f_in:\n",
    "            dv, lr = pickle.load(f_in)\n",
    "        pbar.update(1)\n",
    "        pbar.close()    \n",
    "\n",
    "\n",
    "    # Prediction\n",
    "    y_pred = predict_duration(df, dv, lr)\n",
    "    print('predicted mean duration:', y_pred.mean().round(2))\n",
    "\n",
    "    # save_results\n",
    "    save_results(df, y_pred, output_file)\n",
    "    \n",
    "    # Upload the Parquet file to S3\n",
    "    s3_bucket = 'your-s3-bucket-name'  # Replace with your S3 bucket name \n",
    "    upload_to_s3(output_file, s3_bucket, s3_key=output_file)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # Global Parameters\n",
    "    year        = int(sys.argv[1]) # 2022\n",
    "    month       = int(sys.argv[2]) # 2\n",
    "    input_file  = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    output_file = f'output/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    categorical = ['PULocationID', 'DOLocationID']    \n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9db77",
   "metadata": {},
   "source": [
    "## Publishing the image to dockerhub\n",
    "\n",
    "This is how we published the image to Docker hub:\n",
    "\n",
    "```bash\n",
    "docker build -t mlops-zoomcamp-model:v1 .\n",
    "docker tag mlops-zoomcamp-model:v1 svizor/zoomcamp-model:mlops-3.10.0-slim\n",
    "docker push svizor/zoomcamp-model:mlops-3.10.0-slim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62679713",
   "metadata": {},
   "source": [
    "To publish the Docker image to Docker Hub, follow these steps:\n",
    "\n",
    "1. Make sure you have a Docker Hub account. If you don't have one, create an account at [https://hub.docker.com/](https://hub.docker.com/).\n",
    "\n",
    "2. Open your terminal or command prompt and log in to Docker Hub using the `docker login` command. Enter your Docker Hub username and password when prompted.\n",
    "\n",
    "3. Build your Docker image and tag it with the appropriate name and version:\n",
    "```bash\n",
    "docker build -t mlops-zoomcamp-model:v1 .\n",
    "```\n",
    "\n",
    "4. Tag the image with the Docker Hub repository name and version:\n",
    "```bash\n",
    "docker tag mlops-zoomcamp-model:v1 svizor/zoomcamp-model:mlops-3.10.0-slim\n",
    "```\n",
    "\n",
    "5. Push the tagged image to Docker Hub:\n",
    "```bash\n",
    "docker push svizor/zoomcamp-model:mlops-3.10.0-slim\n",
    "```\n",
    "\n",
    "6. Wait for the image to be uploaded to Docker Hub. Once the upload is complete, you can access and manage your Docker image through your Docker Hub account.\n",
    "\n",
    "Replace `your-dockerhub-username` with your actual Docker Hub username. Also, adjust the repository name and version according to your preference.\n",
    "\n",
    "After completing these steps, your Docker image will be published to Docker Hub, and you can share it with others or use it in other environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7775c3",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://forms.gle/4tnqB5yGeMrTtKKa6\n",
    "* It's possible that your answers won't match exactly. If it's the case, select the closest one.\n",
    "* You can submit your answers multiple times. In this case, the last submission will be used for scoring.\n",
    "\n",
    "\n",
    "## Deadline\n",
    "\n",
    "The deadline for submitting is 26 June 2023 (Monday) 23:00 CEST. \n",
    "After that, the form will be closed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34682f",
   "metadata": {},
   "source": [
    "# End of The Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "314.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
