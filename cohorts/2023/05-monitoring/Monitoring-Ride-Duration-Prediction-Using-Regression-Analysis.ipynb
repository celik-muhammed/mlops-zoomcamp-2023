{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2525fb2f",
   "metadata": {},
   "source": [
    "<div style=\"align: center; margin: 0; padding: 0; height: 250px;\">\n",
    "    <br>\n",
    "    <img src=\"https://www.nyc.gov/assets/tlc/images/content/hero/MRP-Closing-Week.jpg\" style=\"display:block; margin:auto; width:65%; height:100%;\">\n",
    "</div><br><br> \n",
    "\n",
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "<!--   https://xkcd.com/color/rgb/   -->\n",
    "  <p style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>TLC Trip Record Data</strong></p>  \n",
    "  \n",
    "  <p style=\"text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:22px; font-weight:normal; text-transform: capitalize; padding: 5px;\"\n",
    "     >Machine Learning Module: MONITORING - Ride Duration Prediction<br>using Regression Analysis<br></p><br>\n",
    "    \n",
    "  <div style=\"align: center;\">\n",
    "  <table style=\"text-align: center; background-color: romance; color: Jaguar; border-radius: 10px; font-family: monospace;\n",
    "                  line-height:1.4; font-size: 21px; font-weight: normal; text-transform: capitalize; padding: 5px; \n",
    "                  margin: 0 auto;\">\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > MONITORING <span style=\"font-size: 16px;\">(to ensure that deployed models remain accurate,<br>reliable, and aligned with business objectives over time)</span></td></tr>\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > MLOps <span style=\"font-size: 16px;\">(CI/CD, Model Versioning, Monitoring, Automated Retraining,<br>Security, Scalability, Collaboration)</span></td></tr>\n",
    "  </table>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "[Introduction to MLOps Monitoring](https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_5/intro.md)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32a8b00d",
   "metadata": {},
   "source": [
    "**Dataset Info**\n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "Yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
    "\n",
    "For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
    "\n",
    "\n",
    "**ATTENTION!**\n",
    "\n",
    "On 05/13/2022, we are making the following changes to trip record files:\n",
    "\n",
    "- All files will be stored in the PARQUET format. Please see the ‘Working With PARQUET Format’ under the Data Dictionaries and MetaData section.\n",
    "- Trip data will be published monthly (with two months delay) instead of bi-annually.\n",
    "- HVFHV files will now include 17 more columns (please see High Volume FHV Trips Dictionary for details). Additional columns will be added to the old files as well. The earliest date to include additional columns: February 2019.\n",
    "- Yellow trip data will now include 1 additional column (‘airport_fee’, please see Yellow Trips Dictionary for details). The additional column will be added to the old files as well. The earliest date to include the additional column: January 2011.\n",
    "\n",
    "\n",
    "**Download the data for March 2023**\n",
    "\n",
    "Dataset: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "\n",
    "**Data Dictionaries and MetaData**\n",
    "\n",
    "- We'll use the same `NYC taxi dataset`, but instead of \"Yellow Taxi Trip Records\", we'll use `\"Green Taxi Trip Records\"`.\n",
    "\n",
    "> `Green Trips Data Dictionary`: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33b74ab3",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "\n",
    "The goal of this homework is to familiarize users with monitoring for ML batch services, using PostgreSQL database to store metrics and Grafana to visualize them. Download the March 2023 Green Taxi data.\n",
    "\n",
    "[homework](https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/cohorts/2023/05-monitoring/homework.md)\n",
    "\n",
    "\n",
    "**Note**: Model Monitoring: Model monitoring is the process of continuously tracking and evaluating the performance and behavior of machine learning models deployed in production. It involves collecting relevant data, assessing model outputs, detecting anomalies or deviations, and triggering alerts or actions when necessary. The goal of model monitoring is to ensure that deployed models remain accurate, reliable, and aligned with business objectives over time.\n",
    "\n",
    "\n",
    "**Table of Content**\n",
    "\n",
    "\n",
    "1. Import Libraries and Ingest Data \n",
    "2. Docker Compose Configuration File\n",
    "    - Create a Docker Compose File\n",
    "    - Create Grafana Data Source Configuration\n",
    "    - Build and Run Docker Compose<br>\n",
    "3. Baseline Model nyc_taxi Data\n",
    "    - Q1. Prepare the dataset\n",
    "    - Q2. Metric\n",
    "    - Q3. Prefect flow\n",
    "    - Q4. Monitoring\n",
    "    - Q5. Dashboard<br>\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78138c11",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>1. Import Libraries & Ingest Data</strong></h1>   \n",
    "</div>\n",
    "\n",
    "> ⚠️ Not Recommended conda `base` env, work on `venv`\n",
    "\n",
    "- https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "conda list -e > requirements.txt\n",
    "\n",
    "# new conda virtual environment\n",
    "conda create --name \"monitoring-ops\" python=3.10 jupyter -y\n",
    "conda activate \"monitoring-ops\"\n",
    "\n",
    "# install all package dependencies\n",
    "pip install -r requirements.txt\n",
    "conda install -c conda-forge --file=requirements.txt      # mostly not work\n",
    "conda install -c conda-forge pandas==2.0.2 -q -y\n",
    "\n",
    "# if The environment is inconsistent, try below\n",
    "conda update -n base -c defaults conda --force-reinstall\n",
    "conda install anaconda --force-reinstall\n",
    "\n",
    "```\n",
    "\n",
    "**You must use the `--no-deps` option in the pip install command in order to avoid bundling dependencies into your conda-package.**\n",
    "\n",
    "If you run pip install without the `--no-deps` option, pip will often install dependencies in your conda recipe and those dependencies will become part of your package. This wastes space in the package and `increases the risk of file overlap`, file clobbering, and broken packages.\n",
    "\n",
    "There might be cases where you want to install a package directly from a local directory or a specific location, without relying on the package indexes. In such situations, you can use the `--no-index` option to tell pip not to look for the package in any indexes.\n",
    "\n",
    "```\n",
    "- command1 & command2  # runs simultaneously\n",
    "- command1 ; command2  # runs sequentially\n",
    "- command1 && command2 # runs sequentially, runs command2 only if command1 succeeds\n",
    "- command1 || command2 # runs sequentially, runs command2 only if command1 fails\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ff0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture cap --no-stderr  # capture outputs  # cap.show()\n",
    "# !cat /etc/os-release\n",
    "# !grep -E -w 'VERSION|NAME|PRETTY_NAME' /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54cd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check enviroment\n",
    "# !conda env list\n",
    "# !conda info -e\n",
    "# !conda info | grep 'active env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74723240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt \n",
    "# To get started with MLflow you'll need to install the appropriate Python package.\n",
    "\n",
    "# for parquet file\n",
    "pyarrow==11.0.0\n",
    "fastparquet==2023.4.0\n",
    "\n",
    "pandas==2.0.2\n",
    "matplotlib==3.7.1\n",
    "orjson==3.8.8          # orjson is a fast, correct JSON library\n",
    "tqdm==4.65.0\n",
    "# requests==2.29.0\n",
    "\n",
    "# ML Model packages\n",
    "scikit-learn==1.2.2\n",
    "\n",
    "# MLOPS packages\n",
    "mlflow==2.3.1\n",
    "prefect==2.10.18\n",
    "prefect-email==0.2.2\n",
    "\n",
    "# MLOPS Cloud packages\n",
    "boto3~=1.24.28\n",
    "prefect-aws==0.3.4\n",
    "\n",
    "# Monitoring\n",
    "joblib==1.2.0\n",
    "evidently==0.3.3\n",
    "psycopg==3.1.9\n",
    "# psycopg[binary]==3.1.9\n",
    "\n",
    "# Optionally\n",
    "pipenv~=2022.7.4\n",
    "jupyter\n",
    "ipykernel\n",
    "ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db08661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Python  : 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0]\n",
      "Platform: Linux Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Actv Env: monitoring-ops\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, IPython.display\n",
    "\n",
    "# !{sys.executable} -m pip install -Uq -r requirements.txt  #  --no-deps --no-cache-dir --force-reinstall --no-index\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# IPython.display.clear_output()\n",
    "print(\"Python  :\", sys.version)\n",
    "print(\"Platform:\", platform.system(), platform.platform())\n",
    "print(\"Actv Env:\", os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ffeeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from numba import NumbaDeprecationWarning\n",
    "# Suppress NumbaDeprecationWarning in umap.distances module\n",
    "warnings.filterwarnings(\"ignore\", category=NumbaDeprecationWarning)\n",
    "\n",
    "import os\n",
    "# TensorFlow to only display error messages \n",
    "# and suppress warnings and informational messages.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import stats\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import click\n",
    "import pickle\n",
    "import datetime\n",
    "# import pathlib\n",
    "# import argparse\n",
    "import requests\n",
    "# import urllib.request\n",
    "from glob import glob\n",
    "\n",
    "# Import joblib for model persistence and tqdm for progress bars\n",
    "from joblib import load, dump\n",
    "# from tqdm import tqdm           # console-based\n",
    "# from tqdm.notebook import tqdm  # jupyter-based\n",
    "from tqdm.auto import tqdm        # automatically selects\n",
    "# tqdm._instances.clear()\n",
    "\n",
    "# Import sklearn for machine learning\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# import mlflow\n",
    "# import wandb\n",
    "# import prefect\n",
    "# from prefect import task, flow, Flow\n",
    "# from prefect.tasks import task_input_hash\n",
    "# from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# Import Evidently for data drift and model performance monitoring\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import ColumnDriftMetric, DatasetDriftMetric\n",
    "from evidently.metrics import DatasetMissingValuesMetric, ColumnQuantileMetric\n",
    "\n",
    "# memory management performs garbage collection \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb564b98",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>2. Recognizing and Understanding Data</strong></h1>   \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebb5b120",
   "metadata": {},
   "source": [
    "## Ingest Data [wget](https://linuxways.net/centos/linux-wget-command-with-examples/) or [curl](https://daniel.haxx.se/blog/2020/09/10/store-the-curl-output-over-there/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2a3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Green Taxi Trip Records\" Download the data for January, February and March 2022\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044ce591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/green_tripdata_2023-03.parquet', './data/reference.parquet']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(f'./data/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23feabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# Create a new directory for storing MLflow data\n",
    "os.makedirs('./config', exist_ok=True)\n",
    "os.makedirs('./pycode', exist_ok=True)\n",
    "# os.makedirs('./data', exist_ok=True)\n",
    "# os.makedirs('./output', exist_ok=True)\n",
    "# os.makedirs('./models', exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8d0d222",
   "metadata": {},
   "source": [
    "# Docker Compose Configuration File\n",
    "\n",
    "A Docker Compose file is used to define and manage a multi-container Docker application. It's a YAML file that contains all the necessary configurations to run the application. The services defined in this file are typically used in a development environment for testing or in a production environment for deployment.\n",
    "\n",
    "## Create a Docker Compose File\n",
    "\n",
    "- In your project directory, create a file named docker-compose.yml\n",
    "\n",
    "Next, we'll configure the Docker Compose file, including the specification of volumes for Grafana. We'll add a PostgreSQL database, an Adminer for managing the database content, and Grafana for dashboarding.\n",
    "\n",
    "Here's an example of docker-compose.yml:\n",
    "\n",
    "```sh\n",
    "touch docker-compose.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ea3d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker-compose.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker-compose.yml\n",
    "\n",
    "# Specifies the Docker Compose file version\n",
    "version: '3.7'\n",
    "\n",
    "# Declares volumes that can be used by services in the Docker Compose file\n",
    "volumes: \n",
    "  # Declares a volume named grafana_data\n",
    "  grafana_data: {}\n",
    "\n",
    "# Defines networks that can be used by services in the Docker Compose file\n",
    "networks:\n",
    "  # Declares a network named front-tier\n",
    "  front-tier:\n",
    "  # Declares a network named back-tier\n",
    "  back-tier:\n",
    "\n",
    "# Defines the services that make up your app\n",
    "services:\n",
    "  # Defines a service named db\n",
    "  db:\n",
    "    # Specifies the Docker image to use for this service\n",
    "    image: postgres\n",
    "    # Ensures that the service is always restarted if it stops\n",
    "    restart: always\n",
    "    # Sets environment variables for the service\n",
    "    environment:\n",
    "      # Sets the password for the Postgres database\n",
    "      POSTGRES_PASSWORD: example\n",
    "    # Maps ports between the host and the container\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    # Specifies the networks that this service is part of\n",
    "    networks:\n",
    "      - back-tier\n",
    "\n",
    "  # Defines a service named adminer\n",
    "  adminer:\n",
    "    image: adminer\n",
    "    restart: always\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    networks:\n",
    "      - back-tier\n",
    "      - front-tier  \n",
    "\n",
    "  # Defines a service named grafana\n",
    "  grafana:\n",
    "    image: grafana/grafana\n",
    "    # Sets the user ID under which the service will run\n",
    "    user: \"472\"\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    # Maps local directories or files to directories inside the container\n",
    "    volumes:\n",
    "      # Maps a local file to a file inside the container, and makes it read-only\n",
    "      - ./config/grafana_datasources.yaml:/etc/grafana/provisioning/datasources/datasource.yaml:ro\n",
    "      - ./config/grafana_dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml:ro\n",
    "      - ./dashboards:/opt/grafana/dashboards\n",
    "    networks:\n",
    "      - back-tier\n",
    "      - front-tier\n",
    "    restart: always"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9fecdcf",
   "metadata": {},
   "source": [
    "## Create Grafana Data Source Configuration\n",
    "\n",
    "- The default username and password for Grafana webpage is admin / admin .\n",
    "\n",
    "We'll create a Grafana data source configuration file. It's used to define the data sources that Grafana should connect to. A data source in Grafana represents a back-end database, such as PostgreSQL, MySQL, InfluxDB, or a myriad of other data storage systems. In your project directory, create a new directory named config and a file within it named grafana_datasources.yaml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3479a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config/grafana_datasources.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config/grafana_datasources.yaml\n",
    "\n",
    "# Specifies the version used in this configuration file\n",
    "apiVersion: 1\n",
    "\n",
    "# Defines the data sources that Grafana should connect to\n",
    "# list of datasources to insert/update\n",
    "# available in the database\n",
    "datasources:\n",
    "  # Defines a data source\n",
    "  - \n",
    "    # Specifies the name of the data source\n",
    "    name: PostgreSQL\n",
    "    # Specifies the type of the data source\n",
    "    type: postgres\n",
    "    # Specifies how Grafana should access the data source\n",
    "    # The 'proxy' mode means that all requests are proxied via the Grafana backend/server\n",
    "    access: proxy\n",
    "    # Specifies the URL (including the port number) of the data source\n",
    "    url: db.:5432\n",
    "    # Specifies the name of the database that Grafana should connect to\n",
    "    database: test\n",
    "    # Specifies the username that Grafana should use to connect to the database\n",
    "    user: postgres\n",
    "    # Specifies secure data like passwords\n",
    "    secureJsonData:\n",
    "      # Sets the password for the database connection\n",
    "      password: 'example'\n",
    "    # Specifies additional JSON data for the data source configuration\n",
    "    jsonData:\n",
    "      # Disables SSL mode for the database connection\n",
    "      sslmode: 'disable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f012b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config/grafana_dashboards.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config/grafana_dashboards.yaml\n",
    "\n",
    "# config file version\n",
    "apiVersion: 1\n",
    "\n",
    "providers:\n",
    "  # <string> an unique provider name. Required\n",
    "  - name: 'Evidently Dashboards'\n",
    "    # <int> Org id. Default to 1\n",
    "    orgId: 1\n",
    "    # <string> name of the dashboard folder.\n",
    "    folder: ''\n",
    "    # <string> folder UID. will be automatically generated if not specified\n",
    "    folderUid: ''\n",
    "    # <string> provider type. Default to 'file'\n",
    "    type: file\n",
    "    # <bool> disable dashboard deletion\n",
    "    disableDeletion: false\n",
    "    # <int> how often Grafana will scan for changed dashboards\n",
    "    updateIntervalSeconds: 10\n",
    "    # <bool> allow updating provisioned dashboards from the UI\n",
    "    allowUiUpdates: false\n",
    "    options:\n",
    "      # <string, required> path to dashboard files on disk. Required when using the 'file' type\n",
    "      path: /opt/grafana/dashboards\n",
    "      # <bool> use folder names from filesystem to create folders in Grafana\n",
    "      foldersFromFilesStructure: true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68a2b5a9",
   "metadata": {},
   "source": [
    "## Build and Run Docker Compose\n",
    "\n",
    "Finally, we'll build and run our Docker Compose configuration. In your terminal, navigate to the directory containing docker-compose.yml and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e25266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker-compose up --build"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d238896",
   "metadata": {},
   "source": [
    "You should see that all your containers are successfully created. You can verify this by accessing Grafana and Adminer through your browser at:\n",
    "- [localhost:3000](http://localhost:3000) and \n",
    "- [localhost:8080](http://localhost:8080), respectively.\n",
    "\n",
    "That's it! You've now set up an environment for MLOps using Docker Compose. In the next part, we'll load some data and start implementing monitoring for our service."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "372df1b9",
   "metadata": {},
   "source": [
    "Then open NEW terminal activate CONDA enviroment in terminal, navigate to the directory containing docker-compose.yml and run below code stop docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7c3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker-compose down"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f41e352",
   "metadata": {},
   "source": [
    "# Baseline Model nyc_taxi Data\n",
    "\n",
    "## Q1. Prepare the dataset\n",
    "\n",
    "Start with `baseline_model_nyc_taxi_data.ipynb`. Download the March 2023 Green Taxi data. We will use this data to simulate a production usage of a taxi trip duration prediction service.\n",
    "\n",
    "**What is the shape of the downloaded data? How many rows are there?**\n",
    "\n",
    "* 72044\n",
    "* 78537 \n",
    "* 62495\n",
    "* 54396"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2036ce14",
   "metadata": {},
   "source": [
    "**march_data.shape**\n",
    "\n",
    "(72044, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6c54455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(github_link.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\"))\n",
    "\n",
    "# !wget -Nq \"https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/05-monitoring/baseline_model_nyc_taxi_data.ipynb\"\n",
    "# !wget -Nq \"https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/05-monitoring/debugging_nyc_taxi_data.ipynb\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "159982ef",
   "metadata": {},
   "source": [
    "## Q2. Metric\n",
    "\n",
    "Let's expand the number of data quality metrics we’d like to monitor! Please add one metric of your choice and a quantile value for the `\"fare_amount\"` column (`quantile=0.5`).\n",
    "\n",
    "Hint: explore evidently metric `ColumnQuantileMetric` (from `evidently.metrics import ColumnQuantileMetric`) \n",
    "\n",
    "What metric did you choose?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c70bbf4",
   "metadata": {},
   "source": [
    "From the provided metrics, the metric **chosen** for evaluation is the **ColumnDriftMetric** for the 'prediction' column. It calculates the drift score and detects whether drift is present. The drift score in this case is 0.0219, and the 'drift_detected' field is False, indicating that no drift is detected in the 'prediction' column.\n",
    "\n",
    "```py\n",
    "# prediction drift\n",
    "result['metrics'][0]['result']['drift_score']\n",
    "\n",
    "# 0.02191274473735342\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4284e92c",
   "metadata": {},
   "source": [
    "## Q3. Prefect flow \n",
    "\n",
    "Let’s update prefect tasks by giving them nice meaningful names, specifying a number of delays and retries.\n",
    "\n",
    "Hint: use `evidently_metrics_calculation.py` script as a starting point to implement your solution. Check the  prefect docs to check task parameters.\n",
    "\n",
    "**What is the correct way of doing that?**\n",
    "- https://docs.prefect.io/2.10.18/api-ref/prefect/tasks/#prefect.tasks.task\n",
    "\n",
    "* `@task(retries_num=2, retry_seconds=5, task_name=\"calculate metrics\")`\n",
    "* `@task(retries_num=2, retry_delay_seconds=5, name=\"calculate metrics\")`\n",
    "* `@task(retries=2, retry_seconds=5, task_name=\"calculate metrics\")`\n",
    "* `@task(retries=2, retry_delay_seconds=5, name=\"calculate metrics\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0fafe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -Nq \"https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/05-monitoring/dummy_metrics_calculation.py\"\n",
    "# !wget -Nq \"https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/05-monitoring/evidently_metrics_calculation.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dcad358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pycode/dummy_metrics_calculation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pycode/dummy_metrics_calculation.py\n",
    "\n",
    "import io\n",
    "import uuid\n",
    "import pytz\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import psycopg\n",
    "import logging \n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s]: %(message)s\")\n",
    "\n",
    "SEND_TIMEOUT = 10\n",
    "rand = random.Random()\n",
    "\n",
    "create_table_statement = \"\"\"\n",
    "drop table if exists dummy_metrics;\n",
    "create table dummy_metrics(\n",
    "\ttimestamp timestamp,\n",
    "\tvalue1 integer,\n",
    "\tvalue2 varchar,\n",
    "\tvalue3 float\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prep_db():\n",
    "\twith psycopg.connect(\"host=localhost port=5432 user=postgres password=example\", autocommit=True) as conn:\n",
    "\t\tres = conn.execute(\"SELECT 1 FROM pg_database WHERE datname='test'\")\n",
    "\t\tif len(res.fetchall()) == 0:\n",
    "\t\t\tconn.execute(\"create database test;\")\n",
    "\t\twith psycopg.connect(\"host=localhost port=5432 dbname=test user=postgres password=example\") as conn:\n",
    "\t\t\tconn.execute(create_table_statement)\n",
    "\n",
    "\n",
    "def calculate_dummy_metrics_postgresql(curr):\n",
    "\tvalue1 = rand.randint(0, 1000)\n",
    "\tvalue2 = str(uuid.uuid4())\n",
    "\tvalue3 = rand.random()\n",
    "\n",
    "\tcurr.execute(\n",
    "\t\t\"insert into dummy_metrics(timestamp, value1, value2, value3) values (%s, %s, %s, %s)\",\n",
    "\t\t(datetime.datetime.now(pytz.timezone('Europe/London')), value1, value2, value3)\n",
    "\t)\n",
    "\n",
    "\n",
    "def main():\n",
    "\tprep_db()\n",
    "\tlast_send = datetime.datetime.now() - datetime.timedelta(seconds=10)\n",
    "\twith psycopg.connect(\"host=localhost port=5432 dbname=test user=postgres password=example\", autocommit=True) as conn:\n",
    "\t\tfor i in range(0, 100):\n",
    "\t\t\twith conn.cursor() as curr:\n",
    "\t\t\t\tcalculate_dummy_metrics_postgresql(curr)\n",
    "\n",
    "\t\t\tnew_send = datetime.datetime.now()\n",
    "\t\t\tseconds_elapsed = (new_send - last_send).total_seconds()\n",
    "\t\t\tif seconds_elapsed < SEND_TIMEOUT:\n",
    "\t\t\t\ttime.sleep(SEND_TIMEOUT - seconds_elapsed)\n",
    "\t\t\twhile last_send < new_send:\n",
    "\t\t\t\tlast_send = last_send + datetime.timedelta(seconds=10)\n",
    "\t\t\tlogging.info(\"data sent\")\n",
    "\t\t\t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffa113cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pycode/evidently_metrics_calculation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pycode/evidently_metrics_calculation.py\n",
    "\n",
    "import warnings\n",
    "from numba import NumbaDeprecationWarning\n",
    "# Suppress NumbaDeprecationWarning in umap.distances module\n",
    "warnings.filterwarnings(\"ignore\", category=NumbaDeprecationWarning)\n",
    "\n",
    "import os\n",
    "# TensorFlow to only display error messages \n",
    "# and suppress warnings and informational messages.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import logging \n",
    "import uuid\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import io\n",
    "import psycopg\n",
    "import joblib\n",
    "\n",
    "from prefect import task, flow\n",
    "\n",
    "# Import Evidently for data drift and model performance monitoring\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import ColumnDriftMetric, ColumnQuantileMetric\n",
    "from evidently.metrics import DatasetDriftMetric, DatasetMissingValuesMetric\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s]: %(message)s\")\n",
    "\n",
    "SEND_TIMEOUT = 10\n",
    "rand = random.Random()\n",
    "\n",
    "create_table_statement = \"\"\"\n",
    "drop table if exists evidently_metrics_calculation;\n",
    "create table evidently_metrics_calculation(\n",
    "\ttimestamp timestamp,\n",
    "\tprediction_drift float,\n",
    "\tnum_drifted_columns integer,\n",
    "\tshare_missing_values float,\n",
    "\tcolumn_quantile_values float\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open('models/lin_reg.bin', 'rb') as f_in:\n",
    "\tmodel = joblib.load(f_in)\t\n",
    "\n",
    "begin = datetime.datetime(2023, 3, 1, 0, 0)\n",
    "raw_data = pd.read_parquet('data/green_tripdata_2023-03.parquet')\n",
    "reference_data = pd.read_parquet('data/reference.parquet')\n",
    "\n",
    "# Define the column mapping for the Evidently report\n",
    "num_features = ['passenger_count', 'trip_distance', 'fare_amount', 'total_amount']\n",
    "cat_features = ['PULocationID', 'DOLocationID']\n",
    "column_mapping = ColumnMapping(\n",
    "    prediction='prediction',\n",
    "    numerical_features=num_features,\n",
    "    categorical_features=cat_features,\n",
    "    target=None\n",
    ")\n",
    "\n",
    "# Create a list of metrics to include in the report\n",
    "metrics = [\n",
    "\tColumnDriftMetric(column_name='prediction'),\n",
    "\tDatasetDriftMetric(),\n",
    "\tDatasetMissingValuesMetric(),    \n",
    "\t# Add the ColumnQuantileMetric for the 'fare_amount' column\n",
    "\tColumnQuantileMetric(column_name='fare_amount', quantile=0.5)\n",
    "]\n",
    "\n",
    "# Create the report with the metrics\n",
    "report = Report(metrics=metrics)\n",
    "\n",
    "\n",
    "@task(name=\"Preprocces Database\", log_prints=None)\n",
    "def prep_db():\n",
    "\twith psycopg.connect(\"host=localhost port=5432 user=postgres password=example\", autocommit=True) as conn:\n",
    "\t\tres = conn.execute(\"SELECT 1 FROM pg_database WHERE datname='test'\")\n",
    "\t\tif len(res.fetchall()) == 0:\n",
    "\t\t\tconn.execute(\"create database test;\")\n",
    "\t\twith psycopg.connect(\"host=localhost port=5432 dbname=test user=postgres password=example\") as conn:\n",
    "\t\t\tconn.execute(create_table_statement)\n",
    "\n",
    "\n",
    "@task(name=\"Calculate Metrics\", retries=2, retry_delay_seconds=5, log_prints=None)\n",
    "def calculate_metrics_postgresql(curr, i):\n",
    "\tcurrent_data = raw_data[(raw_data.lpep_pickup_datetime >= (begin + datetime.timedelta(i))) &\n",
    "\t\t(raw_data.lpep_pickup_datetime < (begin + datetime.timedelta(i + 1)))]\n",
    "\n",
    "\t#current_data.fillna(0, inplace=True)\n",
    "\tcurrent_data['prediction'] = model.predict(current_data[num_features + cat_features].fillna(0))\n",
    "\n",
    "\treport.run(\n",
    "\t\treference_data = reference_data, \n",
    "\t\tcurrent_data = current_data,\n",
    "\t\tcolumn_mapping=column_mapping\n",
    "\t)\n",
    "\tresult = report.as_dict()\n",
    "\n",
    "\tprediction_drift = result['metrics'][0]['result']['drift_score']\n",
    "\tnum_drifted_columns = result['metrics'][1]['result']['number_of_drifted_columns']\n",
    "\tshare_missing_values = result['metrics'][2]['result']['current']['share_of_missing_values']\n",
    "\tcolumn_quantile_values = result['metrics'][3]['result']['current']['value']\n",
    "\n",
    "\tcurr.execute(\n",
    "\t\t\"insert into evidently_metrics_calculation(timestamp, prediction_drift, num_drifted_columns, share_missing_values, column_quantile_values) values (%s, %s, %s, %s, %s)\",\n",
    "\t\t(begin + datetime.timedelta(i), prediction_drift, num_drifted_columns, share_missing_values, column_quantile_values)\n",
    "\t)\n",
    "\n",
    "\n",
    "@flow(name=\"Batch Monitoring Backfill\", log_prints=None)\n",
    "def batch_monitoring_backfill():\n",
    "\tprep_db()\n",
    "\tlast_send = datetime.datetime.now() - datetime.timedelta(seconds=10)\n",
    "\twith psycopg.connect(\"host=localhost port=5432 dbname=test user=postgres password=example\", autocommit=True) as conn:\n",
    "\t\tfor i in range(0, 27):\n",
    "\t\t\twith conn.cursor() as curr:\n",
    "\t\t\t\tcalculate_metrics_postgresql(curr, i)\n",
    "\n",
    "\t\t\tnew_send = datetime.datetime.now()\n",
    "\t\t\tseconds_elapsed = (new_send - last_send).total_seconds()\n",
    "\t\t\tif seconds_elapsed < SEND_TIMEOUT:\n",
    "\t\t\t\ttime.sleep(SEND_TIMEOUT - seconds_elapsed)\n",
    "\t\t\twhile last_send < new_send:\n",
    "\t\t\t\tlast_send = last_send + datetime.timedelta(seconds=10)\n",
    "\t\t\tlogging.info(\"data sent\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tbatch_monitoring_backfill()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a13a020",
   "metadata": {},
   "source": [
    "so we edit dummy_metrics_calculation.py and evidently_metrics_calculation.py\n",
    "\n",
    "- Start Prefect server\n",
    "```sh\n",
    "prefect server start\n",
    "\n",
    "```\n",
    "- start again docker compose, if you want to rebuild your services use --build:\n",
    "```sh\n",
    "docker-compose up --build\n",
    "```\n",
    "\n",
    "- Let's test our script RUN the script and go to Grafana and Adminer through your browser at:\n",
    "    - [localhost:3000](http://localhost:3000) and \n",
    "    - [localhost:8080](http://localhost:8080), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "287af268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python pycode/dummy_metrics_calculation.py\n",
    "# python pycode/evidently_metrics_calculation.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cebdce2",
   "metadata": {},
   "source": [
    "## Q4. Monitoring\n",
    "\n",
    "Let’s start monitoring. Run expanded monitoring for a new batch of data (March 2023). \n",
    "\n",
    "What is the maximum value of metric `quantile = 0.5` on th `\"fare_amount\"` column during March 2023 (calculated daily)?\n",
    "\n",
    "* 10\n",
    "* 12.5\n",
    "* 14\n",
    "* 14.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "711d9aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>prediction_drift</th>\n",
       "      <th>num_drifted_columns</th>\n",
       "      <th>share_missing_values</th>\n",
       "      <th>column_quantile_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-01 00:00:00</td>\n",
       "      <td>0.315424</td>\n",
       "      <td>4</td>\n",
       "      <td>0.064327</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-02 00:00:00</td>\n",
       "      <td>2.080613</td>\n",
       "      <td>4</td>\n",
       "      <td>0.065870</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-03 00:00:00</td>\n",
       "      <td>1.743415</td>\n",
       "      <td>4</td>\n",
       "      <td>0.066032</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-04 00:00:00</td>\n",
       "      <td>0.306647</td>\n",
       "      <td>4</td>\n",
       "      <td>0.066331</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-05 00:00:00</td>\n",
       "      <td>0.874525</td>\n",
       "      <td>4</td>\n",
       "      <td>0.064108</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-03-06 00:00:00</td>\n",
       "      <td>1.054192</td>\n",
       "      <td>4</td>\n",
       "      <td>0.064483</td>\n",
       "      <td>13.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-03-07 00:00:00</td>\n",
       "      <td>0.811607</td>\n",
       "      <td>4</td>\n",
       "      <td>0.061817</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-03-08 00:00:00</td>\n",
       "      <td>0.073897</td>\n",
       "      <td>2</td>\n",
       "      <td>0.054498</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-03-09 00:00:00</td>\n",
       "      <td>0.132309</td>\n",
       "      <td>6</td>\n",
       "      <td>0.067361</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-03-10 00:00:00</td>\n",
       "      <td>0.118955</td>\n",
       "      <td>4</td>\n",
       "      <td>0.070164</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-03-11 00:00:00</td>\n",
       "      <td>0.492009</td>\n",
       "      <td>4</td>\n",
       "      <td>0.063666</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-03-12 00:00:00</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>4</td>\n",
       "      <td>0.064969</td>\n",
       "      <td>12.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-03-13 00:00:00</td>\n",
       "      <td>2.924406</td>\n",
       "      <td>4</td>\n",
       "      <td>0.066958</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-03-14 00:00:00</td>\n",
       "      <td>6.138966</td>\n",
       "      <td>4</td>\n",
       "      <td>0.066716</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-03-15 00:00:00</td>\n",
       "      <td>0.571801</td>\n",
       "      <td>4</td>\n",
       "      <td>0.069341</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-03-16 00:00:00</td>\n",
       "      <td>0.543694</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067731</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-03-17 00:00:00</td>\n",
       "      <td>2.987289</td>\n",
       "      <td>5</td>\n",
       "      <td>0.065287</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-03-18 00:00:00</td>\n",
       "      <td>0.967212</td>\n",
       "      <td>5</td>\n",
       "      <td>0.064890</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-03-19 00:00:00</td>\n",
       "      <td>0.602905</td>\n",
       "      <td>5</td>\n",
       "      <td>0.066491</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-03-20 00:00:00</td>\n",
       "      <td>0.991133</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023-03-21 00:00:00</td>\n",
       "      <td>0.230983</td>\n",
       "      <td>5</td>\n",
       "      <td>0.066878</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2023-03-22 00:00:00</td>\n",
       "      <td>1.040482</td>\n",
       "      <td>4</td>\n",
       "      <td>0.065117</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2023-03-23 00:00:00</td>\n",
       "      <td>1.576190</td>\n",
       "      <td>4</td>\n",
       "      <td>0.066755</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2023-03-24 00:00:00</td>\n",
       "      <td>0.525138</td>\n",
       "      <td>4</td>\n",
       "      <td>0.066268</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-03-25 00:00:00</td>\n",
       "      <td>0.392023</td>\n",
       "      <td>5</td>\n",
       "      <td>0.071206</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-03-26 00:00:00</td>\n",
       "      <td>6.200788</td>\n",
       "      <td>4</td>\n",
       "      <td>0.066565</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2023-03-27 00:00:00</td>\n",
       "      <td>2.658853</td>\n",
       "      <td>4</td>\n",
       "      <td>0.065198</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              timestamp  prediction_drift  num_drifted_columns  \\\n",
       "0   2023-03-01 00:00:00          0.315424                    4   \n",
       "1   2023-03-02 00:00:00          2.080613                    4   \n",
       "2   2023-03-03 00:00:00          1.743415                    4   \n",
       "3   2023-03-04 00:00:00          0.306647                    4   \n",
       "4   2023-03-05 00:00:00          0.874525                    4   \n",
       "5   2023-03-06 00:00:00          1.054192                    4   \n",
       "6   2023-03-07 00:00:00          0.811607                    4   \n",
       "7   2023-03-08 00:00:00          0.073897                    2   \n",
       "8   2023-03-09 00:00:00          0.132309                    6   \n",
       "9   2023-03-10 00:00:00          0.118955                    4   \n",
       "10  2023-03-11 00:00:00          0.492009                    4   \n",
       "11  2023-03-12 00:00:00          0.567100                    4   \n",
       "12  2023-03-13 00:00:00          2.924406                    4   \n",
       "13  2023-03-14 00:00:00          6.138966                    4   \n",
       "14  2023-03-15 00:00:00          0.571801                    4   \n",
       "15  2023-03-16 00:00:00          0.543694                    4   \n",
       "16  2023-03-17 00:00:00          2.987289                    5   \n",
       "17  2023-03-18 00:00:00          0.967212                    5   \n",
       "18  2023-03-19 00:00:00          0.602905                    5   \n",
       "19  2023-03-20 00:00:00          0.991133                    4   \n",
       "20  2023-03-21 00:00:00          0.230983                    5   \n",
       "21  2023-03-22 00:00:00          1.040482                    4   \n",
       "22  2023-03-23 00:00:00          1.576190                    4   \n",
       "23  2023-03-24 00:00:00          0.525138                    4   \n",
       "24  2023-03-25 00:00:00          0.392023                    5   \n",
       "25  2023-03-26 00:00:00          6.200788                    4   \n",
       "26  2023-03-27 00:00:00          2.658853                    4   \n",
       "\n",
       "    share_missing_values  column_quantile_values  \n",
       "0               0.064327                   12.80  \n",
       "1               0.065870                   13.50  \n",
       "2               0.066032                   13.50  \n",
       "3               0.066331                   13.50  \n",
       "4               0.064108                   13.50  \n",
       "5               0.064483                   13.05  \n",
       "6               0.061817                   12.80  \n",
       "7               0.054498                   12.80  \n",
       "8               0.067361                   13.50  \n",
       "9               0.070164                   13.50  \n",
       "10              0.063666                   13.50  \n",
       "11              0.064969                   12.99  \n",
       "12              0.066958                   12.80  \n",
       "13              0.066716                   12.80  \n",
       "14              0.069341                   13.50  \n",
       "15              0.067731                   13.50  \n",
       "16              0.065287                   13.50  \n",
       "17              0.064890                   14.00  \n",
       "18              0.066491                   13.50  \n",
       "19              0.067174                   13.00  \n",
       "20              0.066878                   13.50  \n",
       "21              0.065117                   13.50  \n",
       "22              0.066755                   13.50  \n",
       "23              0.066268                   13.50  \n",
       "24              0.071206                   13.50  \n",
       "25              0.066565                   13.50  \n",
       "26              0.065198                   12.80  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/evidently_metrics_calculation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3b0d6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/evidently_metrics_calculation.csv')['column_quantile_values'].max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f89cba8",
   "metadata": {},
   "source": [
    "## Q5. Dashboard\n",
    "\n",
    "\n",
    "Finally, let’s add panels with new added metrics to the dashboard. After we customize the  dashboard lets save a dashboard config, so that we can access it later. Hint: click on “Save dashboard” to access JSON configuration of the dashboard. This configuration should be saved locally.\n",
    "\n",
    "Where to place a dashboard config file?\n",
    "\n",
    "* `project_folder` (05-monitoring)\n",
    "* `project_folder/config`  (05-monitoring/config)\n",
    "* `project_folder/dashboards`  (05-monitoring/dashboards)\n",
    "* `project_folder/data`  (05-monitoring/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_folder/dashboards (05-monitoring/dashboards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab7775c3",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://forms.gle/PJaYeWsnWShAEBF79\n",
    "* You can submit your solution multiple times. In this case, only the last submission will be used\n",
    "* If your answer doesn't match options exactly, select the closest one\n",
    "\n",
    "\n",
    "## Deadline\n",
    "\n",
    "The deadline for submitting is 7 July (Friday), 23:00 CEST (Berlin time). \n",
    "\n",
    "After that, the form will be closed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf34682f",
   "metadata": {},
   "source": [
    "# End of The Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "314.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
